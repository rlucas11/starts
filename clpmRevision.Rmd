---
title: "Why The Cross-Lagged Panel Model is Almost Never the Right Choice"
shorttitle: "Cross-Lagged Panel Model"
author: 
  - name: Richard E. Lucas
    affiliation: 1
    corresponding: yes    # Define only one corresponding author
    address: "316 Physics Rd., Michigan State University, East Lansing, MI 48823"
    email: "lucasri@msu.edu"
affiliation:
  - id: 1
    institution: "Department of Psychology, Michigan State University"


abstract: |
  The cross-lagged panel model (CLPM) is a widely used technique for examining reciprocal causal processes using longitudinal data. Critics of the CLPM have noted that it fails to account for certain person-level confounds. Because of this, models that incorporate stable-trait components (such as the random intercept cross-lagged panel model [RI-CLPM] or the bivariate Stable Trait Autoregressive Trait [STARTS] model) have become popular alternatives. Debates about the merits of the CLPM have continued, however, with some researchers arguing that the CLPM is more appropriate than modern alternatives for examining common psychological questions. In this paper, I argue that these defenses of the CLPM fail to acknowledge widely discussed problems with the interpretation of analyses of multilevel data. I discuss some possible sources of confusion regarding between- and within-person effects that these models estimate, and I then show in simulated data that the CLPM is very likely to find spurious cross-lagged effects when they don't exist, while also underestimating them when they do. I argue that there are no situations where the CLPM is preferable to alternatives that incorporate information about stable traits (though there are, of course, research questions for which neither the CLPM nor alternatives that incorporate a stable trait are appropriate). 
  
  
keywords: "cross-lagged panel model, longitudinal, structural equation modeling"

wordcount: 

header-includes:
   - \usepackage{todonotes}
   - \usepackage{setspace}
   - \AtBeginEnvironment{tabular}{\singlespacing}
   - \AtBeginEnvironment{lltable}{\singlespacing}
   - \AtBeginEnvironment{ThreePartTable}{\singlespacing}
   - \AtBeginEnvironment{tablenotes}{\doublespacing}
   - \captionsetup[table]{font={stretch=1.5}}
   - \captionsetup[figure]{font={stretch=1.5}}
   - \raggedbottom

bibliography:
   - '/home/rich/Dropbox/MyLibraryZ2.bib'
   - r-references.bib
floatsintext: yes
mask: no
linenumbers: no
documentclass: "apa6"
classoption: "man"
output: 
  papaja::apa6_pdf:
  fig_caption: yes

---

```{r setup, include=FALSE}
## Load packages
library(lavaan)
library(tidyverse)
library(knitr)
library(rethinking)
library(papaja)

## Load scripts and models
source("scripts/gen_starts.R") ## Generate data
source("scripts/clpm2_c.R") ## Lavaan model for 2-wave clpm with constraints
source("scripts/clpm3_c.R") ## Lavaan model for 3-wave clpm with constraints
source("scripts/clpm5_c.R") ## Lavaan model for 5-wave clpm with constraints
source("scripts/clpm10_c.R") ## Lavaan model for 10-wave clpm with constraints
source("scripts/ri_clpm3_c.R") ## Lavaan model for 3-wave ri-clpm with constraints
source("scripts/ri_clpm10_c.R") ## Lavaan model for 10-wave ri-clpm with constraints
source("scripts/starts_c.R") ## Lavaan model for 10-wave starts with constraints
source("scripts/run_sim.R") ## Script to run simulations

## Set options
options(knitr.kable.NA='')

```

The cross-lagged panel model (CLPM) is a widely used technique for examining causal processes using longitudinal data. With at least two waves of data, it is possible to estimate the association between a predictor at Time 1 and an outcome at Time 2, controlling for a measure of the outcome at Time 1. With some assumptions, this association can be interpreted as a causal effect of the predictor on the outcome. The simplicity of the model along with its limited data requirements have made the CLPM a popular choice for the analysis of longitudinal data. For instance, @usami_unified_2019 reviewed medical journal articles published between 2009 and 2019 and found 270 papers that used this methodological approach. A broader search of google scholar returned 3,910 papers that use the term "cross-lagged panel model" in the last 40 years.[^search]

[^search]: As of July 20, 2022.

The CLPM improves on simpler cross-sectional analyses by controlling for contemporaneous associations between the predictor and outcome when predicting future scores on the outcome. Presumably, confounding factors should be reflected in this initial association, which would mean that any additional cross-lagged associations between the Time 1 predictor and the Time 2 outcome would reflect a causal effect of the former on the latter (again, with some assumptions).  @hamaker_critique_2015 pointed out, however, that the CLPM does not adequately account for stable-trait-level confounds, and they proposed the random-intercept cross-lagged panel model (RI-CLPM) as an alternative [also see @allison2009fixed; @berry_practical_2017; @zyphur_data_2020]. The RI-CLPM includes stable-trait variance components that reflect variance in the predictor and outcome that is stable across waves. Hamaker et al. showed that failure to account for these random intercepts and the associations between them can lead to incorrect conclusions about cross-lagged paths. As others have noted [e.g., @ludtke_critique_2021; @usami_differences_2020], this critique of the cross-lagged panel model has already been cited frequently and has had an important impact on researchers who use longitudinal data. 

Despite this impact, debates about the relative merits of the CLPM versus the RI-CLPM (and more complex alternatives) continue. Most notably, @orth_testing_2021 argued that sometimes researchers are actually interested in the effects that a classic CLPM tests and that the choice of model should depend on one's theories about the underlying process. Orth et al.'s paper has already been cited over 120 times even though it was published less than a year ago at the time of this writing. Many of the citing papers justify their use of the CLPM based on the arguments that Orth et al. put forth. The goal of this paper is to examine this defense of the CLPM, focusing first on the interpretation of models like the RI-CLPM that include a stable-trait component, followed by simulations that demonstrate the problems with the CLPM and the utility of its alternatives. These simulations show that when the CLPM is used, spurious cross-lagged associations are common and the likelihood of finding such spurious effects can reach 100% in many realistic scenarios. At the same time, the CLPM is also likely to underestimate cross-lagged associations when they do exist. I conclude that there is no situation where the CLPM is preferable to alternatives that model a stable trait and that the CLPM should be abandoned as an approach for examining causal processes in longitudinal data. 

## Ambiguity About Between- and Within-Person Effects

In their critique of the CLPM, @hamaker_critique_2015 described the RI-CLPM as a multilevel model that separates between-person associations from within-person associations. Given the way that these terms have been used in the literature, @hamaker_critique_2015 are certainly correct. But what *is* a between-person association and how does it differ from a within-person association? These questions are critical, as answers to them form the basis for some debates about the CLPM. 

Certain aspects of the between/within distinction are clear and unambiguous. When data are collected from multiple participants at a single point in time, there can only be between-person variance and all associations that can be observed in these data are necessarily between-person associations. For instance, in cross-sectional data, a negative correlation between self-esteem and depression can only be interpreted as a between-persons association: People who score high on measures of self-esteem tend to score low on measures of depression. If, on the other hand, just a single individual is assessed repeatedly over time, all variance is within-person variance and all associations would be within-person associations. For example, if we tracked a single person's self-esteem and depression over time, a negative correlation would reflect a within-person association: When self-esteem is high in that individual, feelings of depression tend to be low.  

The potential for confusion arises, however, when data are collected from multiple people across multiple occasions. Such data include information both about how people differ from one another (between-person variance) and how each person changes over time (within-person variance). Describing effects and associations as "between" versus "within" becomes more challenging. For instance, if feelings of depression were assessed multiple times over the course of a school semester and cross-sectional differences in self-esteem from the start of the semester predicted changes over that period, would this interaction reflect a between-person association or one that is within-person? Although I believe that most would methodologists would label this a between-person effect (because individual differences in self-esteem are predicting individual differences in depression slopes), within-person data (each person's change over the course of the study) are used to estimate this between-persons association. The example shows that labeling associations as between or within is not always straightforward. 

This is important because @orth_testing_2021 rely heavily on the *description* of the cross-lagged effect in the RI-CLPM as a *within-person effect* in their defense of the CLPM. They state that "a potential disadvantage of the proposed alternatives to the CLPM is that they estimate within-person prospective effects only, but not between-person prospective effects" (p. 1014) and that "in many fields researchers are also interested in gaining information about the consequences of between-person differences" (p. 1014). They go on to argue that "a limitation of the RI-CLPM is that it does not provide any information about the consequences of between-person differences. In the RI-CLPM, the between-person differences are relegated to the random intercept factors" (p. 1026). Later on the same page, they state that "The RI-CLPM includes [an] unrealistic assumption, specifically that the between-person variance is perfectly stable" (p. 1026). @orth_testing_2021 do acknowledge later on the same page that "some portion of the systematic between-person variance will be included in the residualized factors" (p. 1026). However, they argue that this discrepancy is a conceptual problem for the RI-CLPM: They state that "the cross-lagged effects in the RI-CLPM are not pure within-person effects but partially confounded with between-person variance" (p. 1026). 

These statements reflect an apparent misunderstanding of the RI-CLPM and related models. Confusion about these issues likely results from the previously mentioned ambiguities regarding the terms "between-person" and "within-person" along with fundamental differences in the ways that different types of multilevel models separate between- and within-person effects. As @curran_disaggregation_2011 noted, what is probably the most familiar way to separate between-person effects from within-person effects is the use of person-mean centering. For instance, in the context of multilevel modeling, researchers are often warned that if they are not careful about how they enter variables into the model, what may look like within-person effects (e.g., the "Level-1" effects in repeated-measures data) can actually reflect a mix of between- and within-person associations. The recommended solution in this context is to person-center the predictor [e.g., @curran_disaggregation_2011; @enders2007centering], where each observation now reflects a deviation from a person's mean. When centering this way, the Level-1 part of the model tests whether occasion-specific deviations from a person's mean predict variability in the outcome. 

In their own critique of the RI-CLPM, @ludtke_critique_2021 imply that the RI-CLPM and related models accomplish the separation of between- and within-person associations in the exact same way as the multilevel modeling approach described by @curran_disaggregation_2011. They cautioned that "researchers should be aware that within-person effects are based on person-mean centered (i.e., ipsatized) scores that only capture temporary fluctuations around individual person means" which would be "less appropriate for understanding the potential effects of causes that explain differences between persons" (p. 18). They do not clarify, however, that the "person mean" in their description is not the observed person mean calculated from the actual observations, but a latent mean that reflects only the variance that is perfectly stable over time. These "means" are---conceptually and empirically---very different things. 

To appreciate this difference, consider the example in Figure \@ref(fig:between). The models shown in this figure represent a single variable, *X*, measured across three occasions. Panel A shows the data-generating process that I used for this example, which reflects a very simple autoregressive model. The initial variance was set to 1, and the wave-to-wave stability was set to .5. With these data, it is possible (and meaningful) to consider what each person's mean would be across these three occasions and what the distribution of those means would be. A simple latent-trait model (like the one shown in Panel B) would capture this conceptualization. Note that in this model, the variance in each wave is partitioned into variance in the person mean (.48 in this example) and variance in the wave-specific deviations from that mean (.68, .42, and .64, in this example). The partitioning of variance in this example is quite similar to the descriptions that @ludtke_critique_2021 and @orth_testing_2021 provide for the variance partitioning in the RI-CLPM. 

```{r between, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Different ways of conceptualizing between-person variance.', out.width = "100%"}

knitr::include_graphics("images/betweenExample.png")

```

It is the third panel of this figure, however, that most closely represents (in a univariate setting) what the RI-CLPM and related models actually do. In this panel, the residuals also have an autoregressive structure, where the residuals for each wave are predicted from the residuals of the wave prior (exactly as they are in the RI-CLPM). When residuals are structured in this way, the latent trait now captures only the variance that is perfectly stable over this time period. The residuals no longer represent deviations from the person mean, they represent deviations from a *perfectly stable trait*[^kou]. Because I created this example to have no perfectly stable variance, these "deviations" are identical to the original variables themselves. 

[^kou]: Thanks to Kou Murayama for discussions that clarified this issue. 

A comparison of the variance estimates across Panels B and C show that the latent trait that links the three observations captures something very different in a simple latent-trait model as compared to a model with structured residuals. Most importantly, although Panel B shows that it is possible and meaningful to think about the person mean and its distribution in these data, the latent variable in Panel C (which is equivalent to the random intercept in the RI-CLPM) does not capture this distribution of means: The variance of these means in the latent-trait model in Panel B is .48, but the variance of the random intercept in the structured-residuals model in Panel C is estimated to be 0. This also means that the occasion-specific residuals in Panel C do not reflect deviations from the person mean, and instead can be thought of as deviations from the perfectly stable trait. This also means that the extent to which interpretations of the structured residuals in a model like that presented in Panel C differ from the interpretation of a standard autoregressive model (as in the data-generating model) depends on the amount of stable-trait variance that exists. Because there is no stable trait variance in this specific hypothetical example, the estimates in the blue box in Panel C (the "within-person" part of the model) recover the parameter values from the data-generating process almost perfectly. 

Orth et al. criticize the RI-CLPM for assuming that all between-person differences are perfectly stable over time, but that critique is not correct. In this and other related models, between-person variance is simply *defined* as that which is perfectly stable. This is an issue of terminology, not assumptions. Moreover, between-person variance (broadly defined) clearly is included in the "within-person" part of Panel C (the part highlighted in the blue box). 

You do not need to remove between-person variance to get within-person effects. 

The first problem with @orth_testing_2021 is that their primary critique of the RI-CLPM rests on an incorrect intepretation of that model, one that is derived from ambiguities in the terminology used to describe between- and within-person effects. 


# Disclosures

## Author Contributions

Richard E. Lucas was responsible for all contributions, including conceptualization, methodology, formal analysis, and writing. 

## Conflicts of Interest

The author declares that there were no conflicts of interest with respect to the authorship or the publication of this article. 

## Prior Versions

A preprint of this paper was posted on the PsyArXiv preprint server: https://psyarxiv.com/pkec7/ .

# References

```{r create_r-references}
## papaja::r_refs(file = "r-references.bib", append = FALSE)
```
