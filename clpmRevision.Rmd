---
title: "Why The Cross-Lagged Panel Model is Almost Never the Right Choice"
shorttitle: "Cross-Lagged Panel Model"
author: 
  - name: Richard E. Lucas
    affiliation: 1
    corresponding: yes    # Define only one corresponding author
    address: "316 Physics Rd., Michigan State University, East Lansing, MI 48823"
    email: "lucasri@msu.edu"
affiliation:
  - id: 1
    institution: "Department of Psychology, Michigan State University"


abstract: |
  The cross-lagged panel model (CLPM) is a widely used technique for examining reciprocal causal processes using longitudinal data. Critics of the CLPM have noted that it fails to account for certain person-level confounds. Because of this, models that incorporate stable-trait components (such as the random intercept cross-lagged panel model [RI-CLPM] or the bivariate Stable Trait Autoregressive Trait [STARTS] model) have become popular alternatives. Debates about the merits of the CLPM have continued, however, with some researchers arguing that the CLPM is more appropriate than modern alternatives for examining common psychological questions. In this paper, I discuss the ways that these defenses of the CLPM fail to acknowledge widely known problems with the interpretation of analyses of multilevel data. I propose some possible sources of confusion regarding between- and within-person effects that these models estimate, and provide alternative ways of thinking about the problems with the CLPM. I then show in simulated data that with realistic assumptions, the CLPM is very likely to find spurious cross-lagged effects when they don't exist, while also underestimating them when they do. I argue that there are no situations where the CLPM is preferable to alternatives that incorporate information about stable traits (though there are, of course, research questions for which neither the CLPM nor alternatives that incorporate a stable trait are appropriate). 
  
  
keywords: "cross-lagged panel model, longitudinal, structural equation modeling"

wordcount: 

header-includes:
   - \usepackage{todonotes}
   - \usepackage{setspace}
   - \AtBeginEnvironment{tabular}{\singlespacing}
   - \AtBeginEnvironment{lltable}{\singlespacing}
   - \AtBeginEnvironment{ThreePartTable}{\singlespacing}
   - \AtBeginEnvironment{tablenotes}{\doublespacing}
   - \captionsetup[table]{font={stretch=1.5}}
   - \captionsetup[figure]{font={stretch=1.5}}
   - \raggedbottom

bibliography:
   - '/home/rich/Dropbox/MyLibraryZ2.bib'
   - r-references.bib
floatsintext: yes
mask: no
linenumbers: no
documentclass: "apa6"
classoption: "man"
output: 
  papaja::apa6_pdf:
  fig_caption: yes

---

```{r setup, include=FALSE}
## Load packages
library(lavaan)
library(tidyverse)
library(knitr)
library(rethinking)
library(papaja)

## Load scripts and models
source("scripts/gen_starts.R") ## Generate data
source("scripts/clpm2_c.R") ## Lavaan model for 2-wave clpm with constraints
source("scripts/clpm3_c.R") ## Lavaan model for 3-wave clpm with constraints
source("scripts/clpm5_c.R") ## Lavaan model for 5-wave clpm with constraints
source("scripts/clpm10_c.R") ## Lavaan model for 10-wave clpm with constraints
source("scripts/ri_clpm3_c.R") ## Lavaan model for 3-wave ri-clpm with constraints
source("scripts/ri_clpm10_c.R") ## Lavaan model for 10-wave ri-clpm with constraints
source("scripts/starts_c.R") ## Lavaan model for 10-wave starts with constraints
source("scripts/run_sim.R") ## Script to run simulations

## Set options
options(knitr.kable.NA='')

```

The cross-lagged panel model (CLPM) is a widely used technique for examining causal processes using longitudinal data. With at least two waves of data, it is possible to estimate the association between a predictor at Time 1 and an outcome at Time 2, controlling for a measure of the outcome at Time 1. With some assumptions, this association can be interpreted as a causal effect of the predictor on the outcome. The simplicity of the model along with its limited data requirements have made the CLPM a popular choice for the analysis of longitudinal data. For instance, @usami_unified_2019 reviewed medical journal articles published between 2009 and 2019 and found 270 papers that used this methodological approach. A broader search of google scholar returned 3,910 papers that use the term "cross-lagged panel model" in the last 40 years.[^search]

[^search]: As of July 20, 2022.

The CLPM improves on simpler cross-sectional analyses by controlling for contemporaneous associations between the predictor and outcome when predicting future scores on the outcome. Presumably, confounding factors should be reflected in this initial association, which would mean that any additional cross-lagged paths between the Time 1 predictor and the Time 2 outcome would reflect a causal effect of the former on the latter (again, with some assumptions).  @hamaker_critique_2015 pointed out, however, that the CLPM does not adequately account for stable-trait-level confounds, and they proposed the random-intercept cross-lagged panel model (RI-CLPM) as an alternative [also see @allison2009fixed; @berry_practical_2017; @zyphur_data_2020]. The RI-CLPM includes stable-trait variance components that reflect variance in the predictor and outcome that is stable across waves. Hamaker et al. showed that failure to account for these random intercepts and the associations between them can lead to incorrect conclusions about cross-lagged paths. As others have noted [e.g., @ludtke_critique_2021; @usami_differences_2020], this critique of the cross-lagged panel model has already been cited frequently and has had an important impact on researchers who use longitudinal data. 

Despite this impact, debates about the relative merits of the CLPM versus the RI-CLPM (and more complex alternatives) continue. Most notably, @orth_testing_2021 argued that sometimes researchers are actually interested in the effects that a classic CLPM tests and that the choice of model should depend on one's theories about the underlying process. Orth et al.'s paper has already been cited over 120 times even though it was only published one year ago at the time of this writing. Many of the citing papers justify their use of the CLPM based on the arguments that Orth et al. put forth. The goal of the current paper is to examine this defense of the CLPM, focusing first on the interpretation of models like the RI-CLPM that include a stable-trait component, followed by simulations that demonstrate the problems with the CLPM and the utility of its alternatives. These simulations show that when the CLPM is used, spurious cross-lagged associations are common and the likelihood of finding such spurious effects can reach 100% in many realistic scenarios. At the same time, the CLPM is also likely to underestimate cross-lagged associations when they do exist. I conclude that there is no situation where the CLPM is preferable to alternatives that model a stable trait and that the CLPM should be abandoned as an approach for examining causal processes in longitudinal data. 

# Ambiguity About Between- and Within-Person Effects

In their critique of the CLPM, @hamaker_critique_2015 described the RI-CLPM as a multilevel model that separates between-person associations from within-person associations. But what *is* a between-person association and how does it differ from a within-person association? Why is it important to separate these levels of analysis in the context of lagged effects? These questions are critical, as answers to them form the basis for some debates about the CLPM. 

Certain aspects of the between/within distinction are clear and unambiguous. When data are collected from multiple participants at a single point in time, there can only be between-person variance. All associations that can be observed in these data are necessarily between-person associations. For instance, in cross-sectional data, a negative correlation between self-esteem and depression can only be interpreted as a between-persons association: People who score high on measures of self-esteem tend to score low on measures of depression. If, on the other hand, just a single individual is assessed repeatedly over time, all variance is within-person variance and all associations would be within-person associations. For example, if a single person's self-esteem and depression were tracked over time, a negative correlation would reflect a within-person association: When self-esteem is high in that individual, feelings of depression tend to be low. 

The potential for confusion arises, however, when data are collected from multiple people across multiple occasions. Such data include information both about how people differ from one another (between-person variance) and how each person changes over time (within-person variance). Describing effects and associations as "between" versus "within" becomes more challenging with these multilevel data. For instance, if feelings of depression were assessed multiple times over the course of a school semester and cross-sectional differences in self-esteem from the start of the semester predicted changes in depression over that period, would this interaction reflect a between-person association or one that is within-person? Although I believe that most methodologists would label this a between-person association (because individual differences in self-esteem are predicting individual differences in depression slopes), within-person data (each person's change over the course of the study) are used to estimate this between-person association. The example shows that the decision to label an association as "between" or "within" is not always linked in a straightforward way to the type of data that contribute to the effect. 

This is important because @orth_testing_2021 rely heavily on the *description* of the cross-lagged paths in the RI-CLPM as a *within-person effect* in their defense of the CLPM. They state that "a potential disadvantage of the proposed alternatives to the CLPM is that they estimate within-person prospective effects only, but not between-person prospective effects" (p. 1014) and that "in many fields researchers are also interested in gaining information about the consequences of between-person differences" (p. 1014). They go on to argue that "a limitation of the RI-CLPM is that it does not provide any information about the consequences of between-person differences. In the RI-CLPM, the between-person differences are relegated to the random intercept factors" (p. 1026). Later on the same page, they state that "The RI-CLPM includes [an] unrealistic assumption, specifically that the between-person variance is perfectly stable" (p. 1026). @orth_testing_2021 do acknowledge later on the same page that "some portion of the systematic between-person variance will be included in the residualized factors" (p. 1026). However, they argue that this discrepancy is a conceptual problem for the RI-CLPM: They state that "the cross-lagged effects in the RI-CLPM are not pure within-person effects but partially confounded with between-person variance" (p. 1026). 

These statements reflect an apparent misunderstanding of the RI-CLPM and related models. Confusion about these issues likely results from the previously mentioned ambiguities regarding the terms "between-person" and "within-person" along with fundamental differences in the ways that different types of multilevel models separate between- and within-person effects. The RI-CLPM separates between-person associations from within-person associations, but it does not do so by "relegating all between-person differences to the random intercept." Because Orth et al.'s [-@orth_testing_2021] defense of the CLPM rests on a flawed interpretation of the alternatives, their defense against these alternatives is not valid. 

As @curran_disaggregation_2011 noted, what is probably the most familiar way to separate between-person effects from within-person effects is the use of person-mean centering. For instance, in the context of multilevel modeling, researchers are often warned that if they are not careful about how they enter variables into the model, what may look like within-person effects (e.g., the "Level-1" effects in repeated-measures data) can actually reflect a mix of between- and within-person associations. The recommended solution in this context is to person-center the predictor [e.g., @curran_disaggregation_2011; @enders2007centering], where each observation now reflects a deviation from a person's mean. When centering this way, the Level-1 part of the model tests whether occasion-specific deviations from a person's mean predict variability in the outcome. 

In their own critique of the RI-CLPM, @ludtke_critique_2021 imply that the RI-CLPM and related models accomplish the separation of between- and within-person associations in the exact same way as the multilevel modeling approach described by @curran_disaggregation_2011 [for discussions about the similarity and differences between lagged models in the multilevel modeling and structural equation modeling contexts, see @hamaker_fixed_2020 and @falkenstrom_how_2022]. They cautioned that "researchers should be aware that within-person effects are based on person-mean centered (i.e., ipsatized) scores that only capture temporary fluctuations around individual person means" which would be "less appropriate for understanding the potential effects of causes that explain differences between persons" (p. 18). They do not clarify, however, that the "person mean" in their description is not the observed person mean calculated from the actual observations (as it would typically be in the traditional multilevel modeling context), but a latent mean that reflects only the variance that is perfectly stable over time. These "means" are---conceptually and empirically---very different things. This also means that what is left after adjusting for these means (the "ipsatized" scores) can also be very different depending on which "mean" is used.

To appreciate this difference, consider the example in Figure \@ref(fig:between). The models shown in this figure represent a single variable, *X*, measured across three occasions. Panel A shows the data-generating process that I used for this example, which reflects a very simple autoregressive model. In this example, the initial variance for *X~1~* was set to 1, and the wave-to-wave stability was set to .5. This simple autoregressive model links between-person differences at Time 1 to between-person differences at Time 2 through a stability coefficient. Note that it would be possible to extend this model to a traditional CLPM by adding an outcome variable at each wave and then testing the lagged paths from the predictor to the outcome and the outcome to the predictor. 

With these data, it is possible (and meaningful) to consider what each person's mean would be across these three occasions. A simple latent-trait model (like the one shown in Panel B) would capture the variance in these means. Note that in this simple latent-trait model, the variance in each wave is partitioned into variance in the person mean (the variance of the common latent trait, which is .48 in this example) and variance in the wave-specific deviations from that mean (.68, .42, and .64). The components of the model in the blue box could be considered the "within-person" part of this model, as these residuals reflect wave-specific deviations from the person mean. The partitioning of variance in this example is quite similar to the descriptions that @ludtke_critique_2021 and @orth_testing_2021 provide for the variance partitioning in the RI-CLPM. 

```{r between, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Different ways of conceptualizing between-person variance.', out.width = "100%"}

knitr::include_graphics("images/betweenExample.png")

```

It is the third panel of this figure, however, that most closely represents (in a univariate setting) what the RI-CLPM and related models actually do. In this panel, the residuals have an autoregressive structure, where the residuals for each wave are predicted from the residuals of the wave prior (exactly as they are in the RI-CLPM). When residuals are structured in this way, they capture between-person variance that is somewhat---but usually not perfectly---stable over time. By structuring the residuals to allow for some wave-to-wave stability, the latent trait now includes only the variance that is *perfectly* stable over this time period. The residuals no longer represent deviations from the person mean, they represent deviations from a *perfectly stable trait*[^kou]. Importantly, although the parts of the model included in the blue box still represent the "within-person" model, these structured residuals now link meaningful between-person differences at Time 1 to between-person differences at Time 2. Indeed, in this specific example, because the data-generating process specifies that there is no perfectly stable variance, these "deviations" are identical to the original variables themselves. The parameter estimates from this model almost perfectly recover the values specified in the simple autoregressive data-generating process[^converge].

[^kou]: Thanks to Kou Murayama for discussions that clarified this issue. 
[^converge]: Note that this model fit to data with no stable trait variance will often result in inadmissable solutions because with true latent-trait variance of 0, it is possible to get estimates that are negative. 

A comparison of the variance estimates across Panels B and C show that the latent trait that links the three observations captures something very different in a simple latent-trait model as compared to a model with structured residuals. However, one could describe either the latent trait from Panel B or the one from Panel C as reflecting the "between-person variance" in these assessments. The latent trait in Panel B captures between-person differences in mean levels of *X* during the observed period of assessment; the latent trait in Panel C captures between-person differences in *a hypothetically perfectly stable* trait. Moreover, one could describe the residuals in either model as reflecting the "within-person" part of the model even though they correspond to conceptually different things. The occasion-specific residuals in Panel C do not reflect deviations from the person mean, they reflect deviations from the perfectly stable trait. 

Orth et al. criticize the RI-CLPM for assuming that all between-person differences are perfectly stable over time and for removing all between-person variance from the within-person associations, but those critiques are not correct. In the RI-CLPM and other related models, between-person variance is simply *defined* as the variance that is perfectly stable over time. This is an issue of terminology, not assumptions. Moreover, between-person variance (broadly defined as differences between individuals) is clearly included in the "within-person" part of the model, as Panel C of Figure \@ref(fig:between) shows. Indeed, for variables for which there is no stable-trait variance, the estimates for the cross-lagged paths from the RI-CLPM will be identical to those from the CLPM because the RI-CLPM reduces to the CLPM in such cases. In these cases, what @orth_testing_2021 would describe as a "between-person prospective effect" (the cross-lagged path from *X~1~* to *Y~2~* is equivalent to what would be described as a "within-person effect" in the RI-CLPM. Thus, the first problem with the arguments presented by @orth_testing_2021 is that their primary critique of the RI-CLPM (and corresponding defense of the CLPM) rests on an incorrect intepretation of that model and its relation to the CLPM. 

# Beyond Between and Within

In the previous section, I discussed ambiguity in the definition of between and within-person effects, focusing on how these ambiguities might lead to misinterpretations of models like the RI-CLPM that include a stable trait component. In the next section, I first review previous explanations for why separating these levels of analysis is critical when examining lagged effects. I then propose an alternative way of thinking about the problems with the CLPM that does not rely on an understanding of the distinction between within- and between-person analyses. 

As noted previously, data that have been collected from multiple people across multiple occasions include information both about how people differ from one another (between-person variance) and how each person changes over time (within-person variance). Methodologists have, for many decades, warned that failure to consider multilevel structures can lead to incorrect conclusions [see @curran_disaggregation_2011, for a review and discussion]. It would be wrong, for instance, to draw conclusions about within-person associations from between-person data or to draw conclusions about between-person differences from within-person data because the association can be completely different at these different levels. In addition---and most importantly for debates about the merits of the CLPM---when data do have a multilevel structure, but this multilevel structure is not taken into account through appropriate analytic methods, the estimates obtained from analyses of these data reflect an uninterpretable mix of between and within-person effects [@raudenbush_hierarchical_2002]. 

The traditional CLPM is a classic example of an analysis that fails to separate between-person associations from within. It is quite easy to show that simple between-person differences can masquerade as within-person effects when the CLPM is used. As a simple demonstration, I generated two waves of data for two variables *X* and *Y* for 10,000 people. Panel A of Figure \@ref(fig:spurious) shows the data-generating process, which is a very simple correlated-latent-trait model. I set the variance of *X* and *Y* to be 1, and the reliability of the indicators to be .5. *X* and *Y* are only associated at the between-person level ($r = .7$)[^ambiguities]. In other words, *X* and *Y* are related only because people who tend to score high on *X* on average also tend to score high on *Y* average; *X* does not predict change in *Y* or vice versa and they have no unique associations within any particular wave. Panel B shows what happens if we fit the CLPM to the generated data. As can be seen in this panel, there would be clear evidence for reciprocal associations between the two, even though there are no over-time associations between *X* and *Y* whatsoever. The CLPM simply cannot distinguish between associations that occur at the stable-trait level from those that incorporate some change over time. 

[^ambiguities]: Note that the terminological ambiguities from the previous section do not play a role in this simple two-wave example, so the meaning of "between-person" differences should be relatively straightforward and unambiguious in this simple case.

```{r spurious, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Spurious cross-lagged effects in data with only between-person associations. Panel A is the data-generating model; Panel B shows estimates from the CLPM fit to the generated data. Coefficients are unstandardized estimates. Residuals are not shown but are estimated.'}
knitr::include_graphics("images/clpm.pdf")
```

Although concerns about the inappropriate handling of multilevel data emerge any time multilevel data are analyzed, they should be particularly salient when lagged effects are examined. This is because a goal of such lagged analyses is typically to clarify causal processes. Only the within-person part of these models, however, provides information about causal effects. If associations at the two levels are conflated, then drawing causal conclusions from these models would be inappropriate. In short, the primary benefit of the CLPM---its ability to provide evidence for causal effects---would be invalidated by the failure to isolate within-person effects. 

@orth_testing_2021 acknowledge that they do indeed believe that the CLPM estimates causal effects. They state that in the context of their focal case study of self-esteem and depression, "the hypothesized causal effect" can be stated to be that: "when individuals have low self-esteem (relative to others), they will experience a subsequent rank-order increase in depression compared to individuals with high self-esteem" (p. 1014). Although this description of the cross-lagged path is technically correct, @orth_testing_2021 do not go on to explain how this association can be interpreted as a causal effect. No formal causal analysis is presented. Unfortunately, when stable-trait variance exists in the measures being analyzed, then this association is confounded. Referring again to Panel B of Figure \@ref(fig:spurious), it is technically correct to say that the path from *X~1~* to *Y~2~* shows that those who score high on *X~1~* have a rank-order increase in *Y* from Wave 1 to Wave 2, but this is due solely to the confounding effect of the stable trait reflected in the latent *X* variable. The rank order on *Y* at Time 2 changes from Time 1 not because of any causal effect of *X*, but because the rank order at *Y~1~* imperfectly reflects the true rank order of *Y*. 

Thus, cross-lagged paths in the CLPM can result from purely between-person associations, purely within-person effects, or some combination of the two (even combinations where the within and between-person associations are in the opposite direction). Rather than clarifying how the CLPM solves the uncontroversial interpretational issues that are inevitably involved with this type of analysis, @orth_testing_2021 sidestep this perennial analytic issue. If these authors believe that the CLPM somehow avoids the interpretational challenges inherent in all analyses of multilevel data, then they must do more than simply assert this to be true. Estimates from the CLPM are uninterpretable for all the same reasons that any analysis of multilevel data that fails to account for the multilevel structure are.

It can sometimes be difficult to think about how the complex nature of multilevel data can affect conclusions about underlying processes. Indeed, I worry that framing the discussion of the CLPM solely as an issue of multilevel structure has led to more confusion than clarity (even though I agree that this conceptualization is technically correct and has led to practical analytic solutions to the problem). Figure \@ref(fig:spurious) helps, however, by providing an alternative way of thinking about the problems with the CLPM and the benefits of alternative models that incorporate a stable-trait component. Models like the RI-CLPM are useful because they test an extremely plausible alternative explanation of the underlying pattern of correlations that is being modeled when the CLPM is used, an alternative explanation that has nothing to do with over-time associations. 

The logic of the CLPM is very similar to the logic of any other regression model where researchers assess whether one variable predicts another after controlling for relevant confounds. When they test whether Time 1 *X* predicts Time 2 *Y* after controlling for Time 1 *Y*, they hope to capture whether there is something unique about *X*---something that cannot be explained by the concurrent association between *X* and *Y*---that helps us predict *Y* at a later time. But as @westfall_statistically_2016 pointed out when discussing the difficulty of establishing incremental predictive validity of any kind, if the measure that we include as a control (i.e., Time 1 *Y*) is not a perfect measure of what researchers are trying to account for, then it is possible---indeed, quite easy---to find spurious "incremental validity" effects. Referring to Figure \@ref(fig:spurious), *Y~1~* is an imperfect measure of the latent variable *Y*. Thus, controlling for *Y~1~* does not control for all of the association between *X* and *Y*, which means that *X~1~* will still have incremental predictive validity of *Y~2~* even after controlling for *Y~1~*. Technically, @orth_testing_2021 are correct that the cross-lagged effect in Panel B means those who score high on *X~1~* would report a "rank-order increase" on *Y* over time, but this rank-order increase results from imperfect control of *Y*, not true change over time. 

One might argue that concerns about inadequate control in the CLPM become moot as long as researchers avoid talking about causal effects. For instance, one might be tempted to interpret the cross-lagged path from *X~1~* to *Y~2~* purely descriptively: Using Orth et al.'s [-@orth_testing_2021] substantive example, it might seem reasonable to conclude from a significant cross-lagged path that initial levels of self-esteem are associated with change in depression over time. However, even this more limited interpretation of this path is not warranted when stable-trait variance exists. Again, Figure \@ref(fig:spurious) shows that the path from *X~1~* to *Y~2~* can emerge simply due to unmodeled stable-trait associations. 

In summary, decades of methodological work show the importance of distinguishing between-person associations from within-person effects when data have a multilevel structure. Failing to do so results in uninterpretable estimates of the association between predictors and outcomes. The CLPM is not an exception to this widely discussed rule. In defending the CLPM, @orth_testing_2021 sidestep the issue of how a model that fails to distinguish between levels can lead to interpretable results; instead, they simply assert that the effects from the CLPM are meaningful. They claim to want to test a "between-person prospective effect" but offer no causal analysis that explains the meaning of such an effect. It is easy to show, however, that when the CLPM is used, it is possible to mistake purely between-person associations for over-time effects, which confirms the long-standing methodological warning about the failure to separate effects at different levels. Moreover, researchers do not even need to think about these issues in terms of multilevel models and the separation of between-person and within-person effects to appreciate the problems with the CLPM. The CLPM simply cannot rule out an extremely plausible alternative explanation for the underlying pattern of correlations. I now turn to a set of simulations that demonstrate just how bad this problem likely is. 

# It's Extremely Easy to Find Spurious Cross-Lagged Effects

The issues discussed in the previous sections show that hypothetically, it is possible to mistake purely between-person associations for over-time effects when the CLPM is used. But how likely are such spurious effects? Unfortunately, it is extremely easy to find spurious cross-lagged associations under conditions that are quite likely in the typical situations where the CLPM is used. @hamaker_critique_2015 and @usami_modeling_2019 conducted simulations to show that the estimates from a cross-lagged panel model were often biased in realistic situations. I don't think they went far enough, though, in describing the practical implications of these simulations or showing just how likely spurious effects are in realistic situations. So the rest of this paper builds on their simulations and tries to clarify when such spurious effects are likely to occur. As I show, there are many realistic scenarios where researchers are almost guaranteed to find spurious cross-lagged effects. 

## A Note About Models

At this point, it is necessary to introduce the models used in these simulations and to clarify the terminology that I will use when describing the components of the models. As @falkenstrom_how_2022 noted, "it is important to first reflect on the relevance of [a statistical analysis method] to the real-world processes a researcher attempts to model" and that "researchers familiar with the study subject can make educated guesses about the nature of this process" (p. 447). It has long been recognized that psychological variables often have features that are both "state-like" and "trait-like" [@hertzog_beyond_1987]. In other words, these variables exhibit stability and change, and it is possible to think about different ways that constructs can stay the same or change over time. 

For instance, @nesselroade_interindividual_1991 noted that there are three types of latent factors that are frequently very useful for explaining variability in repeated measures of individual difference constructs---state factors, slowly changing "trait" factors, and a completely stable trait factors. State factors are the most fleeting, as they reflect variance that is unique to a single measurement occasion. These state factors can include random measurement error, but they also any reliable variance that does not carry over from one wave to the next. If a construct consisted solely of state variance, there would be no stability from wave to the next.  

In contrast, stable-trait factors reflect variance that is perfectly stable across all waves of assessment. If a construct consisted solely of stable trait variance, then wave-to-wave stability would be perfect, regardless of the interval between them. In between these two extremes are slowly changing trait factors where variance at one wave predicts variance at the next, but with less than perfect stability. Stability of this slowly changing trait factor declines with increasing interval length. Of course, these three components do not exhaust all possible patterns of stability and change [see, e.g., @usami_unified_2019, @zyphur_data_2020], but they reflect reasonable assumptions about features that are likely to generalize to a wide range of psychological variables[^means]. 

[^means]: One obvious addition would be mean-level change including linear growth. 

It is possible to frame the CLPM and its modern alternatives in the context of these sources of variance. This is especially helpful when considering what data to simulate. For instance, Panel A of Figure \@ref(fig:riclpmFig) shows a diagram of the model that has been the focus of this paper, the CLPM. This model includes one latent variable per wave for the predictor (*X*) and the outcome (*Y*), and these latent variables have an autoregressive structure with cross-lagged associations. Notice that the CLPM does not include any measurement error for the indicators. This means that the latent variables from the autoregressive part of the model are equivalent to the observed variables (which is why it is also possible to draw an equivalent CLPM model with only observed variables). 

```{r riclpmFig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Diagram of the three models used in this paper.', out.height="90%"}
knitr::include_graphics("images/comboFigure.pdf")
```

Panel B of \@ref(fig:riclpmFig) shows the diagram of the RI-CLPM. The difference between the CLPM and the RI-CLPM is that the RI-CLPM includes a random-intercept (labeled "Stable Trait" in the figure, in accordance with the conceptual description provided earlier) that accounts for "time-invariant, trait-like stability" [@hamaker_critique_2015, p.104]. Including this stable-trait component changes the meaning of the autoregressive part of the model. Whereas in the CLPM, the cross-lagged paths reflect associations between the *X* and *Y* variables over time, in the RI-CLPM, these paths reflect associations among wave-specific deviations from a person's stable-trait level. This is what allows for the separation of between and within-person effects [@allison2009fixed; @curran_disaggregation_2011; @hamaker_critique_2015]. Note that the CLPM is nested within the RI-CLPM; the CLPM is equivalent to the RI-CLPM with the random-intercept (or stable-trait) variance constrained to 0. This also means that if one tries to fit the RI-CLPM to data with no stable-trait variance, the interpretation of the "within-person" or autoregressive part of the model will be identical to the interpretation of the CLPM. 

The final model, presented in Panel C \@ref(fig:riclpmFig), is the bivariate Stable Trait, Autoregressive Trait, State (STARTS) model [@kenny_trait_1995; @Kenny2001], which I have only briefly mentioned up to this point. The STARTS model differs from the RI-CLPM in that it does include measurement error. More precisely, the STARTS includes a wave-specific "state" component (labeled *s~t~* in the figure), which reflects variance in an observed variable that is perfectly "state-like" and unique to that occasion. As noted earlier, this state component can include measurement error or any reliable variance that is unique to a single wave of assessment. The idea that some amount of pure state variance would exist in measures of psychological constructs is quite plausible [@Fraley2005PR], but simpler models like the RI-CLPM have often been preferred because the STARTS requires more waves of data than the RI-CLPM (four, to be precise) and often has estimation problems [e.g., @cole_empirical_2005; @orth_testing_2021; @usami_modeling_2019]. 

Recently, @usami_unified_2019 clarified that the CLPM, RI-CLPM, STARTS and many other longitudinal models could be thought of as variations of an overarching "unified" model that captures many different forms of change [also see @zyphur_data_2020]. For instance, an alternative model---the Latent Curve Model with Structured Residuals [@curran_separation_2014]---can be thought of as an RI-CLPM with a random slope. Because debates about the utility of the CLPM have primarily focused on debates about the inclusion of the random-intercept, I focus here only on the comparison of the CLPM to the RI-CLPM and the STARTS, as this comparison highlights these debates most clearly. It is certainly true, however, that if the other forms of change included in the unified model were part of the actual data generating process, then all the models covered in this paper would be misspecified and could lead to biased estimates. 

	
## The Simulations

When considering what types of situations to simulate, I focus on realistic scenarios for the types of data to which the CLPM is likely to be applied. For instance, it is likely that most variables that psychologists (and other social and health scientists) choose to study over time have a longitudinal structure where stability declines with increasing interval length (reflecting an autoregressive structure), yet this decline approaches or reaches an asymptote where further increases in interval length are no longer associated with declines in stability (reflecting the influence of a stable trait). It is also likely that most measures of psychological constructs have some amount of pure state variability, which could reflect measurement error or true state-like influences. 

To test the plausibility of these starting assumptions, I selected ten diverse variables that have been included in almost every wave of the long-running Household Income and Labour Dynamics in Australia (HILDA) panel study, which now spans 20 waves of assessment [@watson_hilda_2012]. I intentionally selected variables from different domains and variables that might have different psychometric properties due to how easily observable they are (e.g., weight and income are likely to be measured with less error than life satisfaction or social support). I then fit the most inclusive of the three models discussed (the STARTS model) to see how much variance each component accounted for. Results are shown in Table \@ref(tab:hilda). 

```{r hilda, echo=FALSE, message=FALSE, warning=FALSE}
hilda <- read_csv("saved/hildaDecompTable.csv")
papaja::apa_table(hilda,
                  caption = "Variance components from the STARTS model for 10 variables in the HILDA. Table entries reflect the proportion of total variance accounted for by that component. All variables were assessed in each of the 20 waves except (), which were included in all but Wave 1.")

```

The first thing to note from this table is that the CLPM, which assumes a purely first-order autoregressive structure, would be misspecified when applied to any of these variables, as there are substantial stable-trait components for all ten variables[^ar2]. Second, for almost all of the variables that were analyzed, the state component is also quite large, often accounting for one-quarter to one-third of the variance in these measures. These estimates can be used to evaluate the plausibility of the values that I chose for the simulation studies. 

[^ar2]: It is important to note that there are other possible data-generating processes that will lead to the appearance of stable trait variance, including autoregressive effects beyond the first order [@ludtke_critique_2021]. However, omitting these higher-order autoregressive effects from a lagged model when they exist will likely have a similar effect on the cross-lagged paths as the omission of a stable trait factor, and thus, these possibilities are not discussed further as the subtle differences are beyond the scope of this paper. 

I used the simulations to test how variation in these factors affects the estimated cross-lagged paths when the CLPM is used. A Shiny app is available where variations of this data-generating model can be specified and the effects on cross-lagged paths can be tested: [http://shinyapps.org/apps/clpm/](http://shinyapps.org/apps/clpm/)[^app]. Readers can use this app to examine the specifications described in the text and to test alternatives. 

[^app]: The app and source code are also available on the corresponding OSF site: [https://osf.io/4qukz/](https://osf.io/4qukz/). In the "Shiny" component, download the "app.R" file and the "scripts" folder and then run it like any other Shiny app. 

Because the focus of this paper is on examining the effects of unmodeled stable trait variance, I set the variance of the stable trait component for the predictor and outcome to be 1 in the primary simulations (though occasionally, I do set stable-trait variance to zero to address specific questions). I then varied the ratio of autoregressive variance to stable-trait variance across four levels: 0, .5, 1, and 2. Similarly, I varied the ration of non-state to state variance (which would reflect the reliability of the measures if state variance consisted only of measurement error) across three levels: .5, .7, and .9. The results shown in Table \@ref(tab:hilda) show that these values correspond to what we might find in real data. Finally, I varied the size of the correlation between the stable traits across four levels from very weak to very strong: .1, .3, .5, and .7. I ran 1,000 simulations for each of five sample sizes: 50, 100, 250, 500, and 1,000). In all simulations, I set the correlation between the initial autoregressive variance components for the predictor and outcome to be .50 and the stability of the autoregressive components to be .50 (though, later, I discuss some modifications to this). I also set the correlations between state components to be 0. Most importantly, all true cross-lagged paths were set to be 0. Consistent with the canonical STARTS model, I included a stationarity constraint, so that variances, correlations, and stability coefficients are constrained to be equal over time. This constraint is not absolutely necessary, but it simplifies discussion of the estimated cross-lagged paths, as there is just one estimate per model. 

After generating the data, I tested a simple two-wave CLPM, keeping track of the average size of the estimated cross-lagged paths and the number of cross-lagged paths that were significant at a level of .05. Note that researchers are often interested in determining which of the two variables in the model has a causal impact on the other rather than on simply testing the effect of one predictor on an outcome. Thus, an effect of *X* on *Y*, *Y* on *X*, or both would often be interpreted as a "hit" in common applications of the CLPM. This means that error rates are typically elevated in the CLPM even without unmodeled stable-trait effects unless corrections for multiple comparisons are used. In these simulations, I report the percentage of runs that result in at least one significant cross-lagged effect (out of two tested), and these can be compared to a baseline error rate of approximately 10%, assuming multiple comparisons are ignored. 

Finally, although I focus on the common two-wave CLPM design, it is important to note that more waves of data lead to increased power to detect smaller effects---even spurious effects. This means that spurious cross-lagged associations are more likely to be found with better, multi-wave designs. Thus, I will also present results from simulations with more waves of data after presenting the primary results. Code used to generate the data, test the models, and run the simulation are available here: [https://osf.io/4qukz/](https://osf.io/4qukz/). All analyses were run using `r papaja::cite_r("r-references.bib", footnote=TRUE)$r`.

`r papaja::cite_r("r-references.bib", footnote=TRUE)$pkgs`


# Disclosures

## Author Contributions

Richard E. Lucas was responsible for all contributions, including conceptualization, methodology, formal analysis, and writing. 

## Conflicts of Interest

The author declares that there were no conflicts of interest with respect to the authorship or the publication of this article. 

## Prior Versions

A preprint of this paper was posted on the PsyArXiv preprint server: https://psyarxiv.com/pkec7/ .

## Data Usage

This paper uses unit record data from Household, Income and Labour Dynamics in Australia Survey [HILDA] conducted by the Australian Government Department of Social Services (DSS). The findings and views reported in this paper, however, are those of the author[s] and should not be attributed to the Australian Government, DSS, or any of DSSâ€™ contractors or partners. DOI: ####

# References

```{r create_r-references}
## papaja::r_refs(file = "r-references.bib", append = FALSE)
```
