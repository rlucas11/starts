---
title: "Why The Cross-Lagged Panel Model is Almost Never the Right Choice"
shorttitle: "Cross-Lagged Panel Model"
author: 
  - name: Richard E. Lucas
    affiliation: 1
    corresponding: yes    # Define only one corresponding author
    address: "316 Physics Rd., Michigan State University, East Lansing, MI 48823"
    email: "lucasri@msu.edu"
affiliation:
  - id: 1
    institution: "Department of Psychology, Michigan State University"


abstract: |
  The cross-lagged panel model (CLPM) is a widely used technique for examining reciprocal causal effects using longitudinal data. Critics of the CLPM have noted that it fails to account for certain person-level confounds. Because of this, models that incorporate stable-trait components (such as the random intercept cross-lagged panel model [RI-CLPM]) have become popular alternatives. Debates about the merits of the CLPM have continued, however, with some researchers arguing that the CLPM is more appropriate than modern alternatives for examining common psychological questions. In this paper, I discuss the ways that these defenses of the CLPM fail to acknowledge well-known limitations of the model. I propose some possible sources of confusion regarding the effects that these models estimate, and I provide alternative ways of thinking about the problems with the CLPM. I then show in simulated data that with realistic assumptions, the CLPM is very likely to find spurious cross-lagged effects when they don't exist while also underestimating them when they do. 
  
  
keywords: "cross-lagged panel model, longitudinal, structural equation modeling"

wordcount: 

header-includes:
   - \usepackage{todonotes}
   - \usepackage{setspace}
   - \AtBeginEnvironment{tabular}{\singlespacing}
   - \AtBeginEnvironment{lltable}{\singlespacing}
   - \AtBeginEnvironment{ThreePartTable}{\singlespacing}
   - \AtBeginEnvironment{tablenotes}{\doublespacing}
   - \captionsetup[table]{font={stretch=1.5}}
   - \captionsetup[figure]{font={stretch=1.5}}
   - \raggedbottom

bibliography:
   - '/home/rich/Dropbox/MyLibraryZ2.bib'
   - r-references.bib
floatsintext: yes
mask: no
linenumbers: no
documentclass: "apa6"
classoption: "man"
output: 
  papaja::apa6_pdf:
  fig_caption: yes

---

```{r setup, include=FALSE}
## Load packages
library(lavaan)
library(tidyverse)
library(knitr)
library(rethinking)
library(papaja)
library(directlabels)
library(gridExtra)

## Load scripts and models
source("scripts/gen_starts.R") ## Generate data
source("scripts/clpm2_c.R") ## Lavaan model for 2-wave clpm with constraints
source("scripts/clpm3_c.R") ## Lavaan model for 3-wave clpm with constraints
source("scripts/clpm5_c.R") ## Lavaan model for 5-wave clpm with constraints
source("scripts/clpm10_c.R") ## Lavaan model for 10-wave clpm with constraints
source("scripts/ri_clpm3_c.R") ## Lavaan model for 3-wave ri-clpm with constraints
source("scripts/ri_clpm10_c.R") ## Lavaan model for 10-wave ri-clpm with constraints
source("scripts/starts_c.R") ## Lavaan model for 10-wave starts with constraints
source("scripts/run_sim.R") ## Script to run simulations
source("scripts/usefulFunctions.R") ## Summarize correlations w/ different lags

## Set options
options(knitr.kable.NA='')

```

The cross-lagged panel model (CLPM) is a widely used technique for examining causal processes using longitudinal data [@heise_causal_1970; @finkel_causal_1995]. With at least two waves of data, it is possible to estimate the association between a predictor at Time 1 and an outcome at Time 2, controlling for a measure of the outcome at Time 1. With some assumptions, this association can be interpreted as a causal effect of the predictor on the outcome. The simplicity of the model along with its limited data requirements have made the CLPM a popular choice for the analysis of longitudinal data. For instance, @usami_unified_2019 reviewed medical journal articles published between 2009 and 2019 and found 270 papers that used this methodological approach. A broader search of google scholar returned over 4,500 papers that use the term "cross-lagged panel model" in the last 40 years[^search].

[^search]: As of December 27, 2022.

The CLPM expands on simpler cross-sectional analyses by controlling for contemporaneous associations between the predictor and outcome when predicting future scores on the outcome. Presumably, confounding factors should be reflected in this initial association, which would mean that any additional cross-lagged associations between the Time 1 predictor and the Time 2 outcome would reflect a causal effect of the former on the latter (again, with some assumptions).  @hamaker_critique_2015 pointed out, however, that the CLPM does not adequately account for stable-trait-level confounds, and they proposed the random-intercept cross-lagged panel model (RI-CLPM) as an alternative [also see @allison2009fixed; @berry_practical_2017; @zyphur_data_2020]. The RI-CLPM includes stable-trait variance components that reflect variance in the predictor and outcome that is stable across waves. Hamaker et al. showed that failure to account for these random intercepts and the associations between them can lead to incorrect conclusions about cross-lagged effects. As others have noted [e.g., @ludtke_critique_2021; @usami_differences_2020], this critique of the cross-lagged panel model has already been cited frequently and has had an important impact on researchers who use longitudinal data. 

Despite this impact, debates about the relative merits of the CLPM versus the RI-CLPM (and more complex alternatives) continue. Most notably, @orth_testing_2021 argued that sometimes researchers are actually interested in the associations that a classic CLPM tests and that the choice of model should depend on one's theories about the underlying process. Orth et al.'s paper has already been cited almost 200 times even though it was only published approximately one year ago at the time of this writing. Many of the citing papers justify their use of the CLPM based on the arguments that Orth et al. put forth. @asendorpf_modeling_2021 presented similar arguments to those raised by @orth_testing_2021, warning that alternatives to the CLPM like the RI-CLPM should not be used to model long-term longitudinal data. 

The goal of the current paper is to examine these defenses of the CLPM, focusing first on the interpretation of models like the RI-CLPM that include a stable-trait component, followed by simulations that demonstrate the problems with the CLPM and the utility of its alternatives. These simulations show that when the CLPM is used, spurious cross-lagged effects are common and the likelihood of finding such spurious effects can reach 100% in many realistic scenarios. At the same time, the CLPM is also likely to underestimate cross-lagged effects when they do exist. Because in most areas of psychological research, trait-like confounders are likely to be present, the CLPM will usually be misspecified and should not be used for causal inference from longitudinal data. 

# Between- and Within-Person Effects: Implications for Model Choice

In their critique of the CLPM, @hamaker_critique_2015 described the RI-CLPM as a multilevel model that separates between-person associations from within-person associations. But what *is* a between-person association and how does it differ from a within-person association? Why is it important to separate these levels of analysis when examining lagged causal effects? These questions are critical, as answers to them form the basis for some debates---and misunderstandings---about the CLPM. 

For instance, @orth_testing_2021 rely heavily on the description of the cross-lagged paths in the RI-CLPM as *within-person effects* in their defense of the CLPM. They state that "a potential disadvantage of the proposed alternatives to the CLPM is that they estimate within-person prospective effects only, but not between-person prospective effects" (p. 1014) and that "in many fields researchers are also interested in gaining information about the consequences of between-person differences" (p. 1014). They go on to argue that "a limitation of the RI-CLPM is that it does not provide any information about the consequences of between-person differences. In the RI-CLPM, the between-person differences are relegated to the random intercept factors" (p. 1026). Later on the same page, they state that "The RI-CLPM includes [an] unrealistic assumption, specifically that the between-person variance is perfectly stable" (p. 1026). @orth_testing_2021 do acknowledge later on in their paper that "some portion of the systematic between-person variance will be included in the residualized factors" (p. 1026). However, they argue that this discrepancy is a conceptual problem for the RI-CLPM: They state that "the cross-lagged effects in the RI-CLPM are not pure within-person effects but partially confounded with between-person variance" (p. 1026). These statements---statements that form the basis of Orth et al.'s defense of the CLPM---reflect a misunderstanding of the RI-CLPM and its relation to the CLPM.

In this section, I argue that the "between-person effects" that @orth_effect_2022 hope to obtain can almost never be accurately estimated using the CLPM and that their claims about the limitations of the RI-CLPM and related models are incorrect. I also briefly discuss additional models beyond the RI-CLPM that can address at least some of the conceptual concerns about the RI-CLPM that remain after these misconceptions are corrected. 

## A Note About Models and Terminology

At this point, it is necessary to introduce more formally the models discussed in this paper and to clarify the terminology that I will use when describing the components of the models. As @falkenstrom_how_2022 noted, "it is important to first reflect on the relevance of [a statistical analysis method] to the real-world processes a researcher attempts to model" and that "researchers familiar with the study subject can make educated guesses about the nature of this process" (p. 447). It has long been recognized that psychological variables often have features that are both "state-like" and "trait-like" [@hertzog_beyond_1987]. In other words, these variables exhibit stability and change, and it is possible to think about different ways that constructs can stay the same or change over time. 

For instance, @nesselroade_interindividual_1991 noted that there are at least three types of latent factors that are frequently very useful for explaining variability in repeated measures of individual difference constructs---state factors, slowly changing "trait" factors, and a completely stable trait factors [also see @Kenny2001]. State factors are the most fleeting, as they reflect variance that is unique to a single measurement occasion. These state factors can include random measurement error, but they can also include any reliable variance that does not carry over from one wave to the next. If a construct consisted solely of state variance, there would be no stability over time.  

In contrast, stable-trait factors reflect variance that is perfectly stable across all waves of assessment. If a construct consisted solely of stable trait variance, then wave-to-wave stability would be perfect regardless of the length of the interval between them. In between these two extremes are slowly changing trait factors where variance at one wave predicts variance at the next, but with less than perfect stability. @Kenny2001 labeled this as "autoregressive trait" variance, to reflect the fact that there is some "trait-like" stability (reflected in a non-zero stability coefficient from one wave to the next), along with change over the long term. Stability of this autoregressive trait factor declines with increasing interval length. Of course, these three components do not exhaust all possible patterns of stability and change [see, e.g., @usami_unified_2019; @zyphur_data_2020], but they reflect reasonable assumptions about features that are likely to generalize to a wide range of psychological variables. 

It is possible to frame the CLPM and its modern alternatives in the context of these sources of variance[^kou]. For instance, Panel A of Figure \@ref(fig:riclpmFig) shows a diagram of the model that has been the focus of this paper: the CLPM. Although the CLPM can be drawn using only observed variables, Figure \@ref(fig:riclpmFig) includes latent variables to emphasize the relation to its more complex alternatives. The model includes one latent variable per wave for the predictor (*X*) and the outcome (*Y*), and these latent variables have an autoregressive structure with cross-lagged associations that are meant to capture the causal effects. Notice that the CLPM does not include any measurement error for the indicators. This means that the latent variables from the autoregressive part of the model are equivalent to the observed variables (which is why it is also possible to draw an equivalent CLPM model with only observed variables). The CLPM assumes that all variance is of the slowly-changing, autoregressive variety described by @nesselroade_interindividual_1991. 

[^kou]: Though see @murayama_thinking_2022 for a discussion of precisely how these components should be conceptualized in a causal-modeling framework. 

```{r riclpmFig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Diagram of the three models used in this paper.', out.height="90%"}
knitr::include_graphics("images/comboFigure.pdf")
```

Panel B of Figure \@ref(fig:riclpmFig) shows the diagram of the RI-CLPM. The difference between the CLPM and the RI-CLPM is that the RI-CLPM includes a random intercept that accounts for "time-invariant, trait-like stability" [@hamaker_critique_2015, p.104]. The random intercept corresponds to purely stable trait variance and is thus labeled "Stable Trait" in the figure. Including this stable-trait component changes the meaning of the autoregressive part of the model. Whereas in the CLPM, the cross-lagged paths reflect associations between the *X* and *Y* variables over time, in the RI-CLPM, these paths reflect associations among wave-specific deviations from a person's stable-trait level. This is what allows for the separation of between and within-person associations and the interpretation of the cross-lagged association as a causal effect [@hamaker_critique_2015; @usami_differences_2020]. 

Note that the CLPM is nested within the RI-CLPM; the CLPM is equivalent to the RI-CLPM with the random-intercept (or stable-trait) variance constrained to 0. This also means that if one tries to fit the RI-CLPM to data with no stable-trait variance, the interpretation of the "within-person" or autoregressive part of the model will be identical to the interpretation of the CLPM. This will be a critical point when evaluating the validity of Orth et al.'s [-@orth_testing_2021] critiques of the RI-CLPM and related models.

The final model, presented in Panel C of Figure \@ref(fig:riclpmFig), is the bivariate Stable Trait, Autoregressive Trait, State (STARTS) model [@kenny_trait_1995; @Kenny2001], which I have not yet mentioned, but which will play a role in the later simulations. The STARTS model differs from the RI-CLPM in that it includes includes a wave-specific "state" component (labeled *S* in the figure), which reflects variance in an observed variable that is perfectly "state-like" and unique to that occasion. This state component can include measurement error or any reliable variance that is unique to a single wave of assessment. The idea that some amount of pure state variance would exist in measures of psychological constructs is quite plausible [@Fraley2005PR], but simpler models like the RI-CLPM have often been preferred because the STARTS requires more waves of data than the RI-CLPM (four, to be precise) and often has estimation problems [e.g., @cole_empirical_2005; @orth_testing_2021; @usami_modeling_2019]. 

Recently, @usami_unified_2019 clarified that the CLPM, RI-CLPM, STARTS and many other longitudinal models could be thought of as variations of an overarching "unified" model that captures many different forms of change [also see @zyphur_data_2020 and @usami_differences_2020]. Because debates about the utility of the CLPM have primarily focused on debates about the inclusion of the random intercept, I focus in this paper primarily on the comparison of the CLPM to the RI-CLPM and the STARTS, as this comparison highlights these debates most clearly. It is certainly true, however, that if the other forms of change included in the unified model were part of the actual data generating process, then all the models covered in this paper would be misspecified and could lead to biased estimates. 

As a final terminological note, I will try to reserve the term "effects" for causal links from one variable to another and I will use the term "associations" for empirical links where causality is not assumed or cannot be determined. I occasionally (and necessarily) deviate from this usage when describing certain claims about what these models can and cannot do. 

## Does the CLPM Provide Estimates of Between-Person Causal Effects?

@orth_testing_2021 noted that their preference for the CLPM is based on their desire for a model in which lagged associations reflect between-person causal effects. Can the cross-lagged paths in the CLPM be interpreted in this way? Unfortunately, in most cases, they cannot, at least with the standard CLPM. This is due to the fact that a critical assumption underlying the CLPM is likely to be violated in the types of data that psychologists and other social scientists are likely to use. Specifically, an important assumption in the CLPM is that the wave-specific disturbances (the *U*s in the Panel A of Figure \@ref(fig:riclpmFig)) are uncorrelated with the variables measured at prior waves [@heise_causal_1970]. Any unmeasured variable that results in (unmodeled) correlations between these wave-specific disturbances will invalidate the interpretation of the cross-lagged associations as causal effects. 

A comparison of model-implied stability coefficients to real-world data shows that this assumption is quite likely to be violated. Figure \@ref(fig:hildaFig) provides an example of such a comparison. To begin, consider a single variable that has a lag-1 autoregressive structure, such that current standing on that variable is determined by past standing (through a stability coefficient), and a disturbance term. Panel A shows the implied stability coefficients over various intervals for three hypothetical variables, each with a lag-1 autoregressive structure. The three lines represent variables that differ in their one-year stability, which ranges from .75 (shown in the top line) to .25 (shown in the bottom-line). For variables with this lag-1 autoregressive structure, the stability over any given interval is a function of the one-year stability raised to the power of the number of years that have elapsed. So, the expected correlation for a measure with a one-year stability of .75 assessed over a two year period would be .75^2^ = `r .75 * .75`. The expected correlation over a three-year period would be .75^3^ = `r .75^3`. Panel A shows that for variables with a lag-1 autoregressive structure, stability coefficients are expected to decline quickly over time, even for variables that were initially quite stable. If actual stability coefficients decline more slowly than what is shown in Panel A of Figure \@ref(fig:hildaFig), then this would suggest that variable does not have a lag-1 autoregressive structure.

```{r hildaFig, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.cap="Implied and actual stability over increasing time lags. The data from Panel A were generated from a lag-1 autoregressive model with no cross-lagged paths. The data from Panel B were generated from a model with cross-lagged effects. The data from Panel C show the actual stability of ten diverse variables from a panel study.", dpi=300}
nYears <- 10

################################################################################
## Plot AR-1 Data
################################################################################
## Function to calculate decline in correlations as lag increases
arTrend <- function(x, n) {
    corVec <- vector(length=n)
    for (i in 1:n) {
        corVec[i] <- x^i
    }
    return(corVec)
}

data <- data.frame(
    Years = rep(1:nYears, 3),
    Correlation = c(
        arTrend(.75, nYears),
        arTrend(.50, nYears),
        arTrend(.25, nYears)
    ),
    Stability = rep(c(.75, .50, .25), each = nYears)
)

arPlot <- ggplot(aes(x=Years, y=Correlation, group=Stability),
                 data=data) +
    geom_line(lwd=.2) +
        theme_grey(base_size = 8) +
        ylim(-.1, 1) +
        scale_x_continuous(labels=c(1:10), breaks=c(1:10)) +
    theme(
        panel.background = element_rect(fill='transparent', color=NA),
        plot.background = element_rect(fill='transparent', color=NA),
		axis.line = element_line(color="black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
    ) +
    ggtitle('Panel A')

################################################################################
## Plot AR-1 Data with Cross-Lagged Paths
################################################################################

cl <- .20 ## 90th Percentile, from Orth
dataCL.1 <- data.frame(
    Years = rep(1:nYears, 3),
    Correlation = c(
        summarizeR(cor(gen_starts(
            n = 10000,
            nwaves = nYears+1,
            ri_x = 0,
            ri_y = 0,
            cor_i = 0,
            x = 1,
            y = 1,
            stab_x = .75,
            stab_y = .75,
            yx = cl,
            xy = cl,
            cor_xy = .5,
            xr = 0,
            yr = 0
        )[, 1:11])),
        summarizeR(cor(gen_starts(
            n = 10000,
            nwaves = nYears+1,
            ri_x = 0,
            ri_y = 0,
            cor_i = 0,
            x = 1,
            y = 1,
            stab_x = .5,
            stab_y = .5,
            yx = cl,
            xy = cl,
            cor_xy = .5,
            xr = 0,
            yr = 0
        )[, 1:11])),
        summarizeR(cor(gen_starts(
            n = 10000,
            nwaves = nYears+1,
            ri_x = 0,
            ri_y = 0,
            cor_i = 0,
            x = 1,
            y = 1,
            stab_x = .25,
            stab_y = .25,
            yx = cl,
            xy = cl,
            cor_xy = .5,
            xr = 0,
            yr = 0
        )[, 1:11]))
    ),
    Stability = rep(c(.75, .50, .25), each = nYears)
)

clPlot <- ggplot(aes(x=Years, y=Correlation, group=Stability),
                 data=dataCL.1) +
    geom_line(lwd=.2) +
        theme_grey(base_size = 8) +
        ylim(-.1, 1) +
        scale_x_continuous(labels=c(1:10), breaks=c(1:10)) +
    theme(
        panel.background = element_rect(fill='transparent', color=NA),
        plot.background = element_rect(fill='transparent', color=NA),
		axis.line = element_line(color="black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        ## axis.text.y=element_blank(),
        ## axis.title.y=element_blank()
    ) +
    ggtitle('Panel B')

################################################################################
## Plot Actual Data from HILDA
################################################################################

hilda <- read_csv("data/hildaCors.csv")
names(hilda) <- c("Year", "Correlation", "Variable")

hildaPlot <- ggplot(aes(x=Year, y=Correlation, fill=Variable),
                 data=hilda[hilda$Year <= 10,]) +
    geom_line(lwd=.2) +
	theme_grey(base_size=8) +
    theme(
        panel.background = element_rect(fill='transparent', color=NA),
        plot.background = element_rect(fill='transparent', color=NA),
		axis.line = element_line(color="black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.background = element_rect(fill='transparent'),
        legend.box.background = element_rect(fill='transparent'),
        ## axis.text.y=element_blank(),
        ## axis.title.y=element_blank(),
    ) +
    ylim(-.1, 1) +
#    xlim(1,12) +
    scale_x_continuous(labels=c(1:10), breaks=c(1:10), limits=c(1,11)) +
    scale_color_manual(values = rep("black", 10)) +
    ggtitle("Panel C")
#direct.label(hildaPlot, "angled.boxes")

################################################################################
## Combine Plots
################################################################################
my.dl <- list(fill="white", "draw.rects")
grid.arrange(arPlot,
             clPlot,
##             direct.label(hildaPlot, list(cex=.3, "angled.boxes")),
             direct.label(hildaPlot, list(cex=.3,
                             "far.from.others.borders",
                             "calc.boxes",
                             "enlarge.box",
                             "my.dl")),
             nrow=1)


```

The standard CLPM builds on this simple lag-1 autoregressive model, extending it to include lagged, reciprocal associations between two or more variables. The existence of any true lagged effects from one variable to another complicates expectations about patterns of stability over time. Specifically, if there are lagged effects from a predictor that itself has some stability, then stability coefficients for the outcome will decline more slowly than if the variable had a simple lag-1 autoregressive structure. 

Panel B shows the implied stability coefficients for data generated from a model that is consistent with a true cross-lagged process. It is important to note that for this example, I chose values that are likely to underestimate the typical decline in stabilities that emerge with increasing interval length because I set all cross-lagged effects to be .20, an effect that is in the 90th percentile of cross-lagged effects found in a recent meta-analysis of papers that used the CLPM [@orth_effect_2022].[^dgp] Comparing Panel B to Panel A shows that the existence of these cross-lagged associations slows the decline in stability coefficients, and that these differences are most pronounced in variables with high initial stabilities. But how do these compare to patterns of stability that are found in real data?

[^dgp]: For these data, I specified the initial correlation between the two variables to be .5. I also specified that they have equal variances, equal reciprocal effects, and equal one-year stability coefficients. These decisions minimize the decline in these coefficients with increasing interval length. If the "predictor" in this model had weaker stability than the "outcome," or if the cross-lagged path from the outcome to the predictor were weaker than that from the predictor to the outcome, then these stability coefficients would decline more quickly than is represented in this figure. 

As a comparison, I used real longitudinal data to examine the typical pattern of stability coefficients for a wide range of variables. Specifically, I selected ten diverse variables[^variables] that have been included in almost every wave of the long-running Household Income and Labour Dynamics in Australia (HILDA) panel study, which now spans 20 waves of assessment [@watson_hilda_2012]. I intentionally selected variables from different domains, variables those that might have different psychometric properties due to how easily observable they are (e.g., weight and income are likely to be measured with less systematic and random error than life satisfaction or social support), and variables that might change in different ways over time. Stability coefficients across lags ranging from 1 to 10 years are shown in Panel C. 

[^variables]: These variables include weight, average monthly wages, self-reported health status, social support, reports of pain, reports of feeling rushed, life satisfation, commuting time, amount of physical activity, and household income.

A comparison of Panels B and C shows that stability coefficients for these diverse real-world variables decline much more slowly than would be predicted by a lag-1 autoregressive model or even a lag-1 CLPM model with substantial cross-lagged effects (i.e., even when cross-lagged paths that exceed the vast majority of effect sizes reported in the literature). These patterns of stability coefficients suggest that additional sources of stability contribute to these patterns, and stability in these unmeasured variables would invalidate the causal conclusions one would normally draw from the CLPM [@heise_causal_1970]. Thus, the standard CLPM is unlikely to accomplish the goal of estimating between-person causal effects that @orth_testing_2021 articulated[^lag2].

[^lag2]: It is important to note that these results reflect expectations from simple lag-1 autoregressive or cross-lagged models, and that more complex versions of these models can account for the slow decline in stability coefficients shown in Panel C. For instance, adding additional lagged effects from earlier waves or adding covariates from variables reflecting stable influences can help satisfy the assumptions underlying the causal interpretation of the cross-lagged paths [@murayama_thinking_2022; @ludtke_comparison_2022]. However, many implementations of the CLPM fail to include such features, and in these cases, this critical assumption is likely violated. 

## Ambiguity About Between- and Within-Person Effects

The problems with Orth et al.'s [-@orth_testing_2021] argument extends beyond the problems with this assumption. As noted above, their primary concern is that researchers are often not interested in "within-person" causal effects and instead want to assess between-person causal effects. However, their description of these effects suggest that they misinterpret what alternatives to the CLPM do and how the estimates from these models should be interpreted. 

Certain aspects of the between/within distinction are clear and unambiguous. When data are collected from multiple participants at a single point in time, there can only be between-person variance. All associations that can be observed in these data are necessarily between-person associations. For instance, in cross-sectional data, a negative correlation between self-esteem and depression can only be interpreted as a between-person association: People who score high on measures of self-esteem tend to score low on measures of depression. If, on the other hand, just a single individual is assessed repeatedly over time, all variance is within-person variance and all associations would be within-person associations. For example, if a single person's self-esteem and depression were tracked over time, a negative correlation would reflect a within-person association: When self-esteem is high in that individual, feelings of depression tend to be low. 

The potential for confusion about these labels arises, however, when data are collected from multiple people across multiple occasions. Such data include information both about how people differ from one another (between-person variance) and how each person changes over time (within-person variance). Describing associations unambiguously as "between" versus "within" becomes more challenging with these multilevel data. The decision to label an association as "between" or "within" is not always linked in a straightforward way to the type of data that contribute to the effect. 

As @curran_disaggregation_2011 noted, what is probably the most familiar way to separate between-person associations from within-person associations is the use of person-mean centering. For instance, in the context of multilevel modeling, researchers are often warned that if they are not careful about how they enter variables into the model, what may look like within-person associations (e.g., the "Level-1" effects in repeated-measures data) can actually reflect a mix of between and within. The recommended solution in this context is to person-center the predictor [e.g., @curran_disaggregation_2011; @enders2007centering], where each observation now reflects a deviation from a person's mean. When centering this way, the Level-1 part of the model tests whether occasion-specific deviations from a person's mean predict variability in the outcome. 

In their own critique of the RI-CLPM, @ludtke_critique_2021 imply that the RI-CLPM and related models accomplish the separation of between- and within-person associations in the exact same way as the multilevel modeling approach described by @curran_disaggregation_2011[^mlm]. They cautioned that "researchers should be aware that within-person effects are based on person-mean centered (i.e., ipsatized) scores that only capture temporary fluctuations around individual person means" which would be "less appropriate for understanding the potential effects of causes that explain differences between persons" (p. 18). @asendorpf_modeling_2021 echoed this description, stating that models like the RI-CLPM are "similar to using ipsatized scores or person-mean centered scores" (p. 830). Neither clarify, however, that the "person mean" in their description is not the observed person mean calculated from the actual observations (as it would typically be in the traditional multilevel modeling context), but a latent mean that reflects only the variance that is perfectly stable over time. These "means" are---conceptually and empirically---very different things. This also means that what is left after adjusting for these means (the "ipsatized" scores) can also be very different depending on which "mean" is used.

[^mlm]: For discussions about the similarity and differences between the multilevel modeling and structural equation modeling approaches, see @hamaker_fixed_2020 and @falkenstrom_how_2022.

To appreciate this difference, consider the example in Figure \@ref(fig:between). The models shown in this figure represent a single variable, *X*, measured across three occasions. Panel A shows the data-generating process that I used for this example, which reflects a very simple autoregressive model. In this example, the initial variance for *X~1~* was set to 1, and the wave-to-wave stability was set to .5. This simple autoregressive model links between-person differences at Time 1 to between-person differences at Time 2 through a stability coefficient. Note that it would be possible to extend this model to a traditional CLPM by adding an outcome variable at each wave and then testing the lagged paths from the predictor to the outcome and the outcome to the predictor. 

```{r between, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Different ways of conceptualizing between-person variance.', out.width = "100%"}

knitr::include_graphics("images/betweenExample.png")

```

With these data, it is possible (and meaningful) to consider what each person's mean would be across these three occasions. A simple latent-trait model (like the one shown in Panel B) would capture the variance in these means. Note that in this simple latent-trait model, the variance in each wave is partitioned into variance in the person mean (the variance of the common latent trait, which is .48 in this example) and variance in the wave-specific deviations from that mean (.68, .42, and .64). The components of the model in the blue box could be considered the "within-person" part of this model, as these residuals reflect wave-specific deviations from the person mean. The partitioning of variance in this example is quite similar to the descriptions that @ludtke_critique_2021, @orth_testing_2021, and @asendorpf_modeling_2021 provide for the variance partitioning in the RI-CLPM. 

It is the third panel of this figure, however, that most closely represents (in a univariate setting) what the RI-CLPM and related models actually do. In this panel, the residuals have an autoregressive structure, where the residuals for each wave are predicted from the residuals of the wave prior (exactly as they are in the RI-CLPM). When residuals are structured in this way, they capture between-person variance that is somewhat---but usually not perfectly---stable over time. By structuring the residuals to allow for some wave-to-wave stability, the latent trait now includes only the variance that is *perfectly* stable over this time period. The residuals no longer represent deviations from the person mean, they represent deviations from this *perfectly stable trait*[^kou]. Importantly, although the parts of the model included in the blue box could still be considered the "within-person" part of the model, these structured residuals now link meaningful between-person differences (broadly defined) at Time 1 to between-person differences at Time 2. Indeed, in this specific example, because the data-generating process specifies that there is no perfectly stable variance, these "deviations" are identical to the original variables themselves. The parameter estimates from this model almost perfectly recover the values specified in the simple autoregressive data-generating process[^converge]. 

[^kou]: Thanks to Kou Murayama for discussions that clarified this issue. 
[^converge]: Note that this model fit to data with no stable trait variance will often result in inadmissable solutions because with true latent-trait variance of 0, it is possible to get estimates that are negative, as is the case in the data generated for this example. 

A comparison of the variance estimates across Panels B and C show that the latent trait that links the three observations captures something very different in a simple latent-trait model as compared to a model with structured residuals. However, one could describe either the latent trait from Panel B or the one from Panel C as reflecting the "between-person variance" in these assessments. The latent trait in Panel B captures between-person differences in mean levels of *X* during the observed period of assessment; the latent trait in Panel C captures between-person differences in *a hypothetically perfectly stable* trait. Moreover, one could describe the residuals in either model as reflecting the "within-person" part of the model even though they correspond to conceptually different things. The occasion-specific residuals in Panel C do not reflect deviations from the person mean, they reflect deviations from the perfectly stable trait. 

Orth et al. criticize the RI-CLPM for assuming that all between-person differences are perfectly stable over time and for removing all between-person variance from the within-person associations, but those critiques are not correct. In the RI-CLPM and other related models, between-person variance is simply *defined* as the variance that is perfectly stable over time. This is an issue of terminology, not assumptions. Indeed, relying on the terminology of the RI-CLPM, Orth et al.'s preferred CLPM would be said to assume that *no between-person variance exists whatsoever*, an assumption that is rarely if ever defensible in studies of psychological phenomena. Moreover, between-person variance (broadly defined as differences between individuals) is clearly included in the "within-person" part of the model, as Panel C of Figure \@ref(fig:between) shows. It is not necessary to completely remove all between-person variance from an association to isolate what would be described as "within-person effects." Indeed, for variables for which there is no perfectly stable trait variance, the estimates for the cross-lagged paths from the RI-CLPM will be identical to those from the CLPM because the RI-CLPM reduces to the CLPM in such cases. In these cases, what @orth_testing_2021 would describe as a "between-person prospective effect" (the cross-lagged path from *X~1~* to *Y~2~*) is equivalent to what would be described as a purely "within-person effect" in the RI-CLPM. Again, the terminology is the problem. 

Finally, Lüdtke and Robitsch's [-@ludtke_critique_2021] warning that the RI-CLPM would be "less appropriate for understanding the potential effects of causes that explain differences between persons" because the within-person effects it estimates are based on scores that "only capture temporary fluctuations around individual person means" (p. 18) applies as much to the CLPM as to its alternatives: the within-person components of models like RI-CLPM are no more "temporary" or "fluctuating" than are the components of the CLPM, as the CLPM starts with the assumption that no stable-trait variance exists. Thus, arguments like those presented by @orth_testing_2021 rest on an incorrect intepretation of the RI-CLPM and its relation to the CLPM. 

## Alternatives to the RI-CLPM

At this point, it is worth noting that there are alternatives to the models discussed so far that may come closer than the CLPM to capturing what @orth_testing_2021 and @asendorpf_modeling_2021 claim to want to assess, though these, too, have limitations. In discussing their support for the CLPM, neither @orth_testing_2021 nor @asendorpf_modeling_2021 deny that purely stable trait variance exists in the data they typically examine. They object, however, to separating purely stable between-person variance from between-person variance that changes slowly over time in an autoregressive manner when estimating cross-lagged effects. For instance, @asendorpf_modeling_2021 stated (in the context of a substantive example of parental overvaluation affecting the childhood narcissism) that "it is the *chronicity* of overvaluation that makes children narcissistic, and this chronicity is captured only if the full range of between-person differences in chronic overvaluation is taken into account" (p. 831, emphasis in the original). In other words, @asendorpf_modeling_2021 claims to want to know whether the stable part of overvaluation causes narcissism. Although this is certainly an important and theoretically interesting question, longitudinal data do not usually help researchers answer it: If it is truly the stable (chronic) part of this association that is of interest, then longitudinal data would not help establish causality as there would be no change over time to analyze. 

One could imagine, however, a process by which a stable predictor impacts the amount or likelihood of change in an outcome from each wave to the next. Models like the RI-CLPM and STARTS would not be able to capture this causal effect of the stable trait on changes at each wave. There are, however, alternatives that attempt do so while still accounting for stable-trait associations. For instance, @zyphur_data_2020 presented an alternative to the CLPM that they call the *General Cross-Lagged Panel Model*. At first glance, this model looks like the RI-CLPM but with additional change components including moving average and cross-lagged moving average components. @usami_differences_2020 noted, however, that a critical difference between the two is the precise way that the lagged effects are modeled [also see @gische_forecasting_2021, for a similar model; see @usami_within-person_2022 for a more detailed discussion and expanded approach for causal estimation in for data-generating processes that include an accumulating factor]. In the RI-CLPM, the lagged effects are attached to the autoregressive or within-person component, a component that reflects deviations from the stable trait (see Panel B of Figure \@ref(fig:riclpmFig)). In contrast, in the General Cross-Lagged Panel Model, the lagged effects are attached directly to the observed variable, as shown in the simplified version presented in Figure \@ref(fig:gclpmFig). Notice that paths from the stable trait flow through the observed predictor to the observed outcome.

```{r gclpmFig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Diagram of a simplified General Cross-Lagged Panel Model with no moving-average effects.', out.height="90%"}
knitr::include_graphics("images/gclpm.pdf")
```

As a result, the model incorporates an *accumulating* factor [@usami_differences_2020]. This term reflects the fact that influence of the stable trait accumulates over time: the stable trait affects observations at each wave directly and indirectly through all prior waves. Importantly, the cross-lagged paths no longer reflect purely within-person effects, as they now capture the association between the combination of stable trait and wave-specific variability and the outcome. Such a model, which helps account for stable-trait associations, appears to map more clearly on to the assumptions that guide @orth_testing_2021 and @asendorpf_modeling_2021, while avoiding some of the problems of the CLPM that were highlighted in this paper. It is important to note, however, that @usami_differences_2020 showed that by incorporating an accumulating factor, the causal interpretation of the cross-lagged effects is less clear than in models like the RI-CLPM and STARTS. It is beyond the scope of this paper to discuss those complexities, but readers who believe that stable traits impact change in an outcome through these types of accumulating processes are encouraged to investigate these models rather than relying on the CLPM. 


# Beyond Between and Within

In the previous section, I discussed ambiguity in the labeling of between and within-person associations, focusing on how these ambiguities might lead to misinterpretations of models like the RI-CLPM that include a stable trait component. In the next section, I first review previous explanations for why separating these levels of analysis is critical when examining lagged effects. I then propose an alternative way of thinking about the problems with the CLPM that does not rely on an understanding of the distinction between within- and between-person analyses. 

As noted previously, data that have been collected from multiple people across multiple occasions include information both about how people differ from one another (between-person variance) and how each person changes over time (within-person variance). Methodologists have, for many decades, warned that failing to consider multilevel structures can lead to incorrect conclusions [see @curran_disaggregation_2011, for a review and discussion]. It would be wrong, for instance, to draw conclusions about within-person associations from between-person data or to draw conclusions about between-person differences from within-person data because the association can be completely different at these different levels. In addition---and most importantly for debates about the merits of the CLPM---when data do have a multilevel structure, but this multilevel structure is not taken into account through appropriate analytic methods, the estimates obtained from analyses of these data reflect an uninterpretable mix of between and within-person associations [@raudenbush_hierarchical_2002]. 

The traditional CLPM is a classic example of an analysis that fails to separate between-person associations from within. It is quite easy to show that simple correlations at the between-person level can masquerade as within-person associations when the CLPM is used. As a simple demonstration, I generated two waves of data for two variables *X* and *Y*. Panel A of Figure \@ref(fig:spurious) shows the data-generating process, which is a very simple correlated-latent-trait model. I set the variance of *X* and *Y* to be 1, and the reliability of the indicators to be .5. *X* and *Y* are only associated at the between-person level ($r = .7$)[^ambiguities]. In other words, *X* and *Y* are related only because people who tend to score high on *X* on average also tend to score high on *Y* average; *X* does not predict change in *Y* or vice versa and they have no unique associations within any particular wave. Panel B shows what happens if we fit the CLPM to the generated data. As can be seen in this panel, there would be clear evidence for reciprocal associations between the two, even though there are no over-time causal effects of *X* on *Y* (indeed there are no unique over-time associations between *X* and *Y* that aren't accounted for by the associations at the stable-trait level). The CLPM simply cannot distinguish between associations that occur at the stable-trait level from those that involve some change over time.[^both]

[^ambiguities]: Note that the terminological ambiguities from the previous section do not play a role in this simple two-wave example, so the meaning of "between-person" differences should be relatively straightforward and unambiguious in this simple case.

[^both]: Note that it is also not possible to rule out the existence of cross-lagged associations simply by testing the model in Panel A, as this model would fit, even if the data-generating process was that shown in Panel B. 

```{r spurious, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Spurious cross-lagged effects in data with only between-person associations. Panel A is the data-generating model; Panel B shows estimates from the CLPM fit to the generated data. Coefficients are unstandardized estimates. Residuals are not shown but are estimated.'}
knitr::include_graphics("images/clpm.pdf")
```

Although concerns about ignoring multilevel structures emerge any time multilevel data are analyzed, they should be particularly salient when lagged effects are examined. This is because a goal of such lagged analyses is typically to clarify causal processes. Only the within-person part of these models, however, provides information about causal effects. If associations at the two levels are conflated, then drawing causal conclusions from these models would be inappropriate. In short, the primary benefit of the CLPM---its ability to provide evidence for causal effects---would be invalidated by the failure to isolate within-person associations. 

@orth_testing_2021 acknowledge that they do indeed believe that the CLPM estimates causal effects. They state that in the context of their focal case study of self-esteem and depression, "the hypothesized causal effect" in the CLPM can be stated to be that: "when individuals have low self-esteem (relative to others), they will experience a subsequent rank-order increase in depression compared to individuals with high self-esteem" (p. 1014). Although this description of the cross-lagged path is technically correct, @orth_testing_2021 do not go on to explain how this association can be interpreted as a causal effect. No formal causal analysis is presented. Unfortunately, when stable-trait variance exists in the measures being analyzed, then this association is confounded. Referring again to Panel B of Figure \@ref(fig:spurious), it is technically correct to say that the path from *X~1~* to *Y~2~* shows that those who score high on *X~1~* have a rank-order increase in *Y* from Wave 1 to Wave 2, but this is due solely to the confounding effect of the stable trait reflected in the latent *Y* variable. The rank order on *Y* changes from Time 1 to Time 2 not because of any causal effect of *X*, but because the rank order at *Y~1~* imperfectly reflects the true rank order of *Y*, and X is related to this rank ordering. 

Thus, cross-lagged paths in the CLPM can result from purely between-person associations, purely within-person associations, or some combination of the two (even combinations where the within and between-person associations are in the opposite direction). Rather than clarifying how the CLPM solves the uncontroversial interpretational issues that are inevitably involved with this type of analysis, @orth_testing_2021 and @asendorpf_modeling_2021 sidestep this perennial analytic issue. If these authors believe that the CLPM somehow avoids the interpretational challenges inherent in all analyses of multilevel data, then they must do more than simply assert this to be true. Estimates from the CLPM are uninterpretable for all the same reasons that any analysis of multilevel data that fails to account for the multilevel structure are.

It can sometimes be difficult to think about how the complex nature of multilevel data can affect conclusions about underlying processes. Indeed, I worry that framing the discussion of the CLPM solely as an issue of multilevel structure has led to confusion (even though I agree that this conceptualization is technically correct and has led to practical analytic solutions to the problem). Figure \@ref(fig:spurious) helps, however, by providing an alternative way of thinking about the problems with the CLPM and the benefits of alternative models that incorporate a stable-trait component. Models like the RI-CLPM are useful because they test a very plausible alternative explanation of the underlying pattern of correlations that is being modeled when the CLPM is used, an alternative explanation that has nothing to do with causal effects of the predictor on the outcome. 

The logic of the CLPM is very similar to the logic of any other regression model where researchers assess whether one variable predicts another after controlling for relevant confounds. When researchers test whether Time 1 *X* predicts Time 2 *Y* after controlling for Time 1 *Y*, they hope to capture whether there is something unique about *X*---something that cannot be explained by the concurrent association between *X* and *Y*---that helps us predict *Y* at a later time. But as @westfall_statistically_2016 pointed out when discussing the difficulty of establishing incremental predictive validity of any kind, if the measure that they include as a control (i.e., Time 1 *Y*) is not a perfect measure of what researchers are trying to account for, then it is possible---indeed, quite easy---to find spurious "incremental validity" effects. Referring to Figure \@ref(fig:spurious), *Y~1~* is an imperfect measure of the latent variable *Y*. Thus, controlling for *Y~1~* does not control for all of the association between *X* and *Y*, which means that *X~1~* will still have incremental predictive validity of *Y~2~* even after controlling for *Y~1~*. 

In summary, decades of methodological work show the importance of distinguishing between-person associations from within-person associations when data have a multilevel structure. Failing to do so results in uninterpretable estimates of the association between predictors and outcomes. The CLPM is not an exception to this widely discussed rule. In defending the CLPM, @orth_testing_2021 and @asendorpf_modeling_2021 sidestep the issue of how a model that fails to distinguish between levels can lead to interpretable results; instead, they simply assert that the effects from the CLPM are meaningful. @orth_testing_2021 claim to want to test a "between-person prospective effect" but do not define what a between-person prospective effect is and they offer no causal analysis that explains the meaning of such an effect. It is easy to show, however, that when the CLPM is used, it is possible to mistake purely between-person associations for over-time associations, which confirms the long-standing methodological warning about the failure to separate effects at different levels. @asendorpf_modeling_2021 went so far as to assert that using models that include a stable-trait component leads to cross-lagged "effects [that] are severely underestimated" (p. 830). Notably, however, he did not provide any evidence supporting that claim, including either simulation-based evidence from a known data-generating process or mathematical analyses of what these models estimate. Moreover, researchers do not even need to think about these issues in terms of multilevel models and the separation of between-person and within-person effects to appreciate the problems with the CLPM. The CLPM cannot rule out a very plausible alternative explanation for the underlying pattern of correlations. I now turn to a set of simulations that demonstrate just how bad this problem likely is. 

# It's Extremely Easy to Find Spurious Cross-Lagged Effects

The issues discussed in the previous sections show that hypothetically, it is possible to mistake purely between-person associations for causal effects when the CLPM is used. But how likely are such spurious effects? Unfortunately, it is extremely easy to find spurious cross-lagged effects under conditions that are quite likely in the typical situations where the CLPM is used. @hamaker_critique_2015 and @usami_modeling_2019 conducted simulations to show that the estimates from a cross-lagged panel model were often biased in realistic situations. I don't think they went far enough, though, in describing the practical implications of these simulations or showing just how likely spurious effects are in realistic situations. So the rest of this paper builds on their simulations and tries to clarify when such spurious effects are likely to occur. As I show, there are many realistic scenarios where researchers are almost guaranteed to find spurious cross-lagged effects. 

	
## The Simulations

When considering what types of situations to simulate, I focus on realistic scenarios for the types of data to which the CLPM is likely to be applied [@Fraley2005PR]. For instance, it is likely that most variables that psychologists (and other social and health scientists) choose to study over time have a longitudinal structure where stability declines with increasing interval length (reflecting an autoregressive structure), yet this decline approaches or reaches an asymptote where further increases in interval length are no longer associated with declines in stability (reflecting the influence of a stable trait). It is also likely that most measures of psychological constructs have some amount of pure state variance, which could reflect measurement error or true state-like influences. 

I then fit the most inclusive of the three models discussed (the STARTS model) to see how much variance each component accounted for. The proportion of variance accounted for by each of the three components (stable trait, autoregressive trait, and state), along with the stability coefficient for the autoregressive component are shown in Table \@ref(tab:hilda). 

```{r hilda, echo=FALSE, message=FALSE, warning=FALSE}
hilda <- read_csv("saved/hildaDecompTable.csv")
papaja::apa_table(hilda,
                  caption = "Variance components and stability estimates from the STARTS model for 10 variables in the HILDA. The first three columns reflect the proportion of total variance accounted for by that component. The fourth column is the estimated stability of the autoregressive component. All variables were assessed in each of the 20 waves except commuting, which was included in all but Wave 1, and weight, which was included in the 15 most recent waves.")

```

The first thing to note from this table is that the CLPM, which assumes a purely first-order autoregressive structure, would likely be misspecified when applied to any of these variables, as there are substantial stable-trait components for all ten variables[^ar2]. Second, although the size of stable-trait variance component varies across the ten variables, it is often comparable in size and sometimes exceeds the estimates for the autoregressive component that is the focus of the CLPM. Finally, for almost all of the variables that were analyzed, the state component is also quite large, often accounting for one-quarter to one-third of the variance in these measures. These estimates can be used to evaluate the plausibility of the values that I chose for the simulation studies. 

[^ar2]: There are other possible data-generating processes that will lead to the appearance of stable trait variance, including autoregressive effects beyond the first order [@ludtke_critique_2021]. Omitting these higher-order autoregressive effects from a lagged model when they exist will have a similar effect on the cross-lagged paths as the omission of a stable trait factor, and thus, these possibilities are not discussed further as the subtle differences are beyond the scope of this paper. Readers are referred to @ludtke_critique_2021 and @ludtke_comparison_2022 for a more detailed discussion of these issues. 

I used the simulations to test how variation in these factors affects the estimated cross-lagged paths when the CLPM is used. A Shiny app is available where variations of this data-generating model can be specified and the effects on cross-lagged paths can be tested: [http://shinyapps.org/apps/clpm/](http://shinyapps.org/apps/clpm/)[^app]. Readers can use this app to examine the specifications described in the text and to test alternatives. 

[^app]: The app and source code are also available on the corresponding OSF site: [https://osf.io/4qukz/](https://osf.io/4qukz/). In the "Shiny" component, download the "app.R" file and the "scripts" folder and then run it like any other Shiny app. 

Because the focus of this paper is on examining the effects of unmodeled stable trait variance, I set the variance of the stable trait component for the predictor and outcome to be 1 in the primary simulations (though occasionally, I do set stable-trait variance to zero to address specific questions). I then varied the ratio of autoregressive variance to stable-trait variance across four levels: 0, .5, 1, and 2. Similarly, I varied the ratio of non-state to total variance (which would reflect the reliability of the measures if state variance consisted only of measurement error) across three levels: .5, .7, and .9. The results in Table \@ref(tab:hilda) show that these values correspond to what we might find in real data. Finally, I varied the size of the correlation between the stable traits across four levels from weak to very strong: .1, .3, .5, and .7. I ran 1,000 simulations for each of five sample sizes: 50, 100, 250, 500, and 1,000). In all simulations, I set the correlation between the initial autoregressive variance components for the predictor and outcome to be .50 and the stability of the autoregressive components to be .50 (though, later, I discuss some modifications to this). I also set the correlations between state components to be 0. Most importantly, all true cross-lagged paths were set to be 0. Consistent with the canonical STARTS model, I included a stationarity constraint, so that variances, correlations, and stability coefficients are constrained to be equal over time. This constraint is not absolutely necessary, but it simplifies discussion of the estimated cross-lagged paths, as there is just one estimate per model. 

After generating the data, I tested a simple two-wave CLPM, keeping track of the average size of the estimated cross-lagged path coefficients and the number of cross-lagged coefficients that were significant at a level of .05. Note that researchers are often interested in determining which of the two variables in the model has a causal impact on the other rather than on simply testing the effect of one predictor on an outcome. Thus, an effect of *X* on *Y*, *Y* on *X*, or both would often be interpreted as a "hit" in common applications of the CLPM. This means that error rates are typically elevated in the CLPM even without unmodeled stable-trait effects unless corrections for multiple comparisons are used. In these simulations, I report the percentage of runs that result in at least one significant cross-lagged effect (out of two tested), and these can be compared to a baseline error rate of approximately 10%, assuming multiple comparisons are ignored. 

Finally, although I focus on the common two-wave CLPM design, it is important to note that more waves of data lead to increased power to detect smaller effects---even spurious effects. This means that spurious cross-lagged effects are more likely to be found with better, multi-wave designs. Thus, I will also present results from simulations with more waves of data after presenting the primary results. Code used to generate the data, test the models, and run the simulation are available here: [https://osf.io/4qukz/](https://osf.io/4qukz/). All analyses were run using `r papaja::cite_r("r-references.bib", footnote=TRUE)$r`.

`r papaja::cite_r("r-references.bib", footnote=TRUE)$pkgs`

## Simulation Results

The proportion of simulations that resulted in at least one significant (spurious) cross-lagged effect in this initial simulation are presented in Figure \@ref(fig:simFig). The X-axis shows results for different sample sizes. The Y-axis reflects the percentage of runs in which a significant cross-lagged effect was found. The columns reflect variation in the ratio of non-state to total variance of the measures (which is equivalent to the reliability of the measure if state variance only consists of random error). The rows reflect variation in the ratio of autoregressive variance to stable-trait variance. The individual lines in each plot reflect different correlations between the two stable traits. The averaged estimates for the cross-lagged effects in each set of simulations (averaging across sample sizes, as this will not affect the estimated effect) are reported in Table \@ref(tab:simTab). What do these simulations tell us about when spurious effects are likely?

```{r simFig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Simulation results for two-wave CLPM. Columns reflect different ratios of non-state to total variance. Rows reflect different ratios of autoregressive to stable-trait variance. Lines reflect different correlations between stable-trait components. Grey line reflects expected number of significant effects due to chance (assuming a critical p-value of .05).', fig.height=8}

results2 <- readRDS("saved/2WaveSimulation.rds")
## Changes names for plot
names(results2) <- c("N", "r","Ratio Non-State Variance", "Ratio AR Variance", "power", "estimatex","estimatey")
##results2$`Ratio AR Variance` <- factor(paste0((results2$Autoregressive*100),"%"), levels = c("0%", "50%", "100%", "200%"))

results2 %>%
    filter(N>25) %>%
    ggplot(aes(x = N, y = power, group = r)) +
    geom_line(aes(linetype=as.factor(r)),color="black", size=.5) +
    scale_x_continuous(breaks = c(50,100,250,500,1000), labels = c("50","100","250","500","1000")) +
    facet_grid(cols = vars(`Ratio Non-State Variance`),
               rows = vars(`Ratio AR Variance`),
               labeller=label_both) +
    theme_bw() +
    theme(legend.position="bottom",
          axis.text.x = element_text(angle = 90, vjust = .5),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank()) +
    geom_hline(aes(yintercept=.0975), color="gray") +
    labs(linetype = "Correlation Between Stable Traits") +
    ylab("Probability of One or More Significant Spurious Cross-Lagged Effects")


```

### When Constructs Have Some Stable-Trait Structure

If the measures include some amount of stable-trait variance--even if the stable traits are uncorrelated---it is likely that spurious cross-lagged effects will emerge. To be clear, this is most problematic when there is considerable stable-trait variance, when these stable traits are correlated, and when the correlation is quite high. However, error rates are elevated across most simulations. For instance, consider results in the third column of Figure \@ref(fig:simFig), where most of the variance is non-state variance. Specifically, focus on the fourth row, where the ratio of autoregressive variance to stable-trait variance is 2:1. This panel reflects the least problematic set of values tested, and even here, error rates approach 100% when correlations between the stable traits are strong (*r* = .70) and sample sizes are moderately large (*N* = 1,000). Even when correlations are more moderate (e.g., *r* = .5), however, these error rates approach 50% in large samples.

```{r simTab, echo=FALSE, message=FALSE, warning=FALSE}

results2 <- readRDS("saved/2WaveSimulation.rds")
names(results2) <- c("N", "r","Non-State Ratio", "Autoregressive", "power", "estimatex","estimatey")

temp <- results2 %>%
    rowwise() %>%
    mutate(estimate=mean(c(estimatex, estimatey))) %>%
    group_by(r, `Non-State Ratio`, Autoregressive) %>%
    summarize(estimate=mean(estimate)) %>%
    pivot_wider(names_from = `Non-State Ratio`, values_from = estimate)

names(temp) <- c("Stable Trait r", "AR Ratio", "0.5", "0.7", "0.9")

temp[c(2:4,6:8,10:12,14:16),1] <- NA
#temp$`AR Variance Ratio` <- paste0(as.character(temp$`AR Variance Ratio`*100),'%')

papaja::apa_table(temp,
                  midrules=c(4,8,12),
                  align=rep("r", 5),
                  format.args=list(na_string=""),
                  col_spanners=list(`Non-State Ratio`=c(3,5)),
                  caption="Average Estimated Cross-Lagged Paths In Each Simulation Condition")

```

Interestingly, error rates are not always monotonically associated with the size of the correlation between the stable traits. Consider the panels in Rows 2, 3, and 4 of Column 3. In these panels, where state variance is low and the ratio of autoregressive variance to stable-trait variance is .5 or higher, the error rates for the lowest correlation tested (*r* = .1, shown in the solid line) are actually higher than error rates for a higher stable-trait correlation of .3. A look at the actual estimates across simulations in Table \@ref(tab:simTab) provides insight into why this is. The second through fourth rows of the fourth column in this table show that the average estimated cross-lagged effects are actually negative when state variance is low, the correlation between the stable traits is low, and there is a substantial amount of autoregressive variance. These negative estimates emerge even though all associations among the latent components were specified to be positive. 

This can be demonstrated even more clearly by simulating data with uncorrelated stable traits, an equal amount of autoregressive and stable-trait variance, and no state variance whatsoever (this simulation is not shown in the figure or table). In this case, the estimated cross-lagged paths will be approximately -.07. This is due to the fact that by failing to account for the stable trait, the model overestimates the stability of *X* and *Y*, which means that the observed correlation between *X* at Time 1 and *Y* at Time 2 is lower than what would be expected based on the initial correlation between *X* and *Y* at Time 1 and the stability over time[^tracing]. These simulations show that when the variables being examined have a trait-like structure, this can lead to spurious cross-lagged effects, even when the stable trait variance is not correlated. When correlations at the stable-trait level are strong, however, the effects of ignoring the stable-trait structure can be substantial. In some realistic scenarios (e.g., moderate correlations between stable traits, 70% of variance accounted for by non-state components, and sample sizes over 100), significant spurious cross-lagged paths are almost guaranteed. 

[^tracing]: This can be understood by using tracing rules. Randomly generating data for 10,000 participants from the data-generating model just described, the correlation between *X~1~* and *X~2~* and between *Y~1~* and *Y~2~* are both around .75. The correlation between *X~1~* and *Y~1~* would be about .25, and the correlation between *X~1~* and *Y~2~* would be about .12. Fitting a CLPM to these data results in estimated stabilities for *X* and *Y* of approximately .77, and a correlation between *X~1~* and *Y~1~* of .25. These values would imply an observed correlation of  $.77 * .25 = .19$ between *X~1~* and *Y~2~*, which is greater than the actual correlation of .12. This discrepancy between the predicted and observed correlations results in the negative estimates for the cross-lagged paths. 

It is also worth highlighting that the size of the spurious effects (which can also be thought of as the extent of bias in these estimates) shown in Table \@ref(tab:simTab) are consistent with the size of estimated cross-lagged effects typically found in the literature. For instance, @orth_effect_2022 sampled from papers that used the CLPM to determine how large these effects typically are. In their analysis, the 25th, 50th, and 75th percentiles for cross-lagged effects were .03, .07, and .12. Even the largest of these values is similar to the spurious effects found in realistic scenarios from the simulations[^standardized]. 

[^standardized]: The values in the table are average unstandardized coeficients, but given the way that the model is specified, they are mostly equivalent to the standardized estimates aggregated by @orth_effect_2022. 

### When Measures Have Error or Reliable Occasion-Specific Variance

The simulations described above focus on situations where the ratio of non-state to total variance is very high, or in other words, when state variance is low. When there is a lot of state variance, including either reliable state variance or even just measurement error, this effect gets worse---potentially much worse. Consider the panel in the first row and the first column of Figure \@ref(fig:simFig). In this case, the ratio of non-state to total variance is set to .5 and there is no autoregressive variance. Note that values in this range are not unrealistic, because this ratio is reduced both by the existence of measurement error and reliable occasion-specific variance. In this scenario, error rates are very high, approaching 100% with large samples, even when the stable-trait correlation is just .3. Samples of 100 can result in spurious cross-lagged effects approximately 60% of the time when stable traits are correlated .5. Even in samples as small as 50, error rates exceed 25% in many situations. 

This outcome is actually quite easy to understand. Indeed, we don't really need simulations at all to predict it. This result is a simple consequence of the issues that @westfall_statistically_2016 discussed and those that I highlighted in Figure \@ref(fig:spurious). Because the latent *X* and *Y* traits are measured imperfectly at each occasion, controlling for Time 1 *Y* when predicting Time 2 *Y* from Time 1 *X* does not fully account for the true association between *X* and *Y*. There will still be a residual association between Time 1 *X* and Time 2 *Y*, which can be accounted for by the freed cross-lagged path in the CLPM. The RI-CLPM (and the STARTS) are useful because they do a better job accounting for this underlying association than the CLPM. 

One might argue that a model that just includes stable-trait variance and error (which is true of all simulations in the first row of Figure \@ref(fig:simFig)) is unrealistic, as there is sure to be some form of autoregressive structure to most variables that psychologists study. That is true, but as the other rows of the figure show, the existence of this stable trait causes problems for the CLPM even when all three sources of variability (stable trait, autoregressive trait, and state) exist.

```{r measurementError, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

## Simulation Values
nValues <- c(500)
rValues <- c(0)
reliabilities <- c(.8)
arValues <- c(1)

## Run Sim
##
loopRow <- 1
results_noSt <- data.frame(N = numeric(),
                      r = numeric(),
                      reliability = numeric(),
                      AR_Var = numeric(),
                      power = numeric(),
                      estimatex = numeric(),
                      estimatey = numeric())
for (i in 1:length(nValues)) {
    for (j in 1:length(rValues)) {
        for (k in 1:length(reliabilities)) {
            for (l in 1:length(arValues)) {
                nValue <- nValues[i]
                rValue <- rValues[j]
                rel <- reliabilities[k]
                arValue <- arValues[l]
                sims <- data.frame(t(mcreplicate(n=1000, run_sim_clpm(waves = 2,
                                                                      studyN=nValue,      # N to generate
                                                                      ri_x=0,     # Random intercept variance for X
                                                                      ri_y=0,     # Random intercept variance for Y
                                                                      cor_i=rValue,   # Correlation between intercepts
                                                                      x=arValue,        # AR variance for X
                                                                      y=arValue,        # AR variance for Y
                                                                      stab_x=.5,  # Stability of X
                                                                      stab_y=.5,  # Stability of Y
                                                                      yx=0,      # Cross lag (Y on X)
                                                                      xy=0,      # Cross lag (X on Y)
                                                                      cor_xy=.5,  # Correlation between X and Y
                                                                      reliability_x=rel,       # Measurement error for X
                                                                      reliability_y=rel       # Measurement error for Y
                                                                      ), mc.cores=14)))
                results_noSt[loopRow,1] <- nValue
                results_noSt[loopRow,2] <- rValue
                results_noSt[loopRow,3] <- rel
                results_noSt[loopRow,4] <- arValue
                results_noSt[loopRow,5] <- sum(sims$X2<.05 | sims$X4<.05)/1000
                results_noSt[loopRow,6] <- mean(sims$X1)
                results_noSt[loopRow,7] <- mean(sims$X3)
                loopRow <- loopRow+1
            }
        }
    }
}


```

At this point, it is important to highlight the fact that at least some of these effects are due more to the existence of measurement error (or reliable state variance) than to the existence of the stable trait. For instance, it is possible to simulate data with an autoregressive structure, to set the variance of the stable trait components to be 0, and to specify no cross-lagged paths. Even with a relatively high ratio of non-state to total variance (e.g., .8 for this simulation, which is not shown in the figure), the average estimated cross-lagged paths would be `r mean(results_noSt$estimatex, results_noSt$estimatey)` and spurious effects would be found `r round(results_noSt$power * 100, 0)`% of the time in a two-wave design with samples of 500 participants. Again, Westfall and Yarkoni's  [-@westfall_statistically_2016] explanation can account for these results: The existence of measurement error or state variance in the observed measures of *Y* means that controlling for *Y~1~* does not control for enough. The result is a spurious cross-lagged effect. 

```{r measurementErrorRI, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

## Actual Simulation Values
nValues <- c(500)
rValues <- c(.5)
reliabilities <- c(.8)
arValues <- c(1)

## Run Sim
##
loopRow <- 1
results_ri <- data.frame(N = numeric(),
                      r = numeric(),
                      reliability = numeric(),
                      AR_Var = numeric(),
                      power = numeric(),
                      estimatex = numeric(),
                      estimatey = numeric())
for (i in 1:length(nValues)) {
    for (j in 1:length(rValues)) {
        for (k in 1:length(reliabilities)) {
            for (l in 1:length(arValues)) {
                nValue <- nValues[i]
                rValue <- rValues[j]
                rel <- reliabilities[k]
                arValue <- arValues[l]
                sims <- data.frame(t(mcreplicate(n=1000, run_sim_riclpm(waves = 10,
                                                                      studyN=nValue,      # N to generate
                                                                      ri_x=1,     # Random intercept variance for X
                                                                      ri_y=1,     # Random intercept variance for Y
                                                                      cor_i=rValue,   # Correlation between intercepts
                                                                      x=arValue,        # AR variance for X
                                                                      y=arValue,        # AR variance for Y
                                                                      stab_x=.5,  # Stability of X
                                                                      stab_y=.5,  # Stability of Y
                                                                      yx=0,      # Cross lag (Y on X)
                                                                      xy=0,      # Cross lag (X on Y)
                                                                      cor_xy=.5,  # Correlation between X and Y
                                                                      reliability_x=rel,       # Measurement error for X
                                                                      reliability_y=rel       # Measurement error for Y
                                                                      ), mc.cores=14)))
                results_ri[loopRow,1] <- nValue
                results_ri[loopRow,2] <- rValue
                results_ri[loopRow,3] <- rel
                results_ri[loopRow,4] <- arValue
                results_ri[loopRow,5] <- sum(sims$pvalue<.05 | sims$pvalue.1<.05, na.rm=TRUE)/(1000-(sum(is.na(sims$pvalue))))
                results_ri[loopRow,6] <- mean(sims$est)
                results_ri[loopRow,7] <- mean(sims$est)
                loopRow <- loopRow+1
            }
        }
    }
}


```

It is also important to note that measurement error and reliable state variance also affect estimates from the RI-CLPM. If we specify a data-generating process that includes all three sources of variance (stable trait, autoregressive trait, and state/measurement error), but no cross-lagged paths, the CLPM will find substantial cross-lagged effects, but so will the RI-CLPM (at least if the autoregressive components of *X* and *Y* are correlated). To demonstrate, I simulated data with the following characteristics. The *X* and *Y* stable traits had variances of 1 and a correlation of .5 and *X* and *Y* autoregressive traits had a variance of 1 and a starting correlation of .5 with stability coefficients of .5. The average estimated cross-lagged effect was `r mean(results_ri$estimatex, results_ri$estimatey)`, which would be easily detectable with moderate to large sample sizes and is similar in size to the typical cross-lagged effect found in the literature [@orth_effect_2022]. 

To examine this issue more systematically and to compare the likelihood of finding spurious effects when using the RI-CLPM to the likelihood when using the CLPM, I repeated the primary simulation using the RI-CLPM. Because the estimates of the cross-lagged paths are not affected by the size of the correlation between the stable trait components when the RI-CLPM is used, instead of varying the correlation between stable traits, I varied the correlation between the initial wave autoregressive components. In addition, the RI-CLPM requires three waves of data instead of the two that I used in the initial simulation. The results are shown in Figure \@ref(fig:riSpurious). 

```{r riSpurious, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Simulation results for three-wave RI-CLPM. Columns reflect different ratios of non-state to total variance. Rows reflect different ratios of autoregressive to stable-trait variance. Lines reflect different correlations between autoregressive-trait components. Grey line reflects expected number of significant effects due to chance (assuming a critical p-value of .05).', fig.height=8}

results <- readRDS("saved/riSimulation3_rAr.rds")
## Changes names for plot
names(results) <- c("N", "r","Ratio Non-State Variance", "Ratio AR Variance", "power", "estimatex","estimatey", "problems")

ggplot(data = results, aes(x = N, y = power, group = r)) +
    geom_line(aes(linetype=as.factor(r)),color="black", size=.5) +
    scale_x_continuous(breaks = c(50,100,250,500,1000)) + expand_limits(y=c(0,1)) +
    facet_grid(cols = vars(`Ratio Non-State Variance`),
               rows = vars(`Ratio AR Variance`),
               labeller=label_both) +
    ## ##    geom_text_repel(data=labels, aes(label=r), size=3) +
    theme_bw() +
    theme(legend.position="bottom",
          axis.text.x = element_text(angle=90, vjust = .5),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank()) +
    geom_hline(aes(yintercept=.0975), color="gray") +
    labs(linetype = "Correlation Between AR Traits") +
    ylab("Probability of One or More Significant Spurious Cross-Lagged Effects")

```

As can be seen in this figure, when there is measurement error variance or reliable state variance, then there is a chance for spurious cross-lagged effects even when the RI-CLPM is used. To be sure, these effects are much less likely than with the CLPM: Error rates typically only exceeded 25% with large samples and very strong correlations between the autoregressive traits, whereas they often approached 100% with the CLPM. Note, this limitation of the RI-CLPM is not an argument *for* the CLPM (though it is an argument for using the STARTS or other more complicated models, when possible). 

One response to the above simulations is to suggest that researchers simply need to use very reliable measures or perhaps model latent variables at each occasion instead of relying on observed variables with less than perfect reliability. This will certainly help, but it is important to remember that the "state" component in the STARTS model includes measurement error *and* reliable occasion-specific variance. Reliable state variance will affect these results in exactly the same way as random measurement error. Unfortunately, researchers don't know how common this reliable state component is in real data, though there is at least some evidence that it can exist and be large enough to be meaningful [@anusic_dependability_2012; @LucasInPressSIR]. Thus, even the use of latent occasions in the CLPM can't solve this problem. 

### When There Are Many Assessment Waves

Although the CLPM is often used with just two waves of assessment, it can also be used with more complex data. Indeed, a general rule for longitudinal data is that more waves are better than fewer, and in situations where stationarity could reasonably be expected, including more waves and imposing equality constraints should lead to more precise estimates of cross-lagged paths. When estimating true effects, this has the benefit of increasing power. When spurious effects would be expected, however, the use of more waves will also increase the probability of those spurious effects being significant [again, see @westfall_statistically_2016, for a discussion of how factors that improve power can increase the ability to find spurious effects]. 

```{r simFig5, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Simulation results for five-wave CLPM. Columns reflect different reliabilities. Rows reflect different ratios of autoregressive to stable-trait variance. Lines reflect different correlations between stable-trait components. Grey line reflects expected number of significant effects due to chance (assuming a critical p-value of .05).', fig.height=8}

results5 <- readRDS("saved/5WaveSimulation.rds")
## Changes names for plot
names(results5) <- c("N", "r","Ratio Non-State Variance", "Ratio AR Variance", "power", "estimatex","estimatey")
## results5$`Ratio AR Variance` <- factor(paste0((results5$`Autoregressive`*100),"%"), levels = c("0%", "50%", "100%", "200%"))

results5 %>%
    filter(N>25) %>%
    ggplot(aes(x = N, y = power, group = r)) +
    geom_line(aes(linetype=as.factor(r)),color="black", size=.5) +
    scale_x_continuous(breaks = c(50,100,250,500,1000), labels = c("50", "100","250","500","1000")) +
    facet_grid(cols = vars(`Ratio Non-State Variance`),
               rows = vars(`Ratio AR Variance`),
               labeller=label_both) +
    theme_bw() +
    theme(legend.position="bottom",
          axis.text.x = element_text(angle = 90, vjust = .5),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank()) +
    geom_hline(aes(yintercept=.0975), color="gray") +
    labs(linetype = "Correlation Between Stable Traits") +
    ylab("Probability of One or More Significant Spurious Cross-Lagged Effects")

inTextResults <- results5
names(inTextResults) <- names(results5) <- c("N", "r","Reliability", "autoregressive", "power", "estimatex","estimatey")


```

Figure \@ref(fig:simFig5) shows a set of simulations that are similar to those reported in Figure \@ref(fig:simFig), but this time using five waves of data and the CLPM with equality constraints across waves. When comparing these two figures, the effect of increasing the number of waves is immediately apparent: Error rates increase considerably. For instance, in the very realistic scenario of an N of 250, a correlation between stable traits of .5, non-state variance ratio of .7, and a 1:1 ratio of stable-trait to autoregressive variance, the error rate increases from `r round(results2[results2$N==250&results2$r==.5&results2$"Non-State Ratio"==.70&results2$Autoregressive==1,5]*100, 0)`% to `r round(inTextResults[inTextResults$N==250&inTextResults$r==.5&inTextResults$Reliability==.7&inTextResults$autoregressive==1,5]*100,0)`% when moving from a two-wave study to a five-wave study. With five waves of data, error rates often exceed 50%, even in samples as small as 50. So features that are generally desirable---large sample sizes and multiple waves of assessment---increase the likelihood of finding spurious cross-lagged effects. 

## Modeling Stable Traits Is Not Conservative

The examples above focused on cases where there were no true cross-lagged effects in the data-generating model. The simulations showed that spurious effects are often very likely to be found. This pattern matches the intuition that models like the the RI-CLPM and STARTS (which include a stable-trait component) are more conservative than the CLPM [@asendorpf_modeling_2021; @ludtke_critique_2021]. However, failure to model associations between stable-trait components can also lead to the *underestimation* of real cross-lagged effects. For instance, consider a situation where the measures are perfectly reliable (and there is no reliable state variance) and the stable trait and autoregressive trait contribute equally (in this particular case, I also specified the correlations among the stable trait and autoregressive traits to be .5). If we simulate data with cross lagged paths of .5 from *X* to *Y* and .2 from *Y* to *X*, the RI-CLPM reproduces these effects perfectly. However, even with no measurement error, the estimates from the CLPM are half the size that they should be, approximately .25 and .10. 

The precise manner in which estimates will be affected depends on the size of these variance components and the correlations between them. Table \@ref(tab:truePaths) shows the results from a separate simulation that examines these effects. Specifically, because the parameter estimates were the focus (rather than the frequency of errors), I followed @ludtke_critique_2021 and generated just one set of 10,000 responses for each of 48 combinations. I varied the correlation between the stable traits across four levels: .1, .3, .5, and .7. Similarly, I varied the correlation between the autoregressive traits across the same four levels. I also varied the ratio of autoregressive to trait variance across three levels: .5, 1, and 2. For this example, state variance was set to be 0 and the stability of the autoregressive components were set to .5. The table only shows results for one cross-lagged path, for which the true value is .50. 


```{r truePaths, echo=FALSE, message=FALSE, warning=FALSE}

################################################################################
## 2-Wave Simulation with True CL Paths
################################################################################

## Actual Simulation Values
nValues <- c(10000)
rValues <- c(.1,.3,.5,.7)
ar_corValues <- c(.1, .3, .5, .7)
reliabilities <- c(.7, 1)
arValues <- c(.5, 1, 2)

## Run Sim
##
loopRow <- 1
results <- data.frame(ar_corValue = numeric(),
                      r = numeric(),
                      reliability = numeric(),
                      AR_Var = numeric(),
                      estimatex = numeric(),
                      estimatey = numeric())
for (i in 1:length(ar_corValues)) {
    for (j in 1:length(rValues)) {
        for (k in 1:length(reliabilities)) {
            for (l in 1:length(arValues)) {
                ar_corValue <- ar_corValues[i]
                rValue <- rValues[j]
                rel <- reliabilities[k]
                arValue <- arValues[l]
                sims <- run_sim_clpm(waves = 2,
                                     studyN=10000,      # N to generate
                                     ri_x=1,     # Random intercept variance for X
                                     ri_y=1,     # Random intercept variance for Y
                                     cor_i=rValue,   # Correlation between intercepts
                                     x=arValue,        # AR variance for X
                                     y=arValue,        # AR variance for Y
                                     stab_x=.5,  # Stability of X
                                     stab_y=.5,  # Stability of Y
                                     yx=.5,      # Cross lag (Y on X)
                                     xy=.2,      # Cross lag (X on Y)
                                     cor_xy=ar_corValue,  # Correlation between X and Y
                                     reliability_x=rel,       # Measurement error for X
                                     reliability_y=rel       # Measurement error for Y
                                     )
                results[loopRow,1] <- ar_corValue
                results[loopRow,2] <- rValue
                results[loopRow,3] <- rel
                results[loopRow,4] <- arValue
                results[loopRow,5] <- sims[[1]]
                results[loopRow,6] <- sims[[3]]
                loopRow <- loopRow+1
            }
        }
    }
}

## Changes names for Table
names(results) <- c("rAR", "rST","Reliability", "Autoregressive", "estimate_x","estimate_y")

temp <- results %>%
    filter(Reliability==1) %>%
    select(-estimate_x, -Reliability) %>%
    pivot_wider(names_from = rST, values_from = estimate_y)


names(temp) <- c("AR r", "AR Ratio", "0.1", "0.3", "0.5", "0.7")

papaja::apa_table(temp,
                  midrules=c(3,6,9),
                  align=rep("r", 10),
                  col_spanners=list("Stable Trait Correlation"=c(3,6)),
                  caption="Estimated Cross-Lagged Path In Each Simulation Condition When True Value = .5.",
                  note="AR r = Correlation between autoregressive components; AR Ratio = Ratio of autoregressive variance to stable-trait variance. State variance is set to 0 for all simulations.")

```

First, consider the example just discussed. Looking at the column where the correlation between the stable traits is .5, and the row where the correlation between the autoregressive traits is .5 and the ratio of autoregressive variance to stable-trait variance is 1, the true cross-lagged effect of .5 is estimated to be .25. One can then move up and down that column or across that row to see the effects of the other factors on this underestimation. For instance, looking at the values in the rows immediately above and below this value shows that the underestimation of the cross-lagged effects is greater when there is more stable trait variance than when there is less. The true cross-lagged effect of .50 is estimated to be .16 when there is twice as much stable trait variance as autoregressive variance, whereas it is estimated to be .33 (still an underestimate, but not as bad), when there is twice as much autoregressive variance as stable-trait variance. 

Moving across the same row shows how this estimate is affected by variation in the correlation between stable traits. As can be seen, the estimate for a true cross-lagged effect of .50 declines from .31 when the correlation between the stable traits is a high .70 to .25 when the correlation is .50, to .21 when the correlation is .30, to .18 when the correlation is 0.1. Again, stable trait variance affects estimates of cross-lagged effects even when the stable traits are weakly correlated or uncorrelated. 

Finally, moving across groups of rows (e.g., Rows 1 through 3 compared to Rows 4 through 6) shows the effect of the correlation between autoregressive components. In this case, the estimate of the cross-lagged parameter is *negatively* associated with the size of the correlation between autoregressive components, declining from .33 when the correlation between autoregressive components is .10 to .19 when the correlation is .70 (again, for the example where the stable-trait correlation is .5 and there is an equal amount of autoregressive and stable-trait variance). 

These simulations show that including a stable-trait component is not more conservative than excluding it when testing cross-lagged effects. Indeed, when true cross-lagged effects exist, the CLPM is likely to underestimate them when there is stable-trait variance. Again, this pattern is predictable just by considering tracing rules for structural equation models. When stable-trait variance exists, the stability of the observed variables is overestimated in a CLPM, resulting in a corresponding underestimation of the cross-lagged paths in most situations. If researchers observe a pattern where cross-lagged effects routinely emerge when the CLPM is used but disappear when a stable trait is modeled, then this would suggest that the effects themselves are likely spurious. 

When considering the implications of this finding, there are two things to keep in mind. First, although the CLPM typically underestimates the true cross-lagged effects, this may not result in decreased power to detect these effects relative to alternative models like the RI-CLPM. This is partly due to the fact that models that separate between-person associations from within frequently have less bias, but at the cost of efficiency; the standard errors in such models are often greater than models that do not correctly separate levels [see @allison2009fixed, for an explanation; see @usami_modeling_2019 for similar examples]. Second, it is difficult to consider how to think about power in the context of the CLPM, where it is so easy to find spurious effects. Indeed, I ran simulations similar to those described earlier, but with true cross-lagged effects. However, when simulating data where one cross-lagged path was zero and the other was greater than zero, estimated "power" to detect both the true effect and the spurious one were quite high. 

The second thing to remember is that the above simulations were conducted specifying that the measures are perfectly reliable and that there is no occasion-specific state variance. This is unlikely in practice. Indeed, when state variance is included, the estimates from the RI-CLPM are also biased. The precise way that state variance impacts estimates is a quite complicated function of all the factors included in the previous simulations, the state factor, and the sign of the correlations and true cross-lagged paths. Because of this complexity, these simulations are beyond the scope of this paper, though the Shiny app is provided for readers to examine the effect of different combinations on estimated cross-lagged paths. Importantly, the STARTS model is appropriate for modeling data that includes stable-trait, autoregressive-trait, and state variance. Although the STARTS model has been somewhat underused in the literature because of frequent estimation problems, recent methodological advances in Bayesian modeling have helped address these concerns [@ludtke_more_2018]. 

# Moving Forward With (or Without) the CLPM

After describing the various approaches available to model longitudinal data, @orth_testing_2021 made the following recommendation: "Before selecting a model, researchers should carefully consider the psychological or developmental process they would like to examine in their research, and then select a model that best estimates that process." There are two problems, however, when using this guidance to advocate for the CLPM. First, the CLPM is explicitly and inarguably a model of constructs that change over time. Specifically, the CLPM is used to estimate reciprocal associations between two (or more) variables *that have a first-order autoregressive structure*. The model is misspecified when used to describe variables that have a stable-trait structure. Of course, it is a truism to say that "all models are wrong," but it is rare to select a model that is known to be wrong in such a fundamental way, especially when more appropriate models exist. The mathematics of the model, along with the simulations provided in this and other papers show that this clear misspecification can frequently lead to spurious effects in realistic scenarios. So, if researchers' careful consideration of the psychological and developmental processes under examination leaves open the possibility that the variables have some stable-trait structure, then the CLPM will be the wrong choice.

Even if we ignore the misspecification involved when applying the CLPM to data with a stable-trait structure, there is an additional problem with Orth et al.'s [-@orth_testing_2021] guidance. If there is a plausible alternative model that describes the data as well as (or better than) the preferred model, then additional work is needed to defend that preferred model. As an obvious example (using the substantive question that motivated Orth et al.'s analysis), if the true causal process linking self-esteem to depression is that changes in self-esteem instantaneously cause a corresponding change in depression (and there are no confounding factors), then that causal effect would be perfectly captured by the cross-sectional correlation between the two variables. Indeed, it would actually be problematic to rely on the cross-lagged association between self-esteem and depression controlling for earlier levels of depression as an estimate of the causal effect. Yet few would find a cross-sectional correlation between self-esteem and depression to be compelling evidence for a causal effect of self-esteem, even if a researcher's preferred model only predicted such cross-sectional associations. This is because there are so many plausible alternative models that explain that cross-sectional effect. Unfortunately, the situation is no better with longitudinal data tested using the CLPM. The CLPM can not rule out the plausible alternative explanation that cross-lagged paths are due to simple between-person correlations. 

When researchers acquire data from multiple people on multiple occasions, those data have a multilevel structure; this point is uncontroversial. It is also uncontroversial to note that when multilevel data are analyzed using analytic approaches that do not separate these levels, the estimates from those models reflect an uninterpretable mix of between and within-person associations. This concern is especially problematic in the analysis of lagged effects precisely because only one level of analysis---the within-person level---can inform causal conclusions. Perhaps there is some reason why this long-standing and well-documented concern about the analysis of multilevel data does not apply in the case of the CLPM, but neither @orth_testing_2021 nor @asendorpf_modeling_2021 articulated such a reason. Indeed, they did not acknowledge this problem or discuss how the CLPM solves it. The CLPM simply cannot distinguish between the "between-person prospective effects" @orth_testing_2021 claim to want to test and simple between-person differences.  

It is also noteworthy that although @asendorpf_modeling_2021 unequivocally asserted that cross-lagged effects are "severely underestimated" when models that include a stable trait are used, he provided no evidence to support this claim. Neither he nor @orth_testing_2021 proposed a data-generating process that can be shown to lead to correct results when modeled using the CLPM but that would result in biased estimates or incorrect conclusions if modeled using more modern alternatives. Generating data from the model that these authors say should often be preferred---the CLPM---results in correct estimates (despite the possibility of inadmissable solutions due to negative estimated variances) when analyzed using the RI-CLPM or STARTS. Note that @ludtke_critique_2021 did identify data-generating processes that lead to biased estimates when the resulting data are analyzed using the RI-CLPM, but their results do not show that the CLPM is more appropriate than the RI-CLPM in these situations[^wrong]. 

[^wrong]: Specifically, @ludtke_critique_2021 showed that the RI-CLPM cannot successfully account for all types of person-level confounds. It is, of course, possible to specify data-generating processes that do result in data that, when analyzed using the RI-CLPM, lead to incorrect conclusions [including the more complex models described by @usami_unified_2019]. However, critics of the RI-CLPM and related models have not provided a model that matches the processes that they describe, that results in correct estimates when modeled using the CLPM, but also results in incorrect estimates when modeled using the RI-CLPM. 


In addition to the conceptual reasons for abandoning the CLPM that I discussed in this paper, the simulations show that relying on the CLPM when the variables in the model have some stable trait variance leads to dramatically inflated error rates (often reaching 100% in realistic scenarios, even with small to moderate sample sizes). The constructs that psychologists study very rarely have a purely autoregressive structure. At some point, the long-term stabilities of most constructs are stronger than would be suggested by the short-term stabilities and the length of time that has elapsed alone. This is likely due to the fact that many constructs have at least some stable-trait variance that is maintained over time (even if that stable-trait variance reflects something like response styles or other method factors). And if there is stable trait variance, it is quite plausible that two constructs correlate at the stable-trait level. Models like the RI-CLPM and STARTS provide a way to test this compelling and plausible model and to adjust for the problematic effects of these stable-trait components when testing reciprocal effects. 

So, if using the CLPM results in dramatically elevated error rates for cross-lagged effects when they do not exist, while also underestimating estimates of these effects when they do exist (again, sometimes dramatically in realistic scenarios), should we ever rely on the CLPM for causal analyses? Given these concerns, it would seem prudent to abandon the use of the CLPM for causal analysis of longitudinal data. When at least three waves of data are available, the RI-CLPM can be used. If no stable trait variance exists, the RI-CLPM will simply reduce to a CLPM. 

To be sure, there are certain situations when alternatives to the CLPM cannot be used, most notably in the very common situation where only two waves of assessment are available. This creates an important challenge for the field, as most studies that use the CLPM only have two waves of data available [@hamaker_critique_2015; @usami_modeling_2019]. However, as Figure \@ref(fig:spurious) showed, there are too many plausible alternative models that can lead to the same set of six correlations among two variables at two time points to draw any conclusions about which of those models is correct. Many have suggested that two-wave designs barely count as "longitudinal" [e.g., @ployhart_two_2014; @rogosa_myths_1995], and these authors' reasons for this claim are quite salient when considering the application of the CLPM to such data. There is simply not enough information available in these data to distinguish among multiple competing models. 

In discussing reciprocal associations using cross-lagged analyses, @rogosa_myths_1995 noted that there is a "hierarchy of research questions about longitudinal data [that] might start with describing how a single attribute---say aggression---changes over time. A next step would be questions about individual differences in change of aggression over time, especially correlates of change in aggression. Only after such questions are well understood does it seem reasonable to address a question about feedback or reciprocal effects, such as how change in aggression relates to change in exposure to TV violence or, does TV violence cause aggressive behavior?" (p. 34). Many researchers have noted that this first step---describing how a construct changes over time---is not possible with only two waves of data [e.g., @Fraley2005PR; @ployhart_two_2014; @rogosa_myths_1995]. 

Given the ubiquity of two-wave designs in psychological and medical research [@usami_modeling_2019], an inevitable question is "what should we do with studies that have only two waves of data?" Longitudinal data are often difficult and expensive to collect, and researchers understandably want to get the most out of the data that they have spent so much time (and often money) to collect. Personally, given the issues discussed in this paper, I believe that the CLPM is almost always the wrong choice, even when it's the only choice. However, I recognize that not everyone will agree, and therefore, it is worthwhile to consider what else can be done to minimize the problems of the CLPM. One step in this direction would be for researchers who use the CLPM with two-wave to explicitly discuss the plausibility of the assumptions that underlie it (i.e., that no stable-trait variance exists in the measures that have been assessed).

For instance, with two waves of data, it is impossible to distinguish between a simple correlated-latent-trait model and a model with no stable trait and cross-lagged effects (i.e., the competing models shown in Figure \@ref(fig:spurious)). One way that researchers could approach this dilemma would be to fit and show results for both models and then describe any theory or prior empirical evidence that supports the acceptance of one model over the other. This would require researchers to be explicit that something *beyond the data themselves* is required to evaluate the plausibility of their chosen model. As @grosz_taboo_2020 and others have noted, making these assumptions more transparent and subject to evaluation by readers would be beneficial to the scientific process. Editors or reviewers could even require such a discussion for papers that use the CLPM with two-wave data. Importantly, these editors and reviewers should critically evaluate the evidence for and plausibility of these underlying assumptions when judging the accuracy of authors' claims about their own data. As a reminder, the critical assumption that must hold for the CLPM to provide unbiased estimates of the causal effect is that there is no stable trait variance. This would be a challenging assumption to defend. 

Consideration of these assumptions may also lead researchers to consider other alternatives to the CLPM. For instance, @kim_gain_2021 noted that although the use of simple difference scores as outcomes is often discouraged in longitudinal research, this approach actually has some benefits for causal inference over approaches (like the CLPM) that control for initial scores on an outcome to estimate causal effects. Indeed, this difference-score (or gain-score) approach can potentially be useful in the precise situation that is the focus of this paper: when stable-trait variance exists. However, this approach relies on a different assumption that is also quite likely to be violated in data that psychologists typically analyze (especially in data that would otherwise be appropriate for the CLPM), namely that initial standing on the outcome does not affect later standing. So, alternatives to the CLPM that require just two waves of data do exist, but they require careful justification and rely on assumptions that may be implausible for much of the data that would otherwise be analyzed using the CLPM. 

I have heard two additional responses to the suggestion that researchers abandon the CLPM. Both involve the use of the model for purposes other than estimating causal effects. For instance, one possibility would be for researchers to use the CLPM in a purely descriptive manner, where no causal conclusions are drawn. To be sure, there are many situations where the type of data that is used as input to the CLPM can be used descriptively. For instance, simply testing whether a cross-sectional association between two variables holds when one of the two variables is assessed at a later time point helps rule out the possibility that the cross-sectional association is due simply to occasion-specific factors (like current mood)[^causal]. The CLPM goes beyond examining these zero-order correlations, however, to examine the association between a predictor and an outcome after controlling for prior standing on that outcome. As @wysocki_statistical_2022 recently noted, "statistical control requires causal justification" [p. 1, also see @grosz_taboo_2020]. In other words, as soon as researchers choose to control for prior standing on the outcome, they are committing to an implied causal justification for the inclusion of that control variable; simply labeling the analysis as "descriptive" does not absolve researchers of that commitment. 

[^causal]: Technically, this is also a causal question, as the researcher would be ruling out current mood as a confounding factor in the association between the predictor and outcome. But what the researcher is left with---the simple correlation between two variables---could only be used for descriptive analyses. 

As @grosz_taboo_2020 note, labeling analyses as descriptive often reflects a general (and they argue, problematic) taboo against causal interpretations of results from observational data. They note that in such cases, "the taboo does not prevent researchers from interpreting findings as causal effects---the [causal] inference is simply made implicitly, and assumptions remain unarticulated" (p. 1243). So, although the type of data that can be modeled using the CLPM can certainly be used descriptively, application of the CLPM usually involve an implicit commitment to a causal interpretation. Because of this, it seems unlikely (though of course, not impossible) that the CLPM will be used in a truly descriptive manner.

The second suggested use-case for the CLPM is in research focused solely on prediction. Unfortunately, researchers also use the term "predict" in different ways, and not all predictive goals avoid the problems discussed in this paper. For instance, the term "predict" is often used as a seemingly more acceptable word than "cause," "influence," or "affect" when authors do not want to run afoul of editors and reviewers who endorse the taboo on causal language when describing analyses involving observational data [@hernan_c-word_2018; @grosz_taboo_2020]. For this type of "prediction," the problems described in this paper still hold. 

Of course, there are research programs where the goal truly is prediction rather than explanation. As @yarkoni_choosing_2017 note, in psychology, prediction has historically been downplayed in favor of explanation, and these authors argue that predictive research should play more of a role in the field. The goal of such research is often quite different from explanatory research, and success is determined simply by how well a model allows researchers to predict scores on an outcome. For instance, clinical researchers may develop a predictive model of depression that includes measures of self-esteem, not because they are interested in developing interventions that capitalize on the causal impact of self-esteem on depression, but simply because they know that people who have low self-esteem right now are more likely to develop depression in the future. In other words, if self-esteem predicts future depression, then screening tools that focus on self-esteem might be effective, regardless of whether there is a causal association. This type of prediction-focused research has, historically been used most often in applied settings [@yarkoni_choosing_2017]. 

It is important to note, however, that when used in this way, the analysis would be barely recognizable as a CLPM. For one thing, researchers using this type of predictive approach would be unlikely to be interested in the reciprocal effects that a cross-lagged model typically tests if prediction of a particular outcome was the goal. More importantly, because the focus is on maximizing prediction, methods often involve machine learning and big data, which are designed to maximize prediction, regardless of the extent to which they map on to a comprehensible theoretical model [@yarkoni_choosing_2017]. Thus, it seems unlikely that standard implementations of the CLPM in psychological research reflect actual examples of research where the goal is prediction. 

## Conclusion

All researchers want to obtain information about the phenomena they study as efficiently as possible. The widely used CLPM is a simple analysis that can be applied in many situation with very little data. Unfortunately, this simple model is not up to the task of clarifying causal processes in longitudinal data. By failing to separate between-person and within-person levels, the CLPM cannot distinguish over-time causal effects from simple between-person associations. Simulations show that the CLPM results in extremely elevated error rates when stable-trait (or state) variance exists; spurious associations are very likely in many different realistic scenarios. This confirms what methodologists have highlighted for many years: By failing to account for the multilevel structure of longitudinal data, models like the CLPM result in uninterpretable estimates. @berry_practical_2017 suggested that it was time to rethink the CLPM, which they described as a workhorse of developmental research. I concur that the introduction of useful alternatives like the RI-CLPM and STARTS, when combined with the demonstrable problems with the CLPM, show that it is time for this workhorse to be retired. 



# Disclosures

## Author Contributions

Richard E. Lucas was responsible for all contributions, including conceptualization, methodology, formal analysis, and writing. 

## Conflicts of Interest

The author declares that there were no conflicts of interest with respect to the authorship or the publication of this article. 

## Prior Versions

A preprint of this paper was posted on the PsyArXiv preprint server: https://psyarxiv.com/pkec7/ .

## Data Usage

This paper uses unit record data from Household, Income and Labour Dynamics in Australia Survey [HILDA] conducted by the Australian Government Department of Social Services (DSS). The findings and views reported in this paper, however, are those of the author[s] and should not be attributed to the Australian Government, DSS, or any of DSS’ contractors or partners. DOI: doi:10.26193/YP7MNU

# References

```{r create_r-references}
## papaja::r_refs(file = "r-references.bib", append = FALSE)
```
