---
title: "It's Time To Abandon the Cross-Lagged Panel Model"
shorttitle: "Abandon the CLPM"
author: 
  - name: Richard E. Lucas
    affiliation: 1
    corresponding: yes    # Define only one corresponding author
    address: "316 Physics Rd., Michigan State University, East Lansing, MI 48823"
    email: "lucasri@msu.edu"
affiliation:
  - id: 1
    institution: "Department of Psychology, Michigan State University"


abstract: |
  The cross-lagged panel model (CLPM) is a widely used technique for examining reciprocal causal processes using longitudinal data. Critics of the CLPM have noted that it fails to separate within-person effects from between-person associations, and models that incorporate stable-trait components (such as the random intercept cross-lagged panel model; RI-CLPM) have become a popular alternative. Debates about the merits of the CLPM have continued, however, with some researchers arguing that the CLPM is more appropriate than modern alternatives for examining common psychological questions. In this paper, I argue that these defenses of the CLPM are based on an incorrect interpretation of alternatives like the RI-CLPM. I then show in simulated data that the CLPM is very likely to find spuriouos cross-lagged effects when they don't exist, while also failing to find them when they do. I argue that there are no situations where the CLPM is preferable to alternatives that incorporate information about stable traits. 
  
  
keywords: "cross-lagged panel model, longitudinal, structural equation modeling"

wordcount: 

header-includes:
   - \usepackage{todonotes}
   - \usepackage{setspace}
   - \AtBeginEnvironment{tabular}{\singlespacing}
   - \AtBeginEnvironment{lltable}{\singlespacing}
   - \AtBeginEnvironment{ThreePartTable}{\singlespacing}
   - \AtBeginEnvironment{tablenotes}{\doublespacing}
   - \captionsetup[table]{font={stretch=1.5}}
   - \captionsetup[figure]{font={stretch=1.5}}
   - \raggedbottom

bibliography:
   - '/home/rich/Dropbox/MyLibraryZ2.bib'
floatsintext: yes
mask: no
linenumbers: no
documentclass: "apa6"
classoption: "man"
output: 
  papaja::apa6_pdf:
  fig_caption: yes
  
appendix: "gen_starts.Rmd"

---

```{r setup, include=FALSE}
library(lavaan)
library(tidyverse)
library(knitr)
library(DiagrammeR)
library(rethinking)
#library(lavaanPlot)

source("scripts/gen_starts.R") ## Generate data
source("scripts/clpm2_c.R") ## Lavaan model for 2-wave clpm with constraints
source("scripts/clpm5_c.R") ## Lavaan model for 5-wave clpm with constraints
source("scripts/clpm10_c.R") ## Lavaan model for 10-wave clpm with constraints
source("scripts/ri_clpm10_c.R") ## Lavaan model for 10-wave ri-clpm with constraints
source("scripts/starts_c.R") ## Lavaan model for 10-wave starts with constraints
source("scripts/run_sim.R") ## Script to run simulations

options(knitr.kable.NA='')

```

The cross-lagged panel model (CLPM) is a widely used technique for examining causal processes using longitudinal data. With at least two waves of data, it is possible to estimate the association between a predictor at Time 1 and an outcome at Time 2, controlling for a measure of the outcome at Time 1. With some assumptions, this association can be interpreted as a causal effect of the predictor on the outcome. The simplicity of the model along with its limited data requirements have made the CLPM a popular choice for the analysis of longitudinal data. 

The CLPM improves on simpler cross-sectional analyses by controlling for contemporaneous associations between the predictor and outcome. Presumably, confounding factors should be reflected in this association, which would mean that any additional cross-lagged associations between the Time 1 predictor and the Time 2 outcome would reflect a causal effect of the former on the latter (again, with some assumptions).  @hamaker_critique_2015 pointed out, however, that the CLPM does not adequately account for stable-trait-level confounds, and they proposed the random-intercept cross-lagged panel model (RI-CLPM) as an alternative [also see @berry_practical_2017]. The RI-CLPM includes stable-trait variance components that reflect variance in the predictor and outcome that is stable across waves. Hamaker et al. showed that failure to account for these random intercepts and the associations between them can lead to incorrect conclusions about cross-lagged paths. They described the RI-CLPM as a multilevel model that separates between-person effects from within-person effects. As others have noted [e.g., @ludtke_critique_2021; @usami_differences_2020], this critique of the cross-lagged panel model has already been cited frequently and has had an important impact on researchers who use longitudinal data.

Despite this impact, debates about the relative merits of the CLPM versus the RI-CLPM (and more complex alternatives) continue. Critics of the RI-CLPM [e.g., @ludtke_critique_2021; @orth_testing_2021] have argued that sometimes researchers are actually interested in the between-person effects that a classic CLPM tests and that the choice of model should depend on one's theories about the underlying process. The goal of this paper is to examine these critiques, focusing first on the accuracy of the critical interpretation of the RI-CLPM, followed by simulations that demonstrate the problems with the CLPM and the utility of its alternatives. These simulations show that when the CLPM is used, spurious cross-lagged associations are common and the likelihood of finding such spurious effects can reach 100% in many realistic scenarios. Yet the CLPM is also likely to underestimate cross-lagged associations when they do exist, leading to greater Type II errors. I conclude that there is no situation where the CLPM is preferable to the RI-CLPM and the CLPM should probably be abandoned as an approach for examining causal processes in longitudinal data. 

## A Note About Models and Terminology

Before I address the critiques of the RI-CLPM, it is necessary to clarify the terminology that I will use when describing the components of the models. The CLPM, the RI-CLPM, and a slightly more complex model---the bivariate Stable Trait, Autoregressive Trait, State (STARTS) model [@kenny_trait_1995; @Kenny2001]---are presented in Panels A, B, and C of Figure \@ref(fig:riclpmFig). The common feature across all three models is that they include one latent variable per wave for the predictor (X) and the outcome (Y), and these latent variables have an autoregressive structure with cross-lagged associations. Consistent with much previous literature [e.g., @curran_disaggregation_2011; @curran_separation_2014; @curran_disaggregating_2012] the developers (and critics) of the RI-CLPM both refer to the autoregressive part of the model as the "within-person" part, but for reasons discussed below, I will follow @Kenny2001 and refer to this as the "autoregressive" part. Similarly, I will rely on the STARTS terminology when describing the other components of the models. 

The only difference between the CLPM and the RI-CLPM is that the RI-CLPM includes a random-intercept (labeled "Stable Trait" in the figure, according the STARTS terminology) that accounts for "time-invariant, trait-like stability" [@hamaker_critique_2015, p.104]. Thus, the CLPM is nested within the RI-CLPM; the CLPM is equivalent to the RI-CLPM with the random-intercept (or stable-trait) variance constrained to 0. 

Notice that neither the CLPM nor the RI-CLPM include any measurement-error variance for the indicators. For the CLPM, this means that the latent variables from the autoregressive part of the model are equivalent to the observed variables (which is why it is also possible to draw an equivalent CLPM model with only observed variables). For the RI-CLPM, the observed variables are determined by the random intercept and the corresponding wave-specific latent variable from the autoregressive part of the model. 

As can be seen in Figure \@ref(fig:riclpmFig), the only difference between the RI-CLPM and the STARTS model is the inclusion of a wave-specific "state" component (labeled *s~t~* in the figure), which reflects variance in an observed variable that is perfectly "state-like" and unique to that occasion. Note that this state component can include measurement error or any reliable variance that is unique to a single wave of assessment. The idea that some amount of pure state variance would exist in measures of psychological constructs is quite plausible, but simpler models like the RI-CLPM have often been preferred because the STARTS requires more waves of data than the RI-CLPM and often has estimation problems [e.g., @cole_empirical_2005; @orth_testing_2021]. 

Recently, @usami_unified_2019 clarified that the CLPM, RI-CLPM, STARTS and many other longitudinal models could be thought of as variations of an overarching "unified" model that captures many different forms of change [also see @zyphur_data_2020]. For instance, an alternative model---the Latent Curve Model with Structured Residuals [@curran_separation_2014]---can be thought of as an RI-CLPM with a random slope. Because debates about the utility of the CLPM have primarily focused on debates about the inclusion of the random-intercept, I focus here only on the comparison of the CLPM to the RI-CLPM and the STARTS, as this comparison highlights these debates most clearly. It is certainly true, however, that if the other forms of change included in the unified model were part of the actual data generating model, then all the models covered in this paper would be misspecified and could lead to biased estimates. 


```{r riclpmFig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Diagram of the three models used in this paper.', out.height="90%"}
knitr::include_graphics("images/comboFigure.pdf")
```


## The Ambiguous Nature of "Between" Versus "Within"

In presenting the RI-CLPM, @hamaker_critique_2015 emphasized that a strength of the model was its ability to separate between-person effects from within-person effects. Developing methods that adequately isolate effects at different levels of analysis is a perennial challenge for all research that uses data with a multilevel structure [@curran_disaggregation_2011]. Whether the focus is cities nested in nations, children nested in classes, or occasions nested within individuals, adequately separating effects at one level of analysis from those at another is essential. Failing to do so can lead to incorrect conclusions, as effects at one level may be misinterpreted as evidence for effects at the other (e.g., the ecological fallacy). 

Critics of the RI-CLPM focus on this distinction when explaining their concerns with the model. Somewhat surprisingly---and counter to the very large literature arguing for the importance of isolating effects at different levels---they argued that when examining reciprocal causal associations, it can sometimes be inappropriate to separate within-person effects from between. For instance, when describing the RI-CLPM, @orth_testing_2021 argued that "a potential disadvantage of the proposed alternatives to the CLPM is that they estimate within-person prospective effects only, but not between-person prospective effects" (p. 1014). They go on to note that "in many fields researchers are also interested in gaining information about the consequences of between-person differences" (p. 1014). Similarly, in their critique of the RI-CLPM, @ludtke_critique_2021 cautioned that "researchers should be aware that within-person effects are based on person-mean centered (i.e., ipsatized) scores that only capture temporary fluctuations around individual person means" which would be "less appropriate for understanding the potential effects of causes that explain differences between persons" (p. 18). Thus, these critics' central objection to the RI-CLPM is that it isolates within-person effects when sometimes that is not desirable[^ludtke]. But what is a within-person effect? When answering this question, both @ludtke_critique_2021 and @orth_testing_2021 are sometimes precise, but also inconsistent. Although some of their statements about the nature of the within-person effects are accurate, others are not correct. Importantly, their primary objections to the RI-CLPM follow directly from the incorrect descriptions of these within-person effects. 

[^ludtke]: Though @ludtke_critique_2021 did also articulate some additional concerns about the ability of the RI-CLPM to adequately account for unobserved confounders. 

@orth_testing_2021 initially state that "in the RI-CLPM, a cross-lagged effect indicates whether a within-person deviation from the trait level of one construct has a prospective effect on change in the within-person deviation from the trait level of the other construct" (p. 1014). This emphasis on "deviations from the trait level" reflects an accurate interpretation of the "within-person" or autoregressive part of the RI-CLPM. Similarly, @ludtke_critique_2021 initially describe the within-person parts of the RI-CLPM as "deviations from between-person parts" of the model (p. 2), which is also correct (if appropriately tautological). However, both authors later restate these description in ways that are either less precise or wrong. 

For instance, @orth_testing_2021 rephrase their original statement about "deviations from the trait level" to say that a within-person effect (in the context of their substantive example---a test of the causal effect of self-esteem on depression) means that "When individuals have lower self-esteem than usual, they will experience a subsequent increase in depression." But what is this "usual" level from which people deviate? Can we interpret the random intercept from the RI-CLPM as a person's "usual level" or their individual mean [as @ludtke_critique_2021, did]? And what exactly is left after scores are deviated from this random intercept?

Unfortunately, researchers use the terms "between" and "within" in different ways in different contexts [@usami_within-person_2021]. Indeed, even in papers that are focused specifically on distinguishing between the two levels, the terms are typically not explicitly defined conceptually [e.g., @curran_disaggregation_2011; @curran_disaggregating_2012]. This leaves open the possibility that researchers mean different things when they talk about these effects, or that they might think that a technique for separating between-person effects from within-person effects does one thing, when it really does something different. 

As @curran_disaggregation_2011 noted, the most common way to separate between-person effects from within-person effects is through person-mean centering. For instance, in the context of multilevel modeling, researchers are often warned that if they are not careful about how they enter variables into the model, what may look like within-person effects (e.g., the "Level-1" effects in repeated-measures data analyzed in a multilevel modeling framework) can actually reflect a mix of between- and within-person associations. The recommended solution is often to person-center the predictor [e.g., @curran_disaggregation_2011; @enders2007centering], where each observation now reflects a deviation from a person's mean. When centering this way, the Level-1 part of the model tests whether occasion-specific deviations from a person's mean predict variability in the outcome. This is clearly a within-person effect as all between-person variance has been removed from the Level-1 predictor. The between-person effect can then be tested by including the person mean as a Level-2 predictor. 

When we move from multilevel models to structural equation models, the situation becomes a bit more complicated. Although many multilevel models can be respecified as structural equation models, some cannot. @curran_disaggregating_2012 provide a detailed explanation of the challenges that this creates for separating between-person and within-person effects in a structural equation modeling context. For instance, a set of person-centered variables cannot be included in a structural equation modeling analysis to estimate the effect of a time-varying covariate because these predictors sum to zero. Fortunately, there are solutions to these challenges, some of which approximate what occurs when person-centered predictors are used in multilevel models. 

Consider a standard growth curve model with latent intercepts and slopes. It is possible to add a time-varying covariate to the model to estimate the true within-person effect of that covariate. To simplify things for this example, assume that there are no trends over time (or variability in those trends), so the mean and variance of the slope are 0 and the growth model reduces to a single latent intercept reflecting a person's mean. The residuals for the measure at each wave reflect occasion-specific deviations from that mean. The within-person effect of the time-varying covariate can be estimated by predicting each residual from the corresponding wave-specific covariate (in the original, uncentered units, and allowing for correlations among the predictor and the latent mean). In this context, it would be appropriate to describe what is captured by the residuals as deviations from the person-level mean. 

The RI-CLPM, however, is different from the latent growth-curve model in one fundamental way: The residuals for the observed variables are structured residuals. This has important implications for their interpretation, and the interpretation of the associations between these residuals and other variables in the model. Consider the models shown in Figure \@ref(fig:betweenFig). I generated three waves of data for 1,000 participants with a simple autoregressive structure. The data-generating model is presented in Panel A. Note that there is no stable-trait variance or state variance, only autoregressive variance. I specified the variance to be 1 and the stability to be .5. It should be obvious that it is possible to estimate a person-level mean along with the variance of that mean and wave-specific deviations from it. The simple latent-trait model in Panel B does this. In this model, the variance of the latent variable is estimated to be .48, and the residuals can be interpreted as deviations from the person mean. 

Panel C shows a simplified STARTS model where the state component is omitted, but both the latent stable trait and autoregressive trait are included. Notice how the interpretation of the components changes when the residuals are structured in the manner of the RI-CLPM or STARTS. The latent X variable in this model can no longer be thought of as being equivalent to the latent person mean, which means that the residuals no longer reflect deviations from that person mean. The latent X variable (or, in other words, the random intercept) now reflects only the variance that is perfectly stable across the three waves. And because the data-generating model specified that the indicators had a purely autoregressive structure, this latent X variable is estimated to have no variance. Moreover, the estimates for the autoregressive part of the model recover the parameter values from the data-generating model. Panel B shows that there is clearly between-persons (i.e., person-mean) variance in these data, but Panel C shows that this variance is not included in the random intercept and is instead captured by the residuals. 

```{r betweenFig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Interpreting Residuals in Different Models'}
knitr::include_graphics("images/betweenExample.pdf")
```

This example highlights the ambiguity of the terms "between" and "within" when describing data and models. Person-centered data from which all variance in person-level means has been removed can clearly be described as "within-persons." Similarly, both the stable-trait variance component in the STARTS/RI-CLPM and simple person means can clearly be described as "between-persons." Yet the latter two are not equivalent, and the "within-person" part of the RI-CLPM/STARTS does not model deviations from the person mean. As Figure \@ref(fig:betweenFig) shows, this within-person part clearly links between-person variance at one point in time (i.e., the variance in $AR_{X_1}$) to between person differences at a later point in time. 

@orth_testing_2021 appear to (incorrectly) assume that by including a random intercept, the RI-CLPM removes all between-person variance from the cross-lagged part of the model. For example, @orth_testing_2021 state this explicitly: They argue that "a limitation of the RI-CLPM is that it does not provide any information about the consequences of between-person differences. In the RI-CLPM, the between-person differences are relegated to the random intercept factors" (p. 1026). But as Figure \@ref(fig:betweenFig) shows (in a univariate context), this statement reflects a fundamental misunderstanding of the RI-CLPM, one that results from the ambiguous use of the terms "between" and "within." Later on the same page, Orth et al. state that "The RI-CLPM includes [an] unrealistic assumption, specifically that the between-person variance in constructs is perfectly stable" (p. 1026). But again, this is wrong: In the RI-CLPM, the term "between-persons" simply refers to variance that is perfectly stable over time. This is an issue of terminology. The RI-CLPM does not assume that all between-persons variance is perfectly stable, it simply *defines* "between-persons" variance as the variance that *is* perfectly stable. There is clearly between-person variance (broadly defined) left in the cross-lagged autoregressive part of the model. 

## An Example

To demonstrate the implications of this ambiguity in the context of bivariate data, take the following example, where data are generated from a true autoregressive model with cross-lagged paths. In other words, the data-generating model looks like Panel A of Figure \@ref(fig:riclpmFig). For comparison with the substantive discussion by Orth et al., let's assume that the predictor is self-esteem and the outcome is depression.  Specifically, in this example, I generated data for 10 waves of self-esteem and depression data, with no random intercept, starting variance of 1 for self-esteem and depression, stabilities of .5 for each, true cross-lagged paths of .50 from self-esteem to depression and .00 from depression to self-esteem. I also specified a Wave 1 correlation between self-esteem and depression of -.5. The code for a general function that generates the data is available in the appendix.


```{r true_clpm, echo=FALSE}
set.seed(1224)
clpm_data <- gen_starts(n = 10000, 
                        nwaves = 10,
                        x = 1, # Variance in Autoregressive Component for X
                        y = 1, # Variance in Autoregressive Component for Y
                        ri_x = 0, # Random Intercept Variance for X
                        ri_y = 0, # Random Intercept Variance for Y
                        xr = 0, # Measurement Error for X
                        yr = 0, # Measurement Error for Y
                        stab_x = .50, # Stability of Autoregressive Component for X
                        stab_y = .50, # Stability of Autoregressive Component for Y
                        yx = -.50, # Crosslagged path, Y regressed on X
                        xy = .00, # Crosslagged path, X regressed on Y
                        cor_xy = -.5, # Correlation between Autoregressive Component for X and Y
                        )

## Fit CLPM Model
fit_clpm <- lavaan(clpm10_c, data = clpm_data)

```

According to @orth_testing_2021, significant cross-lagged paths from self-esteem to depression in this model can be interpreted to mean that "When individuals have low self-esteem (relative to others), they will experience a subsequent rank-order increase in depression compared to individuals with high self-esteem." In other words, this model links between-person differences in self-esteem at Time 1 to change in depression from Time 1 to Time 2. They argue that this is precisely what many researchers would want to estimate in many common situations.

They also argue that when you test a model that includes random intercepts, the interpretation of the cross-lagged paths change. They state that a significant cross-lagged path in the context of the RI-CLPM means that "When individuals have lower self-esteem than usual, they will experience a subsequent increase in depression" (p. 1014). The problem noted above is that what is captured by the "stable trait" in the RI-CLPM model is potentially (and frequently) different than a person's "average level" or "usual level." Again, the stable trait does not incorporate all between-person variance; it only includes variance that is perfectly stable across all waves. So what a substantial cross-lagged path really reflects in a RI-CLPM context is that "When individuals have lower self-esteem *than what would be predicted from their levels on the stable trait*, they will experience a subsequent increase in depression." 

This distinction may sound subtle, but it is important. For one thing, even when clear "between-person" differences exist in the data---for instance, in the data I generated---if one tries to fit the RI-CLPM to data without a stable-trait component, the estimate for the variance of the random intercept will be zero. If the RI-CLPM simply took what between-person variance exists and "relegated" it to a stable-trait component that reflected a person's long-term average, you would always be able to find a random intercept as long as variance in person means existed.

```{r add_ri, echo=FALSE, message=FALSE, warning=FALSE }
## First, run RI-CLPM on Original Data for Comparison
fit_ri_clpm <- lavaan(ri_clpm10_c, data = clpm_data)

## Generate latent X and Y intercepts (perfectly stable method variance, correlated .80 for X and Y)
st_data <- rmnorm(10000, varcov = matrix(c(.50, -.40, -.40, .50), nrow=2))

## Combine data
combo_data <- cbind(clpm_data, st_data)

## Add method variance to original scores
temp_x <- data.frame(sapply(seq(1,10), function(x) rowSums(combo_data[,c(x,21)])))
temp_y <- data.frame(sapply(seq(11,20), function(x) rowSums(combo_data[,c(x,22)])))
names(temp_y) <- paste0("y", 1:10)

## Combine dataframes
combo_data <- cbind(temp_x, temp_y)
names(combo_data) <- tolower(names(combo_data))
```


```{r models, echo=FALSE}

## Models with combo data
fit_ri <- lavaan(ri_clpm10_c, data = combo_data)
#summary(fit_ri)

```


```{r riTable, echo=FALSE, message=FALSE, warning=FALSE}

## Extract table data
clpm_table <- parameterEstimates(fit_clpm)[c(1,2,3,4,37,38,57),c(5:6)]
riclpm_table <- parameterEstimates(fit_ri_clpm)[c(41,50,68,59,77,78,79,80,99,100),c(5:6)]
riclpm_table2 <- parameterEstimates(fit_ri)[c(41,50,68,59,77,78,79,80,99,100),c(5:6)]

## Combine Tables
table2 <- cbind(riclpm_table[c(1:4,7,8,10,5,6,9),], riclpm_table2[c(1:4,7,8,10,5,6,9),])
clpm_table[8:10,] <- NA

combined_table <- cbind(clpm_table, table2)

rownames(combined_table) <- c("Stability of Self-Esteem",
                          "Stability of Depression",
                          "Self-Esteem Predicted by Depression",
                          "Depression Predicted by Self-Esteem",
                          "Variance of Self-Esteem AR Trait",
                          "Variance of Depression AR Trait",
                          "Covariance Between AR Components",
                          "Variance of Self-Esteem Stable Trait",
                          "Variance of Depression Stable Trait",
                          "Covariance of Stable Traits"
                          )

options(knitr.kable.NA='')

papaja::apa_table(combined_table
          , align = c("l", rep("r",6))
          , caption = "Comparison of Estimates for Cross-Lagged Effects"
          , note = "The estimates for the CLPM columns reflect the CLPM fit to the original data. The estimates for the RI-CLPM column reflect the RI-CLPM fit to the original data. The estimates for the RI-CLPM column reflect the RI-CLPM fit to the original data with the stable trait variance added."
          , landscape = TRUE
          , format.args=list(na_string="")
          , col_spanners = list(`CLPM` = c(2,3),
                                `RI-CLPM` = c(4,5),
                                `RI-CLPM with Trait` = c(6,7)))


```

The first two columns of Table \@ref(tab:riTable) shows the estimates from fitting the CLPM to the generated data. The second two columns show the estimates from the RI-CLPM fit to the same data. The estimates for the cross-lagged autoregressive part of the model are almost identical. This again shows, in the bivariate context, that the RI-CLPM is not equivalent to a CLPM with ipsatized (mean-deviated) data. The cross-lagged part of the RI-CLPM reflects deviations from the latent stable trait, and if there is no variance in this stable trait, then the RI-CLPM is equivalent to the CLPM. The fact that the CLPM is a reduced version of the RI-CLPM where the random-intercept variance is set to 0 is, of course, obvious just by looking at the models. The point of this example is to clarify what that means for the interpretation of these cross-lagged paths. 

A potential response to this example would be to acknowledge that using the RI-CLPM will not artificially create random-intercept variance when it does not exist, while still arguing that when stable between-person variance does exist, the model somehow distorts the cross-lagged paths.  But this is also not correct. Imagine that we take the same data from above, but now we add some stable-trait variance. Let's assume for this example that the added stable-trait variance is just shared method variance. Specifically, we can generate data representing a method factor for self-esteem and one for depression. In generating these data, I set the variance of these stable-trait components to .50, and I assumed that the stable traits are pretty strongly (and negatively, given the direction of the items) correlated (though this doesn't really matter for this specific example). We can then just add the generated method-variance scores to the original data. 


```{r means, echo=FALSE, message=FALSE, warning=FALSE}

## Find person means for original data and combined data
clpm_means <- data.frame(x=rowMeans(clpm_data[,1:10]), y=rowMeans(clpm_data[,11:20]))
combo_means <- data.frame(xc=rowMeans(combo_data[,1:10]), yc=rowMeans(combo_data[,11:20]))
all_means <- cbind(clpm_means, combo_means, st_data)
names(all_means) <- paste(rep(c("clpm", "combo", "ri"), each=2), c("x","y"), sep = "_")

```

If we fit the RI-CLPM to the combined data, you get exactly what you'd expect: The estimates for the random intercept simply reflect the method variance we've added. These results are shown in the final columns of Table \@ref(tab:riTable). The variance of each estimated random intercept is about .50 and the covariance is about -.40 (for a correlation of -.80). These random intercepts don't capture any of the between-person differences in the original data (which are substantial) because the original individual differences are not *perfectly* stable across waves. In other words, the model does not, as Orth et al. suggested "relegate all between-person differences to the random intercept," it simply pulls out those between-person differences that are completely stable over time. 

More importantly, Table \@ref(tab:riTable) shows that the estimates for the original autoregressive, cross-lagged part of the model are the same as what you get when you run the CLPM on the original data. The variances, stabilities, and cross-lagged paths are the same (as they should be). Contrary to Orth et al., the cross-lagged part of the model still links the same between-person variance in self-esteem at Time 1 to between-person variance in depression at Time 2. The interpretation of this part of the model is exactly the same when no stable-trait variance exists as it is when there is stable-trait variance, but that stable-trait variance is modeled using the RI-CLPM. Specifically, in the RI-CLPM, the cross-lagged effects still assess whether those who have high self-esteem relative to others at Time 1 report larger changes in depression over time. The difference is that these effects have now been adjusted for stable differences in measurement error; they have certainly not been "ipsatized" in any way. 

Of course, in real data, we typically do not know whether the stable-trait variance that exists reflects something that is clearly not theoretically interesting (as is true of method variance). My point is that adding some stable trait variance (in this case, method variance) and then modeling this stable variance using the RI-CLPM doesn't suddenly make the cross-lagged part any more "within-persons" than it was before. Indeed, I am pretty sure that what @orth_testing_2021 would call a between-persons model---the CLPM fit to the original data I generated---would actually be described by @hamaker_critique_2015 as a within-persons model, not because of what the model is doing, but simply because there is no stable-trait variance confounding the effects. Again, this is an issue of terminology. 

An additional response that the critics of the RI-CLPM could make is that there is some important reason to avoid separating purely stable between-person variance from the between-person variance that remains in the autoregressive part of an RI-CPLM or STARTS model when testing causal associations with longitudinal data. However, this is *not* the argument that these critics made. They explicitly state that their objection to the RI-CLPM was that it removed all between-person differences from the autoregressive/cross-lagged part of the model, which resulted in cross-lagged paths that capture only associations between ipsatized scores (which reflect deviations from people's usual level) for the predictor and outcome. Perhaps there is a reason why both types of individual differences (those captured by the stable trait and those that remain in the autoregressive part of the model) would be expected to be associated with wave-to-wave changes, but these reasons were not put forth. 

It is noteworthy that neither @ludtke_critique_2021 nor @orth_testing_2021 described a data-generating model that would result in biased estimates or incorrect conclusions if analyzed incorrectly using the RI-CLPM. As I demonstrated above, generating data from the model that they claim to prefer---the CLPM---results in correct estimates (despite inadmissable solutions due to negative estimated variances) when analyzed using the RI-CLPM. I believe that it is not possible to specify a data-generating model that corresponds to the processes that the critics describe that would also lead to incorrect estimates when analyzed using the RI-CLPM[^wrong]. 

[^wrong]: @ludtke_critique_2021 did show that the RI-CLPM cannot successfully control for all types of confounds, but this is an issue that is distinct from the question of whether we can simply recover the structure of these variables over time. Of course, it is possible to specify data-generating models that do result in data that, when analyzed using the RI-CLPM, lead to incorrect conclusions (including the more complex models described by @usami_unified_2019). My point is that the critics of the RI-CLPM have not provided a model that matches the processes that they describe and also results in incorrect estimates when modeled using the RI-CLPM. 

Why does this matter? All of the limitations of the RI-CLPM and all of the reasons for preferring the CLPM to the RI-CLPM discussed by Orth et al. [-@orth_testing_2021, p. 1026] are based on an incorrect description of what the cross-lagged part of the RI-CLPM really does. Orth et al. conclude that "the RI-CLPM does not allow testing what many researchers ... are interested in: the prospective between-person effect" (p. 1026). I think the very term "prospective between-person effect" reflects the confusion about the meaning of the terms "between" and "within" that I noted; but to the extent that such a thing exists, the RI-CLPM captures it just as well as the CLPM. 

# An Alternative Case for Modeling Stable Trait Variance

Given the ambiguity of the terms "within" and "between" in the context of models like the CLPM and its alternatives, it can be helpful discuss benefits of the RI-CLPM over the CLPM without referring to this distinction[^caveat]. Specifically, the RI-CLPM is useful because it tests an extremely plausible alternative explanation of the underlying pattern of correlations that is being modeled when the CLPM is used. It is important to consider this alternative explanation when testing the CLPM, regardless of what theoretical model the researcher prefers.

[^caveat]: Though ultimately, I think that Hamaker et al.'s [-@hamaker_critique_2015] framing is a more precise way of saying the same thing; my point is that we don't necessarily need to the within/between framing to understand the problems with the CLPM. 

The logic of the CLPM is very similar to the logic of any other regression model where we assess whether one variable predicts another after controlling for relevant confounds. When we test whether Time 1 X predicts Time 2 Y after controlling for Time 1 Y, we hope to capture whether there is something unique about X---something that cannot be explained by the concurrent association between X and Y---that helps us predict Y at a later time. But as @westfall_statistically_2016 pointed out when discussing the difficulty of establishing incremental predictive validity of any kind, if the measure that we include as a control (i.e., Time 1 X) is not a perfect measure of what we're trying to account for, then it is possible---indeed, quite easy---to find spurious "incremental validity" effects. This, I think, is a simpler way of thinking about the strengths of the RI-CLPM relative to the CLPM, especially given the ambiguity of terminology in multilevel models.

## It's Extremely Easy to Find Spurious Cross-Lagged Effects

The problem with the CLPM is that is easy---in fact, extremely easy---to find spurious cross-lagged associations under conditions that are extremely likely in the typical situations where the CLPM is used. Hamaker et al. (2015) conducted simulations to show that the estimates from a cross-lagged panel model were often biased in realistic situations. I don't think they went far enough, though, in describing the practical implications of these simulations or showing just how likely spurious effects are in realistic situations. So the rest of this paper simply builds on their simulations and tries to clarify when such spurious effects are likely to occur. As I show, there are many realistic scenarios where researchers are guaranteed to find spurious cross-lagged effects. 

### The Simulations

When considering what types of situations to simulate, I focus on realistic scenarios for the types of data to which the CLPM is likely to be applied. For instance, it is likely that most variables that psychologists choose to study over time have a STARTS-like structure, where stability declines with increasing interval length (reflecting an autoregressive structure), yet this decline approaches or reaches an asymptote where further increases in interval length are no longer associated with declines in stability (reflecting the influence of a stable trait). It is also likely that most measures of psychological constructs have some amount of pure state variability, which could reflect measurement error or true state-like influences. Starting with this assumption, it is then possible to test how variation in these factors affect the estimated cross-lagged paths when the CLPM is used. A Shiny app is available where variations of this starting model can be specified and the effects on cross-lagged paths can be tested: [shinyapps.io/rlucas11/clpm](https://shinyapps.io/rlucas11/clpm). Readers can use this app to examine the specifications described in the text and to test alternatives. 

Because the focus of this paper is on examining the effects of unmodeled stable trait variance, I set the variance of the stable trait component for the predictor and outcome to be 1 in the primary simulations (though occasionally, I do set stable-trait variance to zero to address specific questions). I then varied the ratio of autoregressive variance to stable-trait variance across four levels: 0, .5, 1, and 2. Similarly, I varied the "reliability" of the measures (defining reliability as the proportion of variance due to stable-trait and autoregressive-trait variance) across three levels: .5, .7, and .9. Even the lowest level is not unrealistic, given that the state component includes both measurement error and reliable state variance. Finally, I varied the size of the correlation between the stable traits across four levels from very weak to very strong: .1, .3, .5, and .7. I ran 1,000 simulations for each of six sample sizes: 25, 50, 100, 250, 500, and 1,000). In all simulations, I set the correlation between the initial autoregressive variance components for the predictor and outcome to be .50 and the stability of the autoregressive components to be .50 (though, later, I discuss some modifications to this). I also set the correlations between state components to be 0. Consistent with the canonical STARTS model, I included a stationarity constraint, so that variances, correlations, and stability coefficients are constrained to be equal over time. This simplifies discussion of the estimated cross-lagged paths, as there is just one estimate per model. 

After generating the data, I tested a simple two-wave CLPM, keeping track of the average size of the estimated cross-lagged paths and the number of cross-lagged paths that were significant at a level of .05. Note that researchers are often interested in determining which of the two variables in the model has a causal impact on the other rather than on simply testing the effect of one predictor on an outcome. Thus, an effect of X on Y, Y on X, or both would often be interpreted as a "hit" in common applications of the CLPM. This means that error rates are typically elevated in the CLPM even without unmodeled stable-trait effects unless corrections for multiple comparisons are used. In these simulations, I report the percentage of runs that result in at least one significant cross-lagged effect (out of two tested), and readers can interpret these results either in comparison to a baseline error rate of 5% or 10% depending on how these multiple comparisons are considered. 

Finally, although I focus on the common two-wave CLPM design, it is important to note that more waves of data lead to increased power to detect smaller effects---even spurious effects. This means that spurious cross-lagged associations are more likely to be found with better, multi-wave designs. Thus, I will also present results from simulations with more waves of data after presenting the primary results. 

The proportion of simulations that resulted in at least one significant cross-lagged effect are presented in Figure \@ref(fig:simFig). The X-axis shows results for different sample sizes. The Y-axis reflects the percentage of runs in which a significant (spurious) cross-lagged path was found. Ideally, this would be close to 10%, which would be the error rate without taking multiple comparisons into account[^error]. The columns reflect variation in the reliability of the measures. The rows reflect variation in the ratio of autoregressive variance to stable-trait variance. The individual lines in each plot reflect different correlations between the two stable traits. The averaged estimates for the cross-lagged paths in each set of simulations (averaging across sample sizes, as this will not affect the estimated effect) is reported in Table \@ref(tab:simTab). So, when can you find spurious effect? Here are a few common situations.

[^error]: More precisely, the error rate based on chance alone should be .0975.

```{r simFig, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Simulation results for two-wave CLPM. Columns reflect different reliabilities. Rows reflect different ratios of autoregressive to stable-trait variance. Lines reflect different correlations between stable-trait components. Grey line reflects expected number of significant effects due to chance (assuming a critical p-value of .05).', fig.height=8}

results2 <- readRDS("saved/2WaveSimulation.rds")
## Changes names for plot
names(results2) <- c("N", "r","Reliability", "Autoregressive", "power", "estimatex","estimatey")
results2$`Ratio AR Variance` <- factor(paste0((results2$Autoregressive*100),"%"), levels = c("0%", "50%", "100%", "200%"))

p2 <- ggplot(data = results2, aes(x = N, y = power, group = r)) +
    geom_line(aes(linetype=as.factor(r)),color="black", size=.5) +
    scale_x_continuous(breaks = c(25,50,100,250,500,1000), labels = c("25", "", "100","250","500","1000")) +
    facet_grid(cols = vars(Reliability),
               rows = vars(`Ratio AR Variance`),
               labeller=label_both) +
    theme_bw() +
    theme(legend.position="bottom",
          axis.text.x = element_text(angle = 90, vjust = .5),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank()) +
    geom_hline(aes(yintercept=.0975), color="gray") +
    labs(linetype = "Correlation Between Stable Traits") +
    ylab("Probability of One or More Significant Cross-Lagged Effects")
p2

```


```{r simTab, echo=FALSE, message=FALSE, warning=FALSE}

results2 <- readRDS("saved/2WaveSimulation.rds")
names(results2) <- c("N", "r","Reliability", "Autoregressive", "power", "estimatex","estimatey")

temp <- results2 %>%
    rowwise() %>%
    mutate(estimate=mean(c(estimatex, estimatey))) %>%
    group_by(r, Reliability, Autoregressive) %>%
    summarize(estimate=mean(estimate)) %>%
    pivot_wider(names_from = Reliability, values_from = estimate)

names(temp) <- c("Stable Trait r", "AR Variance Ratio", "0.5", "0.7", "0.9")

temp[c(2:4,6:8,10:12,14:16),1] <- NA
temp$`AR Variance Ratio` <- paste0(as.character(temp$`AR Variance Ratio`*100),'%')

papaja::apa_table(temp,
                  midrules=c(4,8,12),
                  align=rep("r", 5),
                  format.args=list(na_string=""),
                  col_spanners=list(Reliability=c(3,5)),
                  caption="Average Estimated Cross-Lagged Paths In Each Simulation Condition")

```


### When Constructs Have Some Stable-Trait Structure

If the measures include some amount of stable-trait variance--even if the stable traits are uncorrelated---it is likely that spurious cross-lagged paths will emerge. To be clear, this is most problematic when the stable traits are correlated and the correlation is quite high. However, error rates are elevated across most simulations. For instance, consider results in the third column of Figure \@ref(fig:simFig), where the reliability is a very high .9. Specifically, focus on the fourth row, where the ratio of autoregressive variance to stable-trait variance is 2:1. This panel reflects the least problematic set of values tested, and even here, error rates approach 100% when correlations between the stable traits are strong (*r* = .70) and sample sizes are large (*N* = 1,000). Even when correlations are more moderate (e.g., *r* = .5), however, these error rates approach 50% in large samples.

Interestingly, error rates are not monotonically associated with the size of the correlation when reliability is high. Consider the panels in Rows 2, 3, and 4 of Column 3. In these panels, where the ratio of autoregressive variance to stable-trait variance is .5 or higher, the error rates for the lowest correlation tested (*r* = .1, shown in the solid line) are actually higher than error rates for a higher stable-trait correlation of .3. A look at the actual estimates across simulations in Table \@ref(tab:simTab) provides insight into why this is. This table shows that the average estimated cross-lagged path is actually negative when reliability is high, the correlation between the stable traits is low, and there is substantial amounts of autoregressive variance. These negative estimates emerge even though all associations among the latent components were specified to be positive. 

This effect can be demonstrated even more clearly by simulating data with uncorrelated stable traits, an equal amount of autoregressive and stable-trait variance, and no state variance whatsoever (this simulation is not shown in the figure). In this case, the estimated cross-lagged paths will be approximately -.07. This is due to the fact that by failing to account for the stable trait, the model overestimates the stability of X and Y, which means that the observed correlation between X at Time 1 and Y at Time 2 is lower than what would be expected based on the initial correlation between X and Y at Time 1 and the stability over time[^tracing]. These simulations show that when the variables being examined have a trait-like structure, this can lead to spurious cross-lagged effects, even when the stable trait variance is not correlated. When correlations at the stable-trait level are strong, however, the effects of ignoring the stable-trait structure can be substantial. In some realistic scenarios (e.g., moderate correlations between stable traits, reliabilities of .70, and sample sizes over 100), significant spurious correlations are almost guaranteed. 

[^tracing]: This can be understood by using tracing rules. If we randomly generate data for 10,000 participants from the data-generating model just described, the correlation between *X1* and *X2* and between *Y1* and *Y2* are both around .75. The correlation between *X1* and *Y1* would be about .25, and the correlation between *X1* and *Y2* would be about .12. Fitting a CLPM to these data results in estimated stabilities for X and Y of approximately .77, and a correlation between *X1* and *Y1* of .25. These values would imply an observed correlation of  $.77 * .25 = .19$ between *X1* and *Y2*, which is greater than the actual correlation of .12. This discrepancy between the predicted and observed correlations results in the negative estimates for the cross-lagged paths. 

### When Measures Have Error or Reliable Occasion-Specific Variance

The simulations described above focus on situations where reliability is very high. When there is measurement error or reliable state variance (as is very likely), this effect gets worse---potentially *much* worse. Consider the panel in the first row and the first column of Figure \@ref(fig:simFig). In this case, reliability is set to .5 and there is no autoregressive variance. Error rates are very high, approaching 100% with large samples, even when the stable-trait correlation is just .3. Samples of 100 can result in spurious cross-lagged effects approximately 60% of the time when stable traits are correlated .5. Even in samples as small as 25, error rates exceed 25% in many situations. 

This outcome is actually quite easy to understand. Indeed, we don't really need simulations at all to predict it. This result is a simple consequence of the issues that @westfall_statistically_2016 discussed. Because X and Y are measured with error at each occasion, controlling for Time 1 Y when predicting Time 2 Y from Time 1 X does not fully account for the true association between X and Y. There will still be a residual association between Time 1 X and Time 2 Y, which can be accounted for by the freed cross-lagged path in the CLPM. The RI-CLPM (and the STARTS) are useful because they do a better job accounting for this underlying association than the CLPM. 

One might argue that a model that just includes stable-trait variance and error (which is true of all simulations in the first row of Figure \@ref(fig:simFig)) is unrealistic, as there is sure to be some form of autoregressive structure to most variables we study. That is true, but as the other rows of the figure show, the existence of this stable trait causes problems for the CLPM even when all three sources of variability (stable trait, autoregressive trait, and state) exist.

```{r measurementError, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

## Actual Simulation Values
nValues <- c(500)
rValues <- c(0)
reliabilities <- c(.8)
arValues <- c(1)

## Run Sim
##
loopRow <- 1
results_noSt <- data.frame(N = numeric(),
                      r = numeric(),
                      reliability = numeric(),
                      AR_Var = numeric(),
                      power = numeric(),
                      estimatex = numeric(),
                      estimatey = numeric())
for (i in 1:length(nValues)) {
    for (j in 1:length(rValues)) {
        for (k in 1:length(reliabilities)) {
            for (l in 1:length(arValues)) {
                nValue <- nValues[i]
                rValue <- rValues[j]
                rel <- reliabilities[k]
                arValue <- arValues[l]
                sims <- data.frame(t(mcreplicate(n=1000, run_sim_clpm(waves = 2,
                                                                      studyN=nValue,      # N to generate
                                                                      ri_x=0,     # Random intercept variance for X
                                                                      ri_y=0,     # Random intercept variance for Y
                                                                      cor_i=rValue,   # Correlation between intercepts
                                                                      x=arValue,        # AR variance for X
                                                                      y=arValue,        # AR variance for Y
                                                                      stab_x=.5,  # Stability of X
                                                                      stab_y=.5,  # Stability of Y
                                                                      yx=0,      # Cross lag (Y on X)
                                                                      xy=0,      # Cross lag (X on Y)
                                                                      cor_xy=.5,  # Correlation between X and Y
                                                                      reliability_x=rel,       # Measurement error for X
                                                                      reliability_y=rel       # Measurement error for Y
                                                                      ), mc.cores=14)))
                results_noSt[loopRow,1] <- nValue
                results_noSt[loopRow,2] <- rValue
                results_noSt[loopRow,3] <- rel
                results_noSt[loopRow,4] <- arValue
                results_noSt[loopRow,5] <- sum(sims$X2<.05 | sims$X4<.05)/1000
                results_noSt[loopRow,6] <- mean(sims$X1)
                results_noSt[loopRow,7] <- mean(sims$X3)
                loopRow <- loopRow+1
            }
        }
    }
}


```

At this point, it's important to highlight the fact that at least some of these effects are due more to the existence of measurement error (or reliable state variance) than to the existence of the stable trait. For instance, we could simulate data with an autoregressive structure, set the variance of the stable trait components to be 0, and specify no cross-lagged paths. Even with relatively high reliability (e.g., .8 for this simulation), the average estimated cross-lagged paths would be `r mean(results_noSt$estimatex, results_noSt$estimatey)` and spurious effects would be found `r round(results_noSt$power * 100, 0)`% of the time in a two-wave design with samples of 500 participants. Again, Westfall and Yarkoni's  [-@westfall_statistically_2016] explanation can account for these results: The existence of measurement error or state variance in the observed measures of Y means that controlling for Y1 does not control for enough. The result is a spurious cross-lagged path. 

```{r measurementErrorRI, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

## Actual Simulation Values
nValues <- c(500)
rValues <- c(.5)
reliabilities <- c(.8)
arValues <- c(1)

## Run Sim
##
loopRow <- 1
results_ri <- data.frame(N = numeric(),
                      r = numeric(),
                      reliability = numeric(),
                      AR_Var = numeric(),
                      power = numeric(),
                      estimatex = numeric(),
                      estimatey = numeric())
for (i in 1:length(nValues)) {
    for (j in 1:length(rValues)) {
        for (k in 1:length(reliabilities)) {
            for (l in 1:length(arValues)) {
                nValue <- nValues[i]
                rValue <- rValues[j]
                rel <- reliabilities[k]
                arValue <- arValues[l]
                sims <- data.frame(t(mcreplicate(n=1000, run_sim_riclpm(waves = 10,
                                                                      studyN=nValue,      # N to generate
                                                                      ri_x=1,     # Random intercept variance for X
                                                                      ri_y=1,     # Random intercept variance for Y
                                                                      cor_i=rValue,   # Correlation between intercepts
                                                                      x=arValue,        # AR variance for X
                                                                      y=arValue,        # AR variance for Y
                                                                      stab_x=.5,  # Stability of X
                                                                      stab_y=.5,  # Stability of Y
                                                                      yx=0,      # Cross lag (Y on X)
                                                                      xy=0,      # Cross lag (X on Y)
                                                                      cor_xy=.5,  # Correlation between X and Y
                                                                      reliability_x=rel,       # Measurement error for X
                                                                      reliability_y=rel       # Measurement error for Y
                                                                      ), mc.cores=14)))
                results_ri[loopRow,1] <- nValue
                results_ri[loopRow,2] <- rValue
                results_ri[loopRow,3] <- rel
                results_ri[loopRow,4] <- arValue
                results_ri[loopRow,5] <- sum(sims$X2<.05 | sims$X4<.05)/1000
                results_ri[loopRow,6] <- mean(sims$X1)
                results_ri[loopRow,7] <- mean(sims$X3)
                loopRow <- loopRow+1
            }
        }
    }
}


```

In addition, measurement error also affects estimates from the RI-CLPM. If we specify a data-generating model that includes all three sources of variance (stable trait, autoregressive trait, and state/measurement error), but no cross-lagged paths, the CLPM will find substantial cross-lagged effects, but so will the RI-CLPM (at least if the autoregressive components of X and Y are correlated). To demonstrate, I simulated data with the following characteristics. The X and Y stable traits had variances of 1 and a correlation of .5 and X and Y autoregressive traits had a variance of 1 and a starting correlation of .5 with stability coefficients of .5. The average estimated cross-lagged path was `r mean(results_ri$estimatex, results_ri$estimatey)`, which would be easily detectable with moderate to large sample sizes. Note, this limitation of the RI-CLPM is not an argument *for* the CLPM (though it is an argument for using the STARTS, when possible). 

One response to the above simulations is to suggest that we simply need to use very reliable measures or perhaps model latent variables at each occasion instead of relying on observed variables with less than perfect reliability. This will certainly help, but it is important to remember that the "state" component in the STARTS model includes measurement error *and* reliable occasion-specific variance. Reliable state variance will affect these results in exactly the same way as random measurement error. Unfortunately, we don't know how common this reliable state component is in real data, though we have at least some evidence that it can exist and be large enough to be meaningful [@LucasInPressSIR]. Thus, even the use of latent occasions in the CLPM can't solve this problem. 

### When There Are Many Assessment Waves

Although the CLPM is often used with just two waves of assessment, it can also be used with more complex data. Indeed, a general rule for longitudinal data is that more waves are better than fewer, and in situations where stationarity could reasonbly be expected, including more waves and imposing equality constraints should lead to more precise estimates of cross-lagged paths. When estimating true effects, this has the benefit of increasing power. When spurious effects would be expected, however, the use of more waves will also increase the probability of those spurious effects being significant [again, see @westfall_statistically_2016, for a discussion of how factors that improve power can increase the ability to find spurious effects]. 

```{r simFig5, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Simulation results for five-wave CLPM. Columns reflect different reliabilities. Rows reflect different ratios of autoregressive to stable-trait variance. Lines reflect different correlations between stable-trait components. Grey line reflects expected number of significant effects due to chance (assuming a critical p-value of .05).', fig.height=8}

results5 <- readRDS("saved/5WaveSimulation.rds")
## Changes names for plot
names(results5) <- c("N", "r","Reliability", "Autoregressive", "power", "estimatex","estimatey")
results5$`Ratio AR Variance` <- factor(paste0((results5$`Autoregressive`*100),"%"), levels = c("0%", "50%", "100%", "200%"))

p5 <- ggplot(data = results5, aes(x = N, y = power, group = r)) +
    geom_line(aes(linetype=as.factor(r)),color="black", size=.5) +
    scale_x_continuous(breaks = c(25,50,100,250,500,1000), labels = c("25", "", "100","250","500","1000")) +
    facet_grid(cols = vars(Reliability),
               rows = vars(`Ratio AR Variance`),
               labeller=label_both) +
    theme_bw() +
    theme(legend.position="bottom",
          axis.text.x = element_text(angle = 90, vjust = .5),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank()) +
    geom_hline(aes(yintercept=.0975), color="gray") +
    labs(linetype = "Correlation Between Stable Traits") +
    ylab("Probability of One or More Significant Cross-Lagged Effects")
p5


```

Figure \@ref(fig:simFig5) shows a set of simulations that are similar to those reported in Figure \@ref(fig:simFig), but this time using five waves of data and the CLPM with equality constraints across waves. When comparing these two figures, the effect of increasing the number of waves is immediately apparent: Error rates increase considerably. For instance, in the very realistic scenario of an N of 250, a correlation between stable traits of .5, reliabilities of .7, and a 1:1 ratio of stable-trait to autoregressive variance, the error rate increases from `r results2[results2$N==250&results2$r==.5&results2$Reliability==.70&results2$Autoregressive==1,5]*100`% to `r results5[results5$N==250&results5$r==.5&results5$Reliability==.7&results5$Autoregressive==1,5]*100`% when moving from a two-wave study to a five-wave study. With five waves of data, error rates often exceed 50%, even in samples as small as 25. So features that are generally desirable---large sample sizes and multiple waves of assessment---increase the likelihood of finding spurious cross-lagged paths. 


## The CLPM Also Has High Type II Error Rates

The examples above focused almost entirely on cases where there were no true cross-lagged associations in the data-generating model. The simulations showed that spurious paths are often very likely to be found. This pattern matches the intuition that the RI-CLPM is more conservative than the CLPM [@ludtke_critique_2021]. However, failure to model associations between stable-trait components can also lead to the *underestimation* of real cross-lagged paths, which will result in elevated Type II error rates. For instance, consider a situation where the measures are perfectly reliable and the stable trait and autoregressive trait contribute equally (in this particular case, I also specified the correlations among the stable trait and autoregressive traits to be .5). If we simulate data with cross lagged paths of .5 from X to Y and .2 from Y to X, the RI-CLPM reproduces these effects perfectly. However, even with no measurement error, the estimates from the CLPM are half the size that they should be, approximately .25 and .10. 

The precise manner in which estimates will be affected depends on the size of these variance components and the correlations between them. Table \@ref(tab:truePaths) shows the results from a separate simulation that examines these effects. Specifically, because the parameter estimates were the focus (rather than the frequency of errors), I followed @ludtke_critique_2021 and generated just one set of 10,000 responses for each of 48 combinations. I varied the correlation between the stable traits across four levels: .1, .3, .5, and .7. Similarly, I varied the correlation between the autoregressive traits across the same four levels. I also varied the ratio of autoregressive to trait variance across three levels: .5, 1, and 2. For this example, reliability was set to be 1 and the stability of the autoregressive components were set to .5. The table only shows results for one cross-lagged path, for which the true value is .50. 


```{r truePaths, echo=FALSE, message=FALSE, warning=FALSE}

################################################################################
## 2-Wave Simulation with True CL Paths
################################################################################

## Actual Simulation Values
nValues <- c(10000)
rValues <- c(.1,.3,.5,.7)
ar_corValues <- c(.1, .3, .5, .7)
reliabilities <- c(.7, 1)
arValues <- c(.5, 1, 2)

## Run Sim
##
loopRow <- 1
results <- data.frame(ar_corValue = numeric(),
                      r = numeric(),
                      reliability = numeric(),
                      AR_Var = numeric(),
                      estimatex = numeric(),
                      estimatey = numeric())
for (i in 1:length(ar_corValues)) {
    for (j in 1:length(rValues)) {
        for (k in 1:length(reliabilities)) {
            for (l in 1:length(arValues)) {
                ar_corValue <- ar_corValues[i]
                rValue <- rValues[j]
                rel <- reliabilities[k]
                arValue <- arValues[l]
                sims <- run_sim_clpm(waves = 2,
                                     studyN=10000,      # N to generate
                                     ri_x=1,     # Random intercept variance for X
                                     ri_y=1,     # Random intercept variance for Y
                                     cor_i=rValue,   # Correlation between intercepts
                                     x=arValue,        # AR variance for X
                                     y=arValue,        # AR variance for Y
                                     stab_x=.5,  # Stability of X
                                     stab_y=.5,  # Stability of Y
                                     yx=.5,      # Cross lag (Y on X)
                                     xy=.2,      # Cross lag (X on Y)
                                     cor_xy=ar_corValue,  # Correlation between X and Y
                                     reliability_x=rel,       # Measurement error for X
                                     reliability_y=rel       # Measurement error for Y
                                     )
                results[loopRow,1] <- ar_corValue
                results[loopRow,2] <- rValue
                results[loopRow,3] <- rel
                results[loopRow,4] <- arValue
                results[loopRow,5] <- sims[[1]]
                results[loopRow,6] <- sims[[3]]
                loopRow <- loopRow+1
            }
        }
    }
}

## Changes names for Table
names(results) <- c("rAR", "rST","Reliability", "Autoregressive", "estimate_x","estimate_y")

temp <- results %>%
    filter(Reliability==1) %>%
    select(-estimate_x, -Reliability) %>%
    pivot_wider(names_from = rST, values_from = estimate_y)


names(temp) <- c("AR r", "AR Ratio", "0.1", "0.3", "0.5", "0.7")

papaja::apa_table(temp,
                  midrules=c(3,6,9),
                  align=rep("r", 10),
                  col_spanners=list("Stable Trait Correlation"=c(3,6)),
                  caption="Average Estimated Cross-Lagged Path In Each Simulation Condition When True Value = .5.",
                  note="AR r = Correlation between autoregressive components; AR Ratio = Ratio of autoregressive variance to stable-trait variance. Reliability is set to 1 for all simulations.")

```


First consider the example just discussed. Looking at the column where the correlation between the stable traits is .5, and the row where the correlation between the autoregressive traits is .5 and the ratio of autoregressive variance to stable-trait variance is 1, the true cross-lagged path of .5 is estimated to be .25. One can then move up and down that column or across that row to see the effects of the other factors on this underestimation. For instance, looking at the values in the rows immediately above and below this value shows that the underestimation of the cross-lagged paths is greater when there is more stable trait variance than when there is less. The true cross-lagged path of .50 is estimated to be .16 when there is twice as much stable trait variance as autoregressive variance, whereas it is estimated to be .33 (still an underestimate, but not as bad), when there is twice as much autoregressive variance as stable-trait variance. 

Moving across the same row shows how this estimate is affected by variation in the correlation between stable traits. As can be seen, the estimate for a true cross-lagged association of .50 declines from .31 when the correlation between the stable traits is a high .70 to .25 when the correlation is .50, to .21 when the correlation is .30, to .18 when the correlation is 0.1. Again, stable trait variance affects estimates of cross-lagged paths even when the stable traits are uncorrelated. 

Finally, moving across groups of rows (e.g., Rows 1 through 3 compared to Rows 4 through 6) shows the effect of the correlation between autoregressive components. In this case, the estimate of the cross-lagged parameter is *negatively* associated with the size of the correlation between autoregressive components, declining from .33 when the correlation between autoregressive components is .10 to .19 when the correlation is .70 (again, for the example where the stable-trait correlation is .5 and there is an equal amount of autoregressive and stable-trait variance). 

These simulations show that the RI-CLPM is not more conservative than the CLPM when testing cross-lagged effects. Indeed, when true cross-lagged associations exist, the RI-CLPM is actually *more likely* to find them than is the CLPM. Again, this pattern is actually easily predictable just by considering tracing rules for structural equation models. When stable-trait variance exists, the stability of the observed variables is overestimated in a CLPM, resulting in a corresponding underestimate of the cross-lagged paths in most situations. If researchers observe a pattern where cross-lagged paths routinely emerge when the CLPM is used but these paths disappear when the RI-CLPM is applied, then this would suggest that the effects themselves are likely spurious. Given the existence of stable-trait variance, true effects are more detectable with the RI-CLPM than the CLPM; if there is no stable-trait variance, the RI-CLPM simply reduces to the CLPM, and thus is equally likely to detect those effects. 

One caveat is that the above simulations were conducted specifying that the measures are perfectly reliable and that there is no occasion-specific state variance. This is unlikely in practice. Indeed, when state variance is included, the estimates from the RI-CLPM are also biased. The precise way that state variance impacts estimates is a quite complicated function of all the factors included in the previous simulations, the state factor, and the direction of the correlations and true cross-lagged paths. Because of this complexity, these simulations are beyond the scope of this paper, though the Shiny app is provided for readers to examine the effect of different combinations on estimated cross-lagged paths. Importantly, the STARTS model is appropriate for modeling data that includes stable-trait, autoregressive-trait, and state variance. Although the STARTS model has been somewhat underused in the literature because of frequent estimation problems, recent methodological advances in Bayesian modeling have helped address these concerns [@ludtke_more_2018]. 

## Moving Forward with the CLPM

After describing the various approaches available to model longitudinal data, @orth_testing_2021 made the following recommendation: "Before selecting a model, researchers should carefully consider the psychological or developmental process they would like to examine in their research, and then select a model that best estimates that process." This sounds like advice with which no one could argue.  But if there is a plausible alternative model that describes the data as well as (or better than) the preferred model, then much more work is needed to defend that selection. 

As an obvious example, if the true causal process linking self-esteem to depression is that changes in self-esteem instantaneously cause a corresponding change in depression (and there are no confounding factors), then that causal effect would be perfectly captured by the cross-sectional correlation between the two variables. Indeed, it would actually be problematic to rely on the cross-lagged association between self-esteem and depression controlling for earlier levels of depression as an estimate of the causal effect. Yet few would find a cross-sectional correlation between self-esteem and depression to be compelling evidence for a causal effect of self-esteem even if a researcher's preferred process only predicted such cross-sectional associations. This is because there are so many plausible alternative models that explain that cross-sectional effect. Unfortunately, the situation is the same with longitudinal data tested using the CLPM. 

In the first part of this paper, I argued that the critics of the RI-CLPM have not articulated a psychological process that is better described by the CLPM than the RI-CLPM. Simply stating that the process of interest is a "between-persons" process is not enough. The autoregressive part of the RI-CLPM clearly does link individual differences at one point in time to changes in the outcome variable, and the critics of the RI-CLPM are wrong that all "between-person" differences are relegated to the random intercept when this model is used. Perhaps there is some theoretical reason to link both completely stable individual differences (the random intercept in the RI-CLPM) *and* individual differences in the autoregressive components to change in the outcome variables. Such an argument would contradict decades of methodological work showing the necessity of separating effects at different levels of analysis for drawing appropriate conclusions [@curran_disaggregation_2011]. Moreover, this is not the argument that the critics of the RI-CLPM articulated. 

More importantly, the simulations reported in this paper show that relying on the CLPM when the variables in the model have some stable trait variance leads to dramatically inflated error rates (often reaching 100% in realistic scenarios, even with moderate sample sizes) while simultaneously reducing the ability to find true cross-lagged effects when they actually exist. In other words, when considering Type I and Type II errors, the CLPM is the worst of both worlds. The constructs that psychologists study very rarely have a purely autoregressive structure. At some point, the long-term stabilities of most constructs are stronger than would be suggested by the short-term stabilities and the length of time that has elapsed alone. This is likely due to the fact that many constructs have at least some stable-trait variance that is maintained over time. And if there is stable trait variance, it is quite plausible that two constructs correlate at the stable-trait level. The RI-CLPM and STARTS models provide a way to test this compelling and plausible model and to adjust for the problematic effects of these stable-trait components for the estimation of cross-lagged effects. 

So, if using the CLPM results in dramatically elevated error rates for cross-lagged paths when they do not exist, while also underestimating estimates of these associations when they do exist (again, sometimes dramatically in realistic scenarios), should we ever rely on the CLPM for causal analyses? My suggestion in the title of this paper that the CLPM be abandoned is not hyperbole; it simply reflects the difficulty identifying any situation where the CLPM would be preferable to alternatives. When at least three waves of data are available, the RI-CLPM can be used. If no stable trait variance exists, the RI-CLPM will simply reduce to a CLPM. 

To be sure, there are certain situations when alternatives to the CLPM cannot be used, most notably in the very common situation where only two waves of assessment are available. However, there are too many plausible alternative models that can lead to the same set of six correlations among two variables at two time points to draw any conclusions about which of those models is correct. In discussing reciprocal associations using cross-lagged analyses, @rogosa_myths_1995 identified this critical issue. He noted that there is a "hierarchy of research questions about longitudinal data [that] might start with describing how a single attribute---say aggression---changes over time. A next step would be questions about individual differences in change of aggression over time, especially correlates of change in aggression. Only after such questions are well understood does it seem reasonable to address a question about feedback or reciprocal effects, such as how change in aggression relates to change in exposure to TV violence or, does TV violence cause aggressive behavior?" (p. 34). Many researchers have noted that this first step---describing how a construct changes over time---is not possible with only two waves of data [e.g., @Fraley2005PR; @ployhart_two_2014; @rogosa_myths_1995]. If researchers do not understand the longitudinal structure of the variables they are examining, the estimates from a CLPM are uninterpretable. 


# References
